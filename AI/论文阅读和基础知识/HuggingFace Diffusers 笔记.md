# HuggingFace Diffusers ç¬”è®°

å‚è€ƒé“¾æ¥ï¼šhttps://huggingface.co/docs/diffusers/index

# ä¸€ã€åŸºç¡€å‡†å¤‡

å…ˆç”¨AutoDLåˆ›å»ºä¸€ä¸ªæœåŠ¡å™¨ï¼Œç„¶åç›´æ¥åœ¨base condaç¯å¢ƒä¸­åšæµ‹è¯•å³å¯ï¼Œå…ˆå®‰è£…diffuserså’Œtransformersåº“ï¼š
```bash
pip install diffusers["torch"] transformers
```



## 1.é…ç½®Huggingfaceé•œåƒ

https://hf-mirror.com/ è¿™ç¯‡ä¸‹é¢æœ‰è¿›è¡Œä»‹ç»ã€‚ä»¥ä¸‹æ˜¯éœ€è¦è¾“å…¥é…ç½®çš„æŒ‡ä»¤ï¼š
```bash
pip install -U huggingface_hub
export HF_ENDPOINT=https://hf-mirror.com
```



# äºŒã€å®æ“

## 1.åŸºæœ¬è®¤çŸ¥

```python
from diffusers import DDPMScheduler, UNet2DModel

scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256") # schedular
model = UNet2DModel.from_pretrained("google/ddpm-cat-256").to("cuda")

scheduler.set_timesteps(50)
# print(scheduler.timesteps)

import torch

sample_size = model.config.sample_size
noise = torch.randn((1, 3, sample_size, sample_size), device="cuda") # (1, 3, 256, 256),æŒ‡çš„æ˜¯ï¼ˆbatch_size, channel, height, widthï¼‰

# ä»¥ä¸‹ä¸ºå»å™ªçš„è¿‡ç¨‹ä»£ç 
input = noise  # åˆå§‹åŒ–ä¸ºçº¯å™ªå£°ï¼ˆlatent spaceç»´åº¦ï¼‰

for t in scheduler.timesteps:  # éå†æ‰€æœ‰timestepï¼ˆä»å¤§åˆ°å°ï¼Œå¦‚1000â†’0ï¼‰
    with torch.no_grad(): # æµ‹è¯•,æ²¡æœ‰åœ¨è®­ç»ƒ
        # æ­¥éª¤1ï¼šUNeté¢„æµ‹å™ªå£°æ®‹å·®
        noisy_residual = model(input, t).sample  # è¾“å…¥å½“å‰å™ªå£°å›¾+æ—¶é—´æ­¥ï¼Œè¾“å‡ºé¢„æµ‹çš„å™ªå£°
    
    # æ­¥éª¤2ï¼šSchedularä¼šæ ¹æ®å½“å‰çš„å™ªå£°å›¾å’Œæ—¶é—´æ­¥ï¼Œè®¡ç®—å‡ºä¸Šä¸€æ­¥çš„å™ªå£°å›¾ï¼ˆå»é™¤ä¸€éƒ¨åˆ†å™ªå£°ï¼‰
    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample  # è®¡ç®—å»å™ªåçš„å›¾åƒ
    
    # æ­¥éª¤3ï¼šæ›´æ–°è¾“å…¥ä¸ºå»å™ªåçš„å›¾åƒ
    input = previous_noisy_sample  # ä½œä¸ºä¸‹ä¸€è½®è¿­ä»£çš„è¾“å…¥

# æŠŠå»å™ªåçš„ç»“æœè½¬æ¢ä¸ºå›¾åƒ
from PIL import Image
import numpy as np
print('==============now input====================')
print(input.shape)  # (1, 3, 256, 256)
# print(input) # èŒƒå›´æ˜¯[-1, 1]
print('==============end====================')

image = (input / 2 + 0.5).clamp(0, 1).squeeze() # squeeze()å»æ‰batch_sizeç»´åº¦
image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy() # permuteå‡½æ•°æ˜¯ä¸ºäº†æŠŠç»´åº¦ä»(3, 256, 256)å˜æˆ(256, 256, 3)ï¼Œ*255æ˜¯ä¸ºäº†æŠŠèŒƒå›´ä»[0,1]å˜æˆ[0,255]ï¼Œround()å››èˆäº”å…¥ï¼Œto(torch.uint8)è½¬æ¢ä¸ºuint8ç±»å‹
image = Image.fromarray(image)
image.save("cat_manualDiffusion.png")  # ä¿å­˜å›¾ç‰‡
```

 Youâ€™ll initialize the necessary components, and set the number of timesteps to create a `timestep` array. The `timestep` array is used in the denoising loop, and for each element in this array, the model predicts a less noisy image. The denoising loop iterates over the `timestep`â€™s, and at each timestep, it outputs a noisy residual and the scheduler uses it to predict a less noisy image at the previous timestep. This process is repeated until you reach the end of the `timestep` array.



## 2.Deconstruct the Stable Diffusion pipeline

æœ¬èŠ‚çš„ç›®æ ‡æ˜¯æ‹†è§£Stable Diffusionã€‚Stable Diffusionæ˜¯è¿ä½œåœ¨Latent Spaceä¸‹çš„ã€‚The encoder compresses the image into a smaller representation, and a decoder converts the compressed representation back into an image. For text-to-image models, youâ€™ll need a tokenizer and an encoder to generate text embeddings. From the previous example, you already know you need a UNet model and a scheduler.

å¯¹SDæœ‰ä¸€ä¸ªåŸºæœ¬çš„å›é¡¾ï¼š

> ä»¥ä¸‹æ˜¯ç²¾ç®€ç‰ˆå›ç­”ï¼Œä»¥è¡¨æ ¼å½¢å¼æ€»ç»“Stable Diffusionæ ¸å¿ƒç»„ä»¶åŠæµç¨‹ï¼š
>
> ---
>
> ### **1. æ ¸å¿ƒç»„ä»¶åŠŸèƒ½**
> | ç»„ä»¶     | ä½œç”¨                                                  | è®­ç»ƒé˜¶æ®µ                                                     | æ¨ç†é˜¶æ®µï¼ˆç”Ÿæˆå›¾åƒï¼‰                                         |
> | -------- | ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
> | **CLIP** | æ–‡æœ¬ç¼–ç å™¨ï¼Œå°†æç¤ºè¯è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡ï¼ˆText Embeddingsï¼‰ | å›ºå®šæƒé‡ï¼ˆé€šå¸¸ä¸è®­ç»ƒï¼‰ï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹                   | å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æç¤ºç¼–ç ä¸ºUNetå¯ç†è§£çš„å‘é‡                   |
> | **VAE**  | å›¾åƒç¼–ç å™¨/è§£ç å™¨ï¼Œåœ¨åƒç´ ç©ºé—´å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´è½¬æ¢       | è®­ç»ƒï¼šç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆå‹ç¼©ç»´åº¦ï¼‰ï¼Œè§£ç é‡å»ºå›¾åƒ           | ç¼–ç å™¨ï¼šä»…ç”¨äºè®­ç»ƒï¼›è§£ç å™¨ï¼šå°†UNetè¾“å‡ºçš„æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºæœ€ç»ˆå›¾åƒ |
> | **UNet** | å™ªå£°é¢„æµ‹å™¨ï¼Œåœ¨æ½œåœ¨ç©ºé—´é€æ­¥å»å™ª                        | è®­ç»ƒï¼šå­¦ä¹ é¢„æµ‹æ·»åŠ åˆ°æ½œåœ¨è¡¨ç¤ºä¸­çš„å™ªå£°ï¼ˆè¾“å…¥å¸¦å™ªæ½œåœ¨å‘é‡+æ—¶é—´æ­¥+æ–‡æœ¬æ¡ä»¶ï¼‰ | è¿­ä»£å»å™ªï¼šæ ¹æ®CLIPæ–‡æœ¬æ¡ä»¶ï¼Œé€æ­¥é¢„æµ‹å¹¶ç§»é™¤å™ªå£°ï¼Œç”Ÿæˆå¹²å‡€çš„æ½œåœ¨è¡¨ç¤º |
>
> ---
>
> ### **2. è®­ç»ƒ vs æ¨ç†æµç¨‹å¯¹æ¯”**
> | é˜¶æ®µ     | è¾“å…¥                         | è¾“å‡º                     | å…³é”®æ“ä½œ                                                     |
> | -------- | ---------------------------- | ------------------------ | ------------------------------------------------------------ |
> | **è®­ç»ƒ** | 1. å›¾åƒ â†’ VAEç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤º  | é‡å»ºå›¾åƒï¼ˆVAEè§£ç ï¼‰      | 1. å¯¹æ½œåœ¨è¡¨ç¤ºåŠ å™ªå£°<br>2. UNetå­¦ä¹ é¢„æµ‹å™ªå£°ï¼ˆæ¡ä»¶ï¼šCLIPæ–‡æœ¬+æ—¶é—´æ­¥ï¼‰ |
> |          | 2. æ–‡æœ¬ â†’ CLIPç¼–ç ä¸ºåµŒå…¥å‘é‡ | å™ªå£°é¢„æµ‹æŸå¤±ï¼ˆUNetè¾“å‡ºï¼‰ |                                                              |
> | **æ¨ç†** | 1. æ–‡æœ¬ â†’ CLIPç”ŸæˆåµŒå…¥å‘é‡   | ç”Ÿæˆå›¾åƒï¼ˆVAEè§£ç ï¼‰      | 1. ä»éšæœºå™ªå£°å¼€å§‹<br>2. UNetè¿­ä»£å»å™ªï¼ˆæ¡ä»¶ï¼šCLIPæ–‡æœ¬+è°ƒåº¦å™¨æ§åˆ¶æ—¶é—´æ­¥ï¼‰ |
> |          | 2. éšæœºå™ªå£° â†’ UNeté€æ­¥å»å™ª   |                          | 3. æœ€ç»ˆæ½œåœ¨è¡¨ç¤º â†’ VAEè§£ç ä¸ºå›¾åƒ                              |
>
> ---
>
> ### **å…³é”®ç‚¹æ€»ç»“**
> - **CLIP**ï¼šæ–‡æœ¬ç†è§£ï¼ˆä¸å‚ä¸æ‰©æ•£è®­ç»ƒï¼Œä»…æä¾›æ¡ä»¶ï¼‰ã€‚  
> - **VAE**ï¼šç©ºé—´å‹ç¼©ï¼ˆè®­ç»ƒæ—¶ç¼–ç /è§£ç ï¼›æ¨ç†æ—¶ä»…è§£ç ï¼‰ã€‚  
> - **UNet**ï¼šæ‰©æ•£æ ¸å¿ƒï¼ˆè®­ç»ƒæ—¶å­¦å™ªå£°é¢„æµ‹ï¼Œæ¨ç†æ—¶æ‰§è¡Œå»å™ªï¼‰ã€‚  
> - **æµç¨‹å·®å¼‚**ï¼šè®­ç»ƒæ—¶UNetå­¦ä¹ å™ªå£°é¢„æµ‹ï¼›æ¨ç†æ—¶ç”¨å­¦åˆ°çš„æ¨¡å‹é€æ­¥å»å™ªç”Ÿæˆå›¾åƒã€‚

è¿™ä¸€èŠ‚çš„ä»£ç å¦‚ä¸‹ï¼š
```python
from PIL import Image
import torch
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler

# è¿™äº›æ–‡ä»¶éƒ½å¯ä»¥å»è¿™é‡Œçœ‹ï¼šhttps://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/tree/main
vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", use_safetensors=True)
tokenizer = CLIPTokenizer.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="tokenizer")
text_encoder = CLIPTextModel.from_pretrained(
    "CompVis/stable-diffusion-v1-4", subfolder="text_encoder", use_safetensors=True
)
unet = UNet2DConditionModel.from_pretrained(
    "CompVis/stable-diffusion-v1-4", subfolder="unet", use_safetensors=True
)

# æ¢ä¸€ä¸ªSchedularè¿›è¡Œå°è¯•
from diffusers import UniPCMultistepScheduler

scheduler = UniPCMultistepScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")

torch_device = "cuda"
vae.to(torch_device)
text_encoder.to(torch_device)
unet.to(torch_device)

# create text embeddings
prompt = ['a photo of a cartoon character, looks like someone in Genshin Impact.']
height = 512
width = 512 # é»˜è®¤SDæ˜¯512x512
num_inference_steps = 50  # Number of denoising steps
guidance_scale = 7.5
generator = torch.Generator(device=torch_device).manual_seed(1145)  # æ–°ä»£ç ï¼ˆåœ¨CUDAä¸Šï¼‰,generatoréœ€è¦å’Œå…¶ä»–çš„tensoråœ¨åŒä¸€ä¸ªdeviceä¸Š
batch_size = len(prompt)

text_input = tokenizer(
    prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt"
) # truncation=Trueè¡¨ç¤ºå¦‚æœè¶…è¿‡æœ€å¤§é•¿åº¦å°±æˆªæ–­ï¼Œreturn_tensors="pt"è¡¨ç¤ºè¿”å›PyTorchçš„tensoræ ¼å¼
# ä¸Šé¢çš„max_lengthæ˜¯77ï¼Œtokenizer.model_max_length = 77
with torch.no_grad():
    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]

# Youâ€™ll also need to generate the unconditional text embeddings which are the embeddings for the padding token. These need to have the same shape (batch_size and seq_length) as the conditional text_embeddings:
max_length = text_input.input_ids.shape[-1] # shape[-1]è¡¨ç¤ºæœ€åä¸€ä¸ªç»´åº¦çš„å¤§å°
print("show two shapes: tokenizer.model_max_length, text_input.input_ids.shape[-1]")
print(tokenizer.model_max_length, text_input.input_ids.shape[-1]) # 77, 77
uncond_input = tokenizer([""] * batch_size, padding="max_length", max_length=max_length, return_tensors="pt")
uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]

# ç°åœ¨æˆ‘ä»¬å¯ä»¥æŠŠæ¡ä»¶å’Œæ— æ¡ä»¶çš„æ–‡æœ¬åµŒå…¥è¿æ¥èµ·æ¥äº†
text_embeddings = torch.cat([uncond_embeddings, text_embeddings])  # [2, 77, 768]
# create random noise, //8 æ˜¯å› ä¸ºunetçš„è¾“å…¥æ˜¯latent spaceç»´åº¦
latents = torch.randn(
    (batch_size, unet.config.in_channels, height // 8, width // 8),
    generator=generator,
    device=torch_device,
)
latents = latents * scheduler.init_noise_sigma # which is required for improved schedulers like UniPCMultistepScheduler.

'''
The last step is to create the denoising loop thatâ€™ll progressively transform the pure noise in latents to an image described by your prompt. Remember, the denoising loop needs to do three things:

Set the schedulerâ€™s timesteps to use during denoising.
Iterate over the timesteps.
At each timestep, call the UNet model to predict the noise residual and pass it to the scheduler to compute the previous noisy sample.
'''
from tqdm.auto import tqdm
scheduler.set_timesteps(num_inference_steps)
for t in tqdm(scheduler.timesteps):
    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
    latent_model_input = torch.cat([latents] * 2)

    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)

    # predict the noise residual
    with torch.no_grad():
        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

    # perform guidance
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

    # compute the previous noisy sample x_t -> x_t-1
    latents = scheduler.step(noise_pred, t, latents).prev_sample

# scale and decode the image latents with vae
latents = 1 / 0.18215 * latents
with torch.no_grad():
    image = vae.decode(latents).sample
image = (image / 2 + 0.5).clamp(0, 1).squeeze()
image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()
image = Image.fromarray(image)
image.save("02_output.png")

```



## 3.AutoPipelineçš„å­¦ä¹ 

ç®€å•æ¥è¯´ï¼Œå°±æ˜¯å¿«é€Ÿéƒ¨ç½²Diffusion Modelå¯¹åº”çš„å­æ¨¡å—ï¼Œä»£ç é‡å¾ˆçŸ­å³å¯å®Œæˆä»»åŠ¡ã€‚

> ### **Diffusers ä¸­çš„ AutoPipeline æ¦‚å¿µï¼ˆç®€è¦ä»‹ç»ï¼‰**
> Hugging Face **Diffusers** åº“ä¸­çš„ **AutoPipeline** æ˜¯ä¸€ä¸ª**è‡ªåŠ¨åŒ–ç®¡é“ï¼ˆPipelineï¼‰**ï¼Œå®ƒå¯ä»¥æ ¹æ®ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆå›¾åƒã€å›¾åƒä¿®å¤ã€è¶…åˆ†è¾¨ç‡ç­‰ï¼‰**è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„é¢„è®­ç»ƒæ¨¡å‹å’Œè°ƒåº¦å™¨ï¼ˆSchedulerï¼‰**ï¼Œç®€åŒ–äº† Stable Diffusion ç­‰æ‰©æ•£æ¨¡å‹çš„ä½¿ç”¨æµç¨‹ã€‚
>
> ---
>
> ### **AutoPipeline çš„ä½œç”¨**
> 1. **è‡ªåŠ¨é€‚é…ä»»åŠ¡**  
>    - æ— éœ€æ‰‹åŠ¨æŒ‡å®š `StableDiffusionPipeline`ã€`StableDiffusionImg2ImgPipeline` ç­‰å…·ä½“ç±»ï¼Œåªéœ€é€šè¿‡ `AutoPipelineForText2Image`ã€`AutoPipelineForImage2Image` ç­‰é€šç”¨æ¥å£ï¼ŒDiffusers ä¼šè‡ªåŠ¨åŠ è½½åˆé€‚çš„æ¨¡å‹ã€‚
>    - æ”¯æŒçš„ä»»åŠ¡åŒ…æ‹¬ï¼š
>      - æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆText-to-Imageï¼‰
>      - å›¾åƒç”Ÿæˆå›¾åƒï¼ˆImage-to-Imageï¼‰
>      - å›¾åƒä¿®å¤ï¼ˆInpaintingï¼‰
>      - è¶…åˆ†è¾¨ç‡ï¼ˆSuper Resolutionï¼‰
>      - æ·±åº¦å›¾ç”Ÿæˆï¼ˆDepth-to-Imageï¼‰ç­‰ã€‚
>
> 2. **è‡ªåŠ¨é€‰æ‹©æ¨¡å‹å’Œè°ƒåº¦å™¨**  
>    - æ ¹æ®ä»»åŠ¡ç±»å‹ï¼ŒAutoPipeline ä¼šä» Hugging Face Hub åŠ è½½**æœ€æ–°æˆ–æœ€ä¼˜çš„é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆå¦‚ `stabilityai/stable-diffusion-2-1`ï¼‰ã€‚
>    - è‡ªåŠ¨é…ç½®é€‚åˆçš„**å™ªå£°è°ƒåº¦å™¨ï¼ˆSchedulerï¼‰**ï¼ˆå¦‚ `PNDMScheduler`ã€`DPMSolverMultistepScheduler`ï¼‰ã€‚
>
> 3. **ç®€åŒ–ä»£ç **  
>    - ä¼ ç»Ÿæ–¹å¼éœ€è¦æ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å‹ã€è°ƒåº¦å™¨ï¼Œè€Œ AutoPipeline åªéœ€ä¸€è¡Œä»£ç å³å¯å®Œæˆåˆå§‹åŒ–ï¼Œä¾‹å¦‚ï¼š
>      ```python
>      from diffusers import AutoPipelineForText2Image
>      pipeline = AutoPipelineForText2Image.from_pretrained("runwayml/stable-diffusion-v1-5")
>      ```
>    - é€‚åˆå¿«é€Ÿå®éªŒå’Œéƒ¨ç½²ï¼Œæ— éœ€å…³å¿ƒåº•å±‚ç»†èŠ‚ã€‚
>
> 4. **å…¼å®¹å¤šç§æ¨¡å‹æ¶æ„**  
>    - æ”¯æŒ Stable Diffusionï¼ˆ1.x/2.x/XLï¼‰ã€Kandinskyã€DeepFloyd IF ç­‰æ‰©æ•£æ¨¡å‹å®¶æ—ã€‚
>    - æœªæ¥æ–°å‘å¸ƒçš„æ¨¡å‹ä¹Ÿèƒ½é€šè¿‡ AutoPipeline è‡ªåŠ¨é€‚é…ã€‚
>
> ---
>
> ### **é€‚ç”¨åœºæ™¯**
> - **å¿«é€ŸåŸå‹å¼€å‘**ï¼šæ— éœ€æ‰‹åŠ¨é…ç½®å³å¯æµ‹è¯•ä¸åŒä»»åŠ¡ã€‚
> - **æ¨¡å‹å¯¹æ¯”**ï¼šè½»æ¾åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼ˆå¦‚ SD v1.5 vs SD XLï¼‰ã€‚
> - **ç”Ÿäº§éƒ¨ç½²**ï¼šå‡å°‘ä»£ç ç»´æŠ¤æˆæœ¬ï¼Œè‡ªåŠ¨é€‚é…æœ€ä½³å®è·µã€‚
>
> ---
>
> ### **æ€»ç»“**
> AutoPipeline æ˜¯ Diffusers æä¾›çš„ä¸€ä¸ª**é«˜çº§æŠ½è±¡æ¥å£**ï¼Œæ—¨åœ¨è®©ç”¨æˆ·æ›´ä¾¿æ·åœ°ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Œ**é¿å…æ‰‹åŠ¨é€‰æ‹©æ¨¡å‹å’Œè°ƒåº¦å™¨**ï¼ŒåŒæ—¶ä¿æŒçµæ´»æ€§å’Œæ€§èƒ½ã€‚é€‚åˆä¸æƒ³æ·±å…¥åº•å±‚ç»†èŠ‚ï¼Œä½†ä»æƒ³é«˜æ•ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼€å‘è€…ã€‚

ä»¥ä¸‹æ˜¯æ–‡ç”Ÿå›¾çš„ç¤ºä¾‹ä»£ç ï¼ˆimg2imgï¼Œinpaintéƒ½æ˜¯ç±»ä¼¼çš„ï¼Œå…·ä½“å¯ä»¥çœ‹huggingfaceçš„å®˜æ–¹é“¾æ¥ï¼š[Train a diffusion model (huggingface.co)](https://huggingface.co/docs/diffusers/main/en/tutorials/basic_training)ï¼‰

```python
# The AutoPipeline automatically detects the correct pipeline class to use.
from diffusers import AutoPipelineForText2Image
import torch

pipe_txt2img = AutoPipelineForText2Image.from_pretrained(
    "dreamlike-art/dreamlike-photoreal-2.0", torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

prompt = "cinematic photo of Godzilla eating sushi with a cat in a izakaya, 35mm photograph, film, professional, 4k, highly detailed"
generator = torch.Generator(device="cpu").manual_seed(37)
image = pipe_txt2img(prompt, generator=generator).images[0]
image.save("cat_autoPipeline.png")  # ä¿å­˜å›¾ç‰‡
```



## 4.Train a diffusion model

[Train a diffusion model (huggingface.co)](https://huggingface.co/docs/diffusers/main/en/tutorials/basic_training)

ä»¥ä¸‹å…ˆåŸºäºè¿™ä¸ªé“¾æ¥ï¼š[diffusers_training_example.ipynb - Colab (google.com)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)

é¦–å…ˆï¼Œå®‰è£…å¿…è¦çš„åŒ…ï¼š

```shell
pip install -U diffusers[training]
```



### ï¼ˆ1ï¼‰Huggingfaceçš„é…ç½®ä¸æ•°æ®é›†ä¸‹è½½

> å…³äºHuggingfaceçš„æ¨¡å‹ä¸‹è½½ï¼Œæ¨èè¿™ä¸ªé“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/663712983

**å°†ç¯å¢ƒå˜é‡çš„é…ç½®å‘½ä»¤å†™å…¥åˆ°ç»ˆç«¯çš„é…ç½®æ–‡ä»¶ä¸­**ï¼Œ**ä½¿å¾—ç»ˆç«¯è‡ªåŠ¨åŠ è½½è¯¥ç¯å¢ƒå˜é‡ï¼Œå…å»æ¯æ¬¡æ‰‹åŠ¨æ‰§è¡Œå‘½ä»¤çš„éº»çƒ¦**ã€‚

```bash
echo 'export HF_ENDPOINT="https://hf-mirror.com"' >> ~/.bashrc
```

- ä¼˜å…ˆæ¨èï¼š`huggingface-cli`
  - å®‰è£…ï¼š`pip install -U huggingface_hub`

#### ï¼ˆaï¼‰æ•°æ®é›†ä¸‹è½½

å¯ä»¥åœ¨ä»£ç ä¸­è¿™æ ·å†™ï¼š

```python
dataset = load_dataset(
    config.dataset_name, 
    split="train", 
    token=globalConfig.huggingface_token, # è¿™é‡Œå¡«å†™huggingfaceçš„tokenï¼Œä¸ºäº†å®‰å…¨èµ·è§æ”¾åˆ°äº†å…¶ä»–æ–‡ä»¶ä¸­
    download_mode="force_redownload")  # load the dataset
```

`print(dataset)`çš„è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```python
Dataset({
    features: ['image'],
    num_rows: 8189
})
```

> You can find additional datasets from the [HugGan Community Event](https://huggingface.co/huggan) or you can use your own dataset by creating a local [`ImageFolder`](https://huggingface.co/docs/datasets/image_dataset#imagefolder). Set `config.dataset_name` to the repository id of the dataset if it is from the HugGan Community Event, or `imagefolder` if youâ€™re using your own images.

å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œå¯ä»¥å°è¯•å¦‚ä¸‹çš„æ–¹æ¡ˆï¼š

> ä½ çš„ä»£ç ä»ç„¶æ— æ³•è¿æ¥åˆ° Hugging Face æ•°æ®é›†ï¼Œå¯èƒ½æ˜¯ç”±äºç½‘ç»œé™åˆ¶æˆ–é•œåƒç«™ä¸ç¨³å®šã€‚ä»¥ä¸‹æ˜¯ **æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†** çš„æ–¹æ³•ï¼ˆé€‚ç”¨äº Linux ç¯å¢ƒï¼‰ï¼Œä»¥åŠä¸€äº›é¢å¤–çš„è§£å†³æ–¹æ¡ˆï¼š
>
> ---
>
> ### **1. æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†ï¼ˆLinux æ–¹æ³•ï¼‰**
> #### **(1) ä½¿ç”¨ `git lfs` ä¸‹è½½æ•°æ®é›†**
> ```bash
> # å®‰è£… git-lfsï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
> sudo apt-get update && sudo apt-get install git-lfs -y
> git lfs install
> 
> # å…‹éš†æ•°æ®é›†ï¼ˆä½¿ç”¨å›½å†…é•œåƒï¼‰
> export HF_ENDPOINT=https://hf-mirror.com
> git lfs clone https://hf-mirror.com/datasets/huggan/anime-faces
> # å¦‚æœä¸­é—´æ–­äº†ï¼Œå¯ä»¥cdè¿›ä¸‹è½½çš„æ–‡ä»¶å¤¹å½“ä¸­ï¼Œè¾“å…¥ï¼š
> git lfs pull # è¦åœ¨ä¸‹è½½åˆ°æœ¬åœ°çš„æ–‡ä»¶å¤¹å½“ä¸­å†™
> ```
> å¦‚æœæ•°æ®é›†è¾ƒå¤§ï¼Œå¯ä»¥åªä¸‹è½½éƒ¨åˆ†æ–‡ä»¶ï¼š
> ```bash
> git clone --depth 1 https://hf-mirror.com/datasets/huggan/anime-faces
> cd anime-faces
> git lfs pull --include="*.jpg"  # åªä¸‹è½½å›¾ç‰‡
> ```
>
> #### **(2) ä½¿ç”¨ `wget` æˆ– `curl` ä¸‹è½½**
> å¦‚æœæ•°æ®é›†æ˜¯ `.zip` æˆ– `.tar.gz` æ ¼å¼ï¼š
> ```bash
> wget https://hf-mirror.com/datasets/huggan/anime-faces/resolve/main/data.zip
> unzip data.zip -d ./anime-faces
> ```
>
> #### **(3) åŠ è½½æœ¬åœ°æ•°æ®é›†**
> ```python
> import os
> from datasets import load_dataset
> 
> # æŒ‡å®šæœ¬åœ°è·¯å¾„
> dataset_path = "./anime-faces"  # æ›¿æ¢ä¸ºä½ çš„è·¯å¾„
> dataset = load_dataset(dataset_path, split="train")
> ```
>
> ---
>
> ### **2. æ£€æŸ¥ç½‘ç»œè¿æ¥**
> ç¡®ä¿ä½ çš„æœåŠ¡å™¨èƒ½è®¿é—® `hf-mirror.com`ï¼š
> ```bash
> ping hf-mirror.com
> curl -v https://hf-mirror.com
> ```
> å¦‚æœæ— æ³•è®¿é—®ï¼Œå¯èƒ½æ˜¯ **é˜²ç«å¢™/ä»£ç†é—®é¢˜**ï¼Œå°è¯•ï¼š
> ```bash
> export http_proxy=http://your-proxy:port
> export https_proxy=http://your-proxy:port
> ```
>
> ---
>
> ### **3. å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™**
> ç¡®ä¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆï¼š
> ```python
> import os
> os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # å¼ºåˆ¶é•œåƒ
> os.environ['HF_HUB_OFFLINE'] = '1'  # ç¦»çº¿æ¨¡å¼ï¼ˆå¦‚æœå·²ä¸‹è½½ï¼‰
> ```
>
> ---
>
> ### **4. ä½¿ç”¨ `datasets` çš„ç¦»çº¿æ¨¡å¼**
> å¦‚æœæ•°æ®é›†å·²ç¼“å­˜ï¼Œç›´æ¥åŠ è½½ç¼“å­˜ï¼š
> ```python
> dataset = load_dataset("huggan/anime-faces", split="train", download_mode="reuse_dataset_if_exists")
> ```
>
> ---
>
> ### **5. æ£€æŸ¥ Hugging Face Token**
> å¦‚æœä½ çš„æ•°æ®é›†æ˜¯ç§æœ‰çš„ï¼Œç¡®ä¿ `token` æ­£ç¡®ï¼š
> ```python
> from huggingface_hub import login
> login(token="your_token_here")  # æ›¿ä»£å…¨å±€å˜é‡
> ```
>
> ---
>
> ### **6. ç›´æ¥ä¸‹è½½ `.parquet` æ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰**
> æŸäº›æ•°æ®é›†æä¾› `.parquet` æ ¼å¼ï¼Œå¯ç›´æ¥ä¸‹è½½ï¼š
> ```python
> import pandas as pd
> url = "https://hf-mirror.com/datasets/huggan/anime-faces/resolve/main/train.parquet"
> df = pd.read_parquet(url)
> ```
>
> ---
>
> ### **7. ä½¿ç”¨ `aria2` åŠ é€Ÿä¸‹è½½**
> å¦‚æœæ•°æ®é›†å¾ˆå¤§ï¼Œå¯ä»¥ç”¨å¤šçº¿ç¨‹ä¸‹è½½ï¼š
> ```bash
> sudo apt-get install aria2
> aria2c -x 16 -s 16 "https://hf-mirror.com/datasets/huggan/anime-faces/resolve/main/data.zip"
> ```
>
> ---
>
> ### **æ€»ç»“**
> 1. **æ¨èæ‰‹åŠ¨ä¸‹è½½**ï¼ˆ`git lfs clone` æˆ– `wget`ï¼‰ï¼Œç„¶åæœ¬åœ°åŠ è½½ã€‚  
> 2. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®ã€‚  
> 3. ç¡®ä¿ `HF_ENDPOINT` ç¯å¢ƒå˜é‡ç”Ÿæ•ˆã€‚  
> 4. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨ç½‘ç»œé™åˆ¶ï¼Œå°è¯•æ›´æ¢ç½‘ç»œç¯å¢ƒã€‚
>
> å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·æä¾›ï¼š
> - ä½ ä½¿ç”¨çš„å…·ä½“æ•°æ®é›†åç§°ï¼ˆ`huggan/anime-faces` è¿˜æ˜¯ `vollerei-id/anime_cartoon2`ï¼Ÿï¼‰
> - `ping hf-mirror.com` çš„ç»“æœ
> - æ˜¯å¦åœ¨ä»£ç†ç¯å¢ƒä¸‹è¿è¡Œï¼Ÿ



#### ï¼ˆbï¼‰å¯è§†åŒ–æ•°æ®é›†ä¸­çš„æ•°æ®

Datasets uses the [Image](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Image) feature to automatically decode the image data and load it as a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html) which we can visualize:

```python
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 4, figsize=(16, 4))
for i, image in enumerate(dataset[:4]["image"]):
    axs[i].imshow(image)
    axs[i].set_axis_off()
# fig.show()
# å¦‚æœéœ€è¦åœ¨linuxä¸Šä¿å­˜å›¾ç‰‡ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç 
plt.savefig("flowers.png", bbox_inches='tight', pad_inches=0.1)
```

å¯è§†åŒ–çš„ç»“æœå¦‚ä¸‹ï¼š

![image-20250424111129755](./assets/image-20250424111129755.png)



#### ï¼ˆcï¼‰æ•°æ®å¤„ç†å¹¶æ”¾å…¥DataLoaderï¼Œå‡†å¤‡è®­ç»ƒ

æ³¨æ„åˆ°ä¸Šé¢çš„å›¾çš„å¤§å°æ˜¯ä¸ä¸€è‡´çš„ï¼Œå› æ­¤éœ€è¦è¿›è¡Œç»Ÿä¸€çš„æ•°æ®å¤„ç†ï¼Œå¦‚ä¸‹ï¼š

- `Resize` changes the image size to the one defined in `config.image_size`.
- `RandomHorizontalFlip` augments the dataset by randomly mirroring the images.
- `Normalize` is important to rescale the pixel values into a [-1, 1] range, which is what the model expects.

```python
# å¤„ç†å›¾ç‰‡
from torchvision import transforms

preprocess = transforms.Compose(
    [
        transforms.Resize((config.image_size, config.image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        # pytorchçš„transform.Normalizeçš„å…¬å¼æ˜¯ï¼šnormalize(x) = (x - mean) / stdï¼Œtransforms.ToTensor() ä¼šå°† PIL å›¾åƒæˆ– NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡ï¼Œå¹¶å°†åƒç´ å€¼ä» [0, 255] ç¼©æ”¾åˆ° [0, 1]ï¼ˆæµ®ç‚¹æ•°ï¼‰ã€‚
        # æ‰€ä»¥å°±ç›¸å½“äºæŠŠåƒç´ å€¼ä» [0, 255] ç¼©æ”¾åˆ° [-1, 1]ï¼Œmeanå’Œstdéƒ½æ˜¯0.5ï¼Œæ‰€ä»¥å°±ç›¸å½“äºæŠŠåƒç´ å€¼ä» [0, 255] ç¼©æ”¾åˆ° [-1, 1]
        transforms.Normalize([0.5], [0.5]), 
    ]
)

# Use ğŸ¤— Datasetsâ€™ set_transform method to apply the preprocess function on the fly during training:
def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    return {"images": images}

dataset.set_transform(transform)
import torch
train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)
```



####  (d)åˆ›å»ºUNet2DModel

Pretrained models in ğŸ§¨ Diffusers are easily created from their model class with the parameters you want. For example, to create a [UNet2DModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d#diffusers.UNet2DModel):

```python
# åˆ›å»ºUNet2DModel
from diffusers import UNet2DModel
model = UNet2DModel(
    sample_size=config.image_size,  # the target image resolution
    in_channels=3,  # the number of input channels, 3 for RGB images
    out_channels=3,  # the number of output channels
    layers_per_block=2,  # how many ResNet layers to use per UNet block
    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block
    down_block_types=(
        "DownBlock2D",  # a regular ResNet downsampling block
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",  # a ResNet downsampling block with spatial self-attention
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",  # a regular ResNet upsampling block
        "AttnUpBlock2D",  # a ResNet upsampling block with spatial self-attention
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)

# It is often a good idea to quickly check the sample image shape matches the model output shape:
sample_image = dataset[0]["images"].unsqueeze(0) # unsqueeze(0)è¡¨ç¤ºåœ¨ç¬¬0ç»´å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œå˜æˆ(1, 3, 128, 128)ï¼Œbatch_size=1
print("Input shape:", sample_image.shape) # Input shape: torch.Size([1, 3, 128, 128])
print("Output shape:", model(sample_image, timestep=0).sample.shape) # Output shape: torch.Size([1, 3, 128, 128])

```



#### ï¼ˆeï¼‰åˆ›å»ºSchedular

The scheduler behaves differently depending on whether youâ€™re using the model for training or inference. During inference, the scheduler generates image from the noise. During training, the scheduler takes a model output - or a sample - from a specific point in the diffusion process and applies noise to the image according to a *noise schedule* and an *update rule*.

Letâ€™s take a look at the [DDPMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/ddpm#diffusers.DDPMScheduler) and use the `add_noise` method to add some random noise to the `sample_image` from before:

```python
import torch
from PIL import Image
from diffusers import DDPMScheduler

noise_scheduler = DDPMScheduler(num_train_timesteps=1000) # ï¼ˆ1ï¼‰
noise = torch.randn(sample_image.shape)
timesteps = torch.LongTensor([50]) # ï¼ˆ2ï¼‰è¿™é‡Œçš„timestepsæ˜¯ä¸€ä¸ªä¸€ç»´çš„tensorï¼Œè¡¨ç¤ºå½“å‰çš„æ—¶é—´æ­¥ï¼Œå€¼ä¸º50, LongTensorè¡¨ç¤ºé•¿æ•´å‹çš„tensor
noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)

# ä»¥ä¸‹ä»£ç çš„æ„æ€æ˜¯æŠŠnoisy_imageçš„èŒƒå›´ä»[-1, 1]ç¼©æ”¾åˆ°[0, 255]ï¼Œç„¶åè½¬æ¢ä¸ºuint8ç±»å‹çš„numpyæ•°ç»„ï¼Œæœ€åè½¬æ¢ä¸ºPILå›¾åƒï¼Œpermuteå‡½æ•°æ˜¯ä¸ºäº†æŠŠç»´åº¦ä»(1, 3, 128, 128)å˜æˆ(1ï¼Œ128, 128, 3)ï¼Œ*255æ˜¯ä¸ºäº†æŠŠèŒƒå›´ä»[-1, 1]ç¼©æ”¾åˆ°[0, 255]ï¼Œto(torch.uint8)è½¬æ¢ä¸ºuint8ç±»å‹
Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])
# ä¿å­˜å›¾ç‰‡ä¸ºpngæ ¼å¼
# Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0]).save("noisy_image.png")

```

å®é™…çš„æŸå¤±å‡½æ•°çš„è®¡ç®—å¼å­å¦‚ä¸‹ï¼ˆä¼šåœ¨åé¢çš„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼‰ï¼š

```python
import torch.nn.functional as F

noise_pred = model(noisy_image, timesteps).sample
loss = F.mse_loss(noise_pred, noise)
```



> åœ¨Diffusionæ¨¡å‹ä¸­ï¼Œï¼ˆ1ï¼‰å’Œï¼ˆ2ï¼‰çš„`timesteps`æ•°å€¼ä¸åŒæ˜¯å› ä¸ºå®ƒä»¬åˆ†åˆ«ä»£è¡¨ä¸åŒçš„å«ä¹‰ï¼Œè¿™ç§å·®å¼‚æ˜¯è®¾è®¡ä¸Šçš„éœ€è¦ï¼Œä¸ä¼šå¯¹æ¨¡å‹æ•ˆæœäº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä»¥ä¸‹æ˜¯å…·ä½“è§£é‡Šï¼š
>
> ---
>
> ### ï¼ˆ1ï¼‰`num_train_timesteps=1000`
> - **å«ä¹‰**ï¼šè¿™è¡¨ç¤ºæ‰©æ•£è¿‡ç¨‹çš„æ€»æ­¥æ•°ä¸º1000æ­¥ã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šé€æ­¥ä»çº¯å™ªå£°ï¼ˆ`t=1000`ï¼‰åå‘é¢„æµ‹åˆ°æ¸…æ™°å›¾åƒï¼ˆ`t=0`ï¼‰ã€‚
> - **ä½œç”¨**ï¼š  
>   - å®šä¹‰äº†å™ªå£°è°ƒåº¦å™¨çš„â€œæ—¶é—´åˆ†è¾¨ç‡â€ï¼Œå³å™ªå£°æ·»åŠ /å»é™¤çš„ç²’åº¦ã€‚  
>   - æ›´å¤§çš„`num_train_timesteps`ï¼ˆå¦‚1000ï¼‰èƒ½è®©æ‰©æ•£è¿‡ç¨‹æ›´å¹³æ»‘ï¼Œä½†ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚  
>   - è¿™ä¸ªå€¼åœ¨è®­ç»ƒæ—¶å›ºå®šï¼Œå½±å“å™ªå£°æ–¹å·®ï¼ˆ`beta_t`ï¼‰çš„è°ƒåº¦ç­–ç•¥ã€‚
>
> ---
>
> ### ï¼ˆ2ï¼‰`timesteps = torch.LongTensor([50])`
> - **å«ä¹‰**ï¼šè¿™é‡ŒæŒ‡å®šäº†å½“å‰æ“ä½œçš„å…·ä½“æ—¶é—´æ­¥æ˜¯ç¬¬50æ­¥ï¼ˆä»1000æ­¥ä¸­é€‰å–ï¼‰ã€‚  ==æˆ‘çš„ç†è§£æ˜¯è¿™è¡Œä»£ç ä»…ç”¨äºæµ‹è¯•åŠ å™ªå£°çš„æ•ˆæœï¼Œå®é™…è®­ç»ƒä¸ä¼šè¿™ä¹ˆåšã€‚==
> - **ä½œç”¨**ï¼š  
>   - åœ¨`add_noise`ä¸­ï¼Œ`timesteps=50`è¡¨ç¤ºå¯¹è¾“å…¥å›¾åƒæ·»åŠ ç¬¬50æ­¥å¯¹åº”çš„å™ªå£°å¼ºåº¦ï¼ˆæ ¹æ®è°ƒåº¦å™¨è®¡ç®—çš„`alpha_t`å’Œ`beta_t`ï¼‰ã€‚  
>   - å¦‚æœç”¨äºè®­ç»ƒï¼Œè¿™ç›¸å½“äºå‘Šè¯‰æ¨¡å‹ï¼šâ€œè¯·å­¦ä¹ å¦‚ä½•ä»ç¬¬50æ­¥çš„çŠ¶æ€å»å™ªâ€ã€‚  
>   - å¦‚æœç”¨äºæ¨ç†ï¼ˆé‡‡æ ·ï¼‰ï¼Œå¯èƒ½è¡¨ç¤ºä»ç¬¬50æ­¥å¼€å§‹åå‘ç”Ÿæˆå›¾åƒï¼ˆä½†é€šå¸¸é‡‡æ ·æ˜¯ä»`t=1000`å¼€å§‹ï¼‰ã€‚
>
> ---
>
> ### ä¸ºä»€ä¹ˆå¯ä»¥ä¸åŒï¼Ÿ
> 1. **æ—¶é—´æ­¥çš„ç‹¬ç«‹æ€§**ï¼š  
>    `num_train_timesteps`æ˜¯å…¨å±€å‚æ•°ï¼Œè€Œ`timesteps`æ˜¯å•æ¬¡æ“ä½œçš„å±€éƒ¨å‚æ•°ã€‚è°ƒåº¦å™¨ä¼šæ ¹æ®å…¨å±€æ­¥æ•°ï¼ˆ1000ï¼‰è®¡ç®—ç¬¬50æ­¥çš„å™ªå£°å¼ºåº¦ï¼Œä¸¤è€…æ˜¯å…¼å®¹çš„ã€‚
>
> 2. **å™ªå£°è°ƒåº¦çš„æ•°å­¦æ€§è´¨**ï¼š  
>    æ— è®º`timesteps`å–å€¼å¤šå°‘ï¼ˆå¦‚50æˆ–500ï¼‰ï¼Œè°ƒåº¦å™¨éƒ½ä¼šè‡ªåŠ¨æ ¹æ®`num_train_timesteps`å½’ä¸€åŒ–åˆ°ç›¸åŒçš„å™ªå£°å°ºåº¦èŒƒå›´ï¼ˆå¦‚çº¿æ€§æˆ–ä½™å¼¦è°ƒåº¦ï¼‰ã€‚ä¾‹å¦‚ï¼Œç¬¬50æ­¥çš„å™ªå£°é‡åœ¨1000æ­¥ä¸­å’Œåœ¨500æ­¥ä¸­çš„â€œç›¸å¯¹å¼ºåº¦â€å¯èƒ½ä¸åŒï¼Œä½†è°ƒåº¦å™¨ä¼šä¿è¯ä¸€è‡´æ€§ã€‚
>
> 3. **è®­ç»ƒæ—¶çš„éšæœºé‡‡æ ·**ï¼š  
>    åœ¨è®­ç»ƒä¸­ï¼Œ`timesteps`ä¼šè¢«éšæœºé‡‡æ ·ï¼ˆå¦‚`torch.randint(0, 1000, (batch_size,))`ï¼‰ï¼Œç›®çš„æ˜¯è®©æ¨¡å‹å­¦ä¹ æ‰€æœ‰æ—¶é—´æ­¥çš„å»å™ªèƒ½åŠ›ã€‚ä»£ç ä¸­çš„`50`åªæ˜¯ä¸€ä¸ªç¤ºä¾‹ã€‚
>
> ---
>
> ### æ˜¯å¦æœ‰å½±å“ï¼Ÿ
> - **æ— è´Ÿé¢å½±å“**ï¼šåªè¦`timesteps`ä¸è¶…è¿‡`num_train_timesteps`ï¼ˆå¦‚50 â‰¤ 1000ï¼‰ï¼Œæ“ä½œæ˜¯å®‰å…¨çš„ã€‚  
> - **è®­ç»ƒæ—¶çš„å…³é”®ç‚¹**ï¼š  
>   - å¦‚æœ`timesteps`æ€»æ˜¯å›ºå®šï¼ˆå¦‚æ€»æ˜¯50ï¼‰ï¼Œæ¨¡å‹å°†æ— æ³•å­¦ä¹ å…¶ä»–æ—¶é—´æ­¥çš„å»å™ªï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚å®é™…è®­ç»ƒä¸­åº”éšæœºé‡‡æ ·`timesteps`ã€‚  
>   - åœ¨æ¨ç†æ—¶ï¼Œé€šå¸¸éœ€è¦ä»`t=1000`é€æ­¥é™åˆ°`t=0`ï¼ˆå®Œæ•´é‡‡æ ·ï¼‰æˆ–ä½¿ç”¨æ›´å°‘çš„æ­¥æ•°ï¼ˆå¦‚DDIMçš„50æ­¥åŠ é€Ÿé‡‡æ ·ï¼‰ã€‚
>
> ---
>
> ### æ€»ç»“
> - `num_train_timesteps=1000`ï¼šå®šä¹‰å™ªå£°è°ƒåº¦çš„æ€»æ­¥æ•°ï¼ˆå…¨å±€å‚æ•°ï¼‰ã€‚  
> - `timesteps=50`ï¼šæŒ‡å®šå½“å‰æ“ä½œçš„ç‰¹å®šæ—¶é—´æ­¥ï¼ˆå±€éƒ¨å‚æ•°ï¼‰ï¼Œå…¶å™ªå£°å¼ºåº¦ç”±è°ƒåº¦å™¨æ ¹æ®å…¨å±€æ­¥æ•°è®¡ç®—ã€‚  
> - **æ­£ç¡®ç”¨æ³•**ï¼šè®­ç»ƒæ—¶éšæœºé‡‡æ ·`timesteps`ï¼Œæ¨ç†æ—¶æŒ‰éœ€é€‰æ‹©æ­¥æ•°ï¼ˆå¯å°‘äº`num_train_timesteps`ï¼‰ã€‚



### ï¼ˆ2ï¼‰è®­ç»ƒæ¨¡å‹

By now, you have most of the pieces to start training the model and all thatâ€™s left is putting everything together.

First, youâ€™ll need an optimizer and a learning rate scheduler. Then, youâ€™ll need a way to evaluate the model. For evaluation, you can use the [DDPMPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ddpm#diffusers.DDPMPipeline) to generate a batch of sample images and save it as a grid:

```python
# train the model
from diffusers.optimization import get_cosine_schedule_with_warmup

optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
lr_scheduler = get_cosine_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=config.lr_warmup_steps,
    num_training_steps=(len(train_dataloader) * config.num_epochs),
)

from diffusers import DDPMPipeline
from diffusers.utils import make_image_grid
import os

def evaluate(config, epoch, pipeline):
    # Sample some images from random noise (this is the backward diffusion process).
    # The default pipeline output type is `List[PIL.Image]`
    images = pipeline(
        batch_size=config.eval_batch_size,
        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Use a separate torch generator to avoid rewinding the random state of the main training loop
    ).images

    # Make a grid out of the images
    image_grid = make_image_grid(images, rows=4, cols=4)

    # Save the images
    test_dir = os.path.join(config.output_dir, "samples")
    os.makedirs(test_dir, exist_ok=True)
    image_grid.save(f"{test_dir}/{epoch:04d}.png")
```



### ï¼ˆ3ï¼‰æ€»çš„ä»£ç 

```python
from dataclasses import dataclass

@dataclass  # è£…é¥°å™¨ï¼Œdataclassçš„è£…é¥°å™¨å¯ä»¥è‡ªåŠ¨ç”Ÿæˆä¸€äº›ç‰¹æ®Šæ–¹æ³•ï¼Œæ¯”å¦‚__init__()ã€__repr__()ç­‰
class TrainingConfig:
    image_size = 128  # the generated image resolution
    train_batch_size = 16
    eval_batch_size = 16  # how many images to sample during evaluation
    num_epochs = 10
    gradient_accumulation_steps = 1 # è¡¨ç¤ºåœ¨æ¯ä¸ªbatchä¸Šç´¯ç§¯å¤šå°‘æ¬¡æ¢¯åº¦å†è¿›è¡Œä¸€æ¬¡åå‘ä¼ æ’­
    learning_rate = 1e-4
    lr_warmup_steps = 500
    save_image_epochs = 2
    save_model_epochs = 30
    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision
    output_dir = 'ddpm-butterflies-128'  # the model namy locally and on the HF Hub

    push_to_hub = False  # whether to upload the saved model to the HF Hub
    hub_private_repo = False  
    overwrite_output_dir = True  # overwrite the old model when re-running the notebook
    seed = 0

config = TrainingConfig()

# loading the dataset
from datasets import load_dataset
config.dataset_name = "huggan/flowers-102-categories"  # the dataset name
import globalConfig
dataset = load_dataset(
    config.dataset_name, 
    split="train", 
    token=globalConfig.huggingface_token, # è¿™é‡Œå¡«å†™huggingfaceçš„tokenï¼Œä¸ºäº†å®‰å…¨èµ·è§æ”¾åˆ°äº†å…¶ä»–æ–‡ä»¶ä¸­
    # download_mode="force_redownload" # æ³¨é‡Šæ‰ï¼Œå¦‚æœéœ€è¦é‡æ–°ä¸‹è½½æ•°æ®é›†å¯ä»¥æ‰“å¼€
    )  # load the dataset
print(dataset)

import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 4, figsize=(16, 4))
for i, image in enumerate(dataset[:4]["image"]):
    axs[i].imshow(image)
    axs[i].set_axis_off()
# fig.show()
# å¦‚æœéœ€è¦åœ¨linuxä¸Šä¿å­˜å›¾ç‰‡ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç 
plt.savefig("flowers.png", bbox_inches='tight', pad_inches=0.1)

# å¤„ç†å›¾ç‰‡
from torchvision import transforms

preprocess = transforms.Compose(
    [
        transforms.Resize((config.image_size, config.image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        # pytorchçš„transform.Normalizeçš„å…¬å¼æ˜¯ï¼šnormalize(x) = (x - mean) / stdï¼Œtransforms.ToTensor() ä¼šå°† PIL å›¾åƒæˆ– NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡ï¼Œå¹¶å°†åƒç´ å€¼ä» [0, 255] ç¼©æ”¾åˆ° [0, 1]ï¼ˆæµ®ç‚¹æ•°ï¼‰ã€‚
        # æ‰€ä»¥å°±ç›¸å½“äºæŠŠåƒç´ å€¼ä» [0, 255] ç¼©æ”¾åˆ° [-1, 1]ï¼Œmeanå’Œstdéƒ½æ˜¯0.5ï¼Œæ‰€ä»¥å°±ç›¸å½“äºæŠŠåƒç´ å€¼ä» [0, 255] ç¼©æ”¾åˆ° [-1, 1]
        transforms.Normalize([0.5], [0.5]), 
    ]
)

# Use ğŸ¤— Datasetsâ€™ set_transform method to apply the preprocess function on the fly during training:
def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    return {"images": images}

dataset.set_transform(transform)
import torch
train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)

# åˆ›å»ºUNet2DModel
from diffusers import UNet2DModel
model = UNet2DModel(
    sample_size=config.image_size,  # the target image resolution
    in_channels=3,  # the number of input channels, 3 for RGB images
    out_channels=3,  # the number of output channels
    layers_per_block=2,  # how many ResNet layers to use per UNet block
    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block
    down_block_types=(
        "DownBlock2D",  # a regular ResNet downsampling block
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",  # a ResNet downsampling block with spatial self-attention
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",  # a regular ResNet upsampling block
        "AttnUpBlock2D",  # a ResNet upsampling block with spatial self-attention
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)

# It is often a good idea to quickly check the sample image shape matches the model output shape:
sample_image = dataset[0]["images"].unsqueeze(0) # unsqueeze(0)è¡¨ç¤ºåœ¨ç¬¬0ç»´å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œå˜æˆ(1, 3, 128, 128)ï¼Œbatch_size=1
print("Input shape:", sample_image.shape) # Input shape: torch.Size([1, 3, 128, 128])
print("Output shape:", model(sample_image, timestep=0).sample.shape) # Output shape: torch.Size([1, 3, 128, 128])

import torch
from PIL import Image
from diffusers import DDPMScheduler

noise_scheduler = DDPMScheduler(num_train_timesteps=1000) # ï¼ˆ1ï¼‰
noise = torch.randn(sample_image.shape)
# æš‚æ—¶ä»…ç”¨äºè§‚å¯Ÿä¸åŒstepåŠ å™ªå£°çš„æ•ˆæœ
timesteps = torch.LongTensor([800]) # ï¼ˆ2ï¼‰è¿™é‡Œçš„timestepsæ˜¯ä¸€ä¸ªä¸€ç»´çš„tensorï¼Œè¡¨ç¤ºå½“å‰çš„æ—¶é—´æ­¥ï¼Œå€¼ä¸º50, LongTensorè¡¨ç¤ºé•¿æ•´å‹çš„tensor
noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)

# ä»¥ä¸‹ä»£ç çš„æ„æ€æ˜¯æŠŠnoisy_imageçš„èŒƒå›´ä»[-1, 1]ç¼©æ”¾åˆ°[0, 255]ï¼Œç„¶åè½¬æ¢ä¸ºuint8ç±»å‹çš„numpyæ•°ç»„ï¼Œæœ€åè½¬æ¢ä¸ºPILå›¾åƒï¼Œpermuteå‡½æ•°æ˜¯ä¸ºäº†æŠŠç»´åº¦ä»(1, 3, 128, 128)å˜æˆ(1ï¼Œ128, 128, 3)ï¼Œ*255æ˜¯ä¸ºäº†æŠŠèŒƒå›´ä»[-1, 1]ç¼©æ”¾åˆ°[0, 255]ï¼Œto(torch.uint8)è½¬æ¢ä¸ºuint8ç±»å‹
Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])
# ä¿å­˜å›¾ç‰‡ä¸ºpngæ ¼å¼
Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0]).save("noisy_image.png")

# train the model
from diffusers.optimization import get_cosine_schedule_with_warmup

optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
lr_scheduler = get_cosine_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=config.lr_warmup_steps,
    num_training_steps=(len(train_dataloader) * config.num_epochs),
)

from diffusers import DDPMPipeline
from diffusers.utils import make_image_grid
import os

def evaluate(config, epoch, pipeline):
    # Sample some images from random noise (this is the backward diffusion process).
    # The default pipeline output type is `List[PIL.Image]`
    images = pipeline(
        batch_size=config.eval_batch_size,
        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Use a separate torch generator to avoid rewinding the random state of the main training loop
    ).images

    # Make a grid out of the images
    image_grid = make_image_grid(images, rows=4, cols=4)

    # Save the images
    test_dir = os.path.join(config.output_dir, "samples")
    os.makedirs(test_dir, exist_ok=True)
    image_grid.save(f"{test_dir}/{epoch:04d}.png")

from accelerate import Accelerator # accelerateæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿè®­ç»ƒçš„åº“,åŒ…å«äº†åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦è®­ç»ƒçš„åŠŸèƒ½
from huggingface_hub import create_repo, upload_folder
from tqdm.auto import tqdm
from pathlib import Path
import os
import torch.nn.functional as F

def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):
    # Initialize accelerator and tensorboard logging
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="tensorboard",
        project_dir=os.path.join(config.output_dir, "logs"),
    )
    if accelerator.is_main_process:
        if config.output_dir is not None:
            os.makedirs(config.output_dir, exist_ok=True)
        if config.push_to_hub:
            repo_id = create_repo(
                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True
            ).repo_id
        accelerator.init_trackers("train_example")

    # Prepare everything
    # There is no specific order to remember, you just need to unpack the
    # objects in the same order you gave them to the prepare method.
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    global_step = 0

    # Now you train the model
    for epoch in range(config.num_epochs):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch}")

        for step, batch in enumerate(train_dataloader):
            clean_images = batch["images"]
            # Sample noise to add to the images
            noise = torch.randn(clean_images.shape, device=clean_images.device)
            bs = clean_images.shape[0]

            # Sample a random timestep for each image
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,
                dtype=torch.int64
            )

            # Add noise to the clean images according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            with accelerator.accumulate(model):
                # Predict the noise residual
                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]
                loss = F.mse_loss(noise_pred, noise)
                accelerator.backward(loss)

                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0], "step": global_step}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)
            global_step += 1

        # After each epoch you optionally sample some demo images with evaluate() and save the model
        if accelerator.is_main_process:
            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)

            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:
                evaluate(config, epoch, pipeline)

            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:
                if config.push_to_hub:
                    upload_folder(
                        repo_id=repo_id,
                        folder_path=config.output_dir,
                        commit_message=f"Epoch {epoch}",
                        ignore_patterns=["step_*", "epoch_*"],
                    )
                else:
                    pipeline.save_pretrained(config.output_dir)
    

# å¼€å§‹è®­ç»ƒ
if __name__ == "__main__":
    # training loop
    train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)
```





## 5.å¯è§†åŒ–Stable Diffusionçš„Attentionå±‚

å¯ä»¥çœ‹è¿™ä¸ªé“¾æ¥ï¼šhttps://github.com/castorini/daam?tab=readme-ov-file





#### è¡¥å……ï¼šæ¨¡å‹æƒé‡ä¸‹è½½

ä½¿ç”¨å¦‚ä¸‹çš„shellæŒ‡ä»¤æ˜¯åˆç†çš„ï¼š

```shell
huggingface-cli download --token hf_*** --resume-download meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf
```

