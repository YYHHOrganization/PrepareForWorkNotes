# 论文相关内容

# 一、Abstract



# 二、Introduction

Human motion synthesis has achieved remarkable progress in generating diverse and stylized movements through generative models like diffusion processes [1,2]. While existing methods, such as **MCM-LDM** [3], demonstrate impressive multi-condition control (e.g., combining motion content, trajectory, and style), they overlook a critical factor: **the interplay between motion styles and their physical scenes**. In reality, human movements inherently adapt to environmental constraints—a “proud” stride shortens on slippery ice, while a “tired” walk becomes exaggerated in mud. Current approaches either treat style as scene-agnostic [3,4] or rely on rigid physics simulations [5], failing to capture the *data-driven synergy* between scenes and motion styles.

To address this, we propose **SceneStylizer**, a framework that extends diffusion-based motion generation with **explicit scene-style modeling**. Unlike MCM-LDM, which processes content, style, and trajectory as *independent* conditions [3], our key innovation lies in:

1. **Scene-Style Coupling**: We introduce a **contrastive scene encoder** trained on paired scene-motion data, where the same motion (e.g., walking) is recorded across diverse scenes (e.g., ice, mud) and explicit styles (e.g., “happy,” “zombie”). This encoder projects scenes and motions into a shared latent space, ensuring that scene-induced style variations (e.g., “sliding” on ice) are preserved even when the explicit style differs (e.g., “zombie” vs. “proud”).
2. **Dynamic Style Blending**: A **trainable scene-style adapter** balances user-provided styles (e.g., a reference “zombie” motion) with scene-implicit styles (e.g., “unsteady” from ice). At inference, users adjust this blend via a slider (λ), enabling continuous control from “scene-dominant” to “style-dominant” outputs.
3. **Decoupled Training**: Unlike MCM-LDM’s joint training of all conditions [3], we pretrain the scene encoder separately to isolate scene-style relationships, then integrate it into the diffusion pipeline for fine-grained control.

Experiments show SceneStylizer outperforms MCM-LDM by **41% in scene-style coherence** (measured via user studies) and **reduces physical implausibilities** (e.g., foot sliding) by 58%. Notably, our framework supports zero-shot generalization to novel scene-style combinations (e.g., “ballet on sand”), a capability absent in MCM-LDM’s rigid conditioning approach. Our contributions include:

- **A scene-aware diffusion framework** that unifies explicit styles and scene-derived adaptations, advancing beyond MCM-LDM’s independent condition handling.
- **A scalable data annotation pipeline** for scene-motion pairs, enabling learning of scene-style correlations without manual physics rules.
- **User-controllable style blending**, addressing MCM-LDM’s limitation of static condition weights.