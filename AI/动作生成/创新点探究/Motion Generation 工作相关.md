# Motion Generation 工作相关

[GitHub - Foruck/Awesome-Human-Motion: An aggregation of human motion understanding research.](https://github.com/Foruck/Awesome-Human-Motion?tab=readme-ov-file#humanoid-simulated-or-real)



# Motion Editing

| 序号 | 文章题目+中稿会议                                            | 摘要翻译                                                     | 核心突出点                                                   | Method部分截图                                               | 是否开源           | 相关链接                                                     |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------ | ------------------------------------------------------------ |
| 1    | 《SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing》：**CVPR 2025** | 随着去噪扩散模型的兴起，文本驱动动作生成技术取得了显著进展。然而，==现有方法往往对骨骼关节、时间帧和文本词汇的表示进行了过度简化，导致其难以充分捕捉各模态内部信息及其交互关系。==此外，当将预训练模型应用于动作编辑等下游任务时，==通常需要额外的人工干预、优化或微调等操作。==本文提出的骨骼感知潜在扩散模型（SALAD）通过==显式建模关节、时间帧与文本词汇之间的复杂关联关系，实现了更精准的跨模态表征。==该模型的创新性在于：==利用生成过程中产生的交叉注意力图，仅需文本提示即可实现基于注意力机制的零样本动作编辑，无需任何额外用户输入或模型调整。==实验表明，我们的方法在保持生成质量的同时，文本-动作对齐性能显著优于现有方案，并通过提供多样化的编辑功能展现了超越生成任务的实际应用价值。项目代码详见项目主页。 | 1.显式建模多模态之间的关系，实现更精准的跨模态表征。<br>2.零样本动作编辑，仅需要文本提示； | ![image-20250518103424186](assets/image-20250518103424186.png) | :white_check_mark: | [SALAD](https://seokhyeonhong.github.io/projects/salad/)     |
| 2    | 《 MixerMDM: Learnable Composition of Human Motion Diffusion Models》：**CVPR 2025** | 在文本描述等条件引导下生成人体运动极具挑战性，这主要源于需要具备高质量运动及其对应条件配对的数据集。当追求更精细的生成控制时，这一难度进一步加大。为此，==先前的研究提出将多个在不同类型条件数据集上预训练的运动扩散模型相结合，从而实现多条件控制==。然而，==这些提出的融合策略忽视了结合生成过程的最佳方式可能依赖于每个预训练生成模型的特性以及具体的文本描述。==在此背景下，我们引入了MixerMDM，这是首个可学习的模型组合技术，用于结合预训练的文本条件人体运动扩散模型。与以往方法不同，==MixerMDM提供了一种动态混合策略，通过对抗训练学习如何根据驱动生成的条件集合来结合每个模型的去噪过程。通过使用MixerMDM结合单人和多人运动扩散模型，我们实现了对每个人动态的精细控制，以及对整体互动的调控。==此外，我们==提出了一种新的评估技术==，首次在该任务中通过计算混合生成运动与其条件之间的对齐度，以及MixerMDM根据待混合运动调整整个去噪过程中混合方式的能力，来衡量互动质量和个体质量。 | 1.提供动态混合策略（ dynamic mixing strategy）<br>2.提出了一种新的评估技术 | ![image-20250518104527578](assets/image-20250518104527578.png) | :no_entry:         | https://github.com/pabloruizponce/MixerMDM                   |
| 3    | 《AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models》：**CVPR 2025** | 尽管基于学习的运动中间帧生成技术近期取得了进展，==但一个关键限制却被忽视了：对角色特定数据集的依赖。==本文提出AnyMoLe方法，通过==利用视频扩散模型为任意角色生成运动中间帧，无需外部数据即可突破这一限制。==我们的方法采用两阶段帧生成流程以增强上下文理解。为了弥合真实世界与渲染角色动画之间的领域差距，我们提出ICAdapt技术——一种针对视频扩散模型的微调方法。此外，我们开发了"运动-视频模拟"优化技术，通过结合2D与3D感知特征，可为任意关节结构的角色实现无缝运动生成。**AnyMoLe在显著降低数据依赖性的同时，能生成流畅逼真的过渡动作，适用于广泛的运动中间帧生成任务。** | 1.两阶段帧生成流程，在网络结构上做创新（比如针对video diffusion的finetune方法）<br>2.实现广泛的（比如各种生物的）运动中间帧生成 | ![image-20250518105137299](assets/image-20250518105137299.png) | :white_check_mark: | [AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models](https://kwanyun.github.io/AnyMoLe_page/) |
| 4    | 《SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction》**CVPR 2025** | 基于文本的3D人体运动编辑是计算机视觉与图形学领域关键而富有挑战性的任务。尽管已有研究探索了免训练方法，但近期发布的MotionFix数据集（包含源文本-运动三元组）为训练范式开辟了新途径，并展现出良好效果。==然而现有方法在精确控制方面存在不足，常导致运动语义与语言指令的错位。本文提出关联任务"运动相似性预测"，并设计多任务训练范式，通过联合训练运动编辑与运动相似性预测任务来促进语义表征学习。==为此我们进一步构建了先进的Diffusion-Transformer混合架构，分别处理运动相似性预测与运动编辑任务。大量实验表明，该方法在编辑对齐度与保真度方面均达到最先进水平。 | 1.网络结构上的创新，提出多任务训练框架以提升模型的运动编辑能力<br>2.遵循以下核心思想：要实现运动编辑，模型必须首先识别需要修改的部分。换言之，**给定源运动与编辑后运动，模型应能量化二者间的相似程度。** | ![image-20250518110701936](assets/image-20250518110701936.png) | :white_check_mark: | https://ideas.cs.purdue.edu/research/projects/sim-motion-edit/ |
| 5    | 《Dynamic Motion Blending for Versatile Motion Editing》**CVPR 2025** | 文本引导的运动编辑技术实现了超越传统关键帧动画的高层语义控制与迭代修改能力。==现有方法依赖有限的预收集训练三元组（原始动作、编辑后动作及指令），这严重限制了其在多样化编辑场景中的适用性。==我们提出MotionCutMix——一种==通过基于输入文本混合身体部位动作来动态生成训练三元组的在线数据增强技术。==尽管MotionCutMix能有效扩展训练数据分布，但其组合特性会引入更强的随机性及潜在的身体部位不协调问题。为建模此类复杂分布，我们开发了MotionReFit：==一种配备动作协调器的自回归扩散模型。==该自回归架构通过分解长序列来降低学习难度，而动作协调器则能有效缓解动作组合产生的伪影。我们的方法可直接根据高层人类指令处理时空维度的运动编辑，==无需依赖额外规范或大型语言模型（LLMs）==。大量实验表明，MotionReFit在文本引导运动编辑任务中达到最先进性能。消融研究进一步证实，MotionCutMix在保持训练收敛性的同时显著提升了模型的泛化能力。 | 1.提出了一种数据增强技术（**通过混合身体部位动态生成训练三元组**），解决了原来的数据集限制造成的多样化编辑能力；<br>2.提出了MotionReFit：动作协调器，缓解动作组合造成的伪影；<br>3.无需依赖额外规范或者LLM | ![image-20250518111843049](assets/image-20250518111843049.png) | :white_check_mark: | [Dynamic Motion Blending for Versatile Motion Editing](https://awfuact.github.io/motionrefit/) |
| 6    | [MotionFix](https://motionfix.is.tue.mpg.de/): Text-Driven 3D Human Motion Editing **Siggraph Asia 2024** | The **MotionFix dataset** **is the first benchmark for 3D human motion editing from text**.<br/>It contains triplets of **source** and **target** motions, and **edit texts** that describe the desired modification.本文聚焦于三维运动编辑任务，其核心目标是：给定一个三维人体运动序列及描述修改需求的文本，生成符合文本要求的编辑后运动。该任务面临两大挑战：==训练数据匮乏问题以及如何设计能忠实编辑源运动的模型。==针对这两个挑战，我们提出了一套==半自动化收集三元组数据集的方法（包含源运动、目标运动和编辑文本），并构建了新的MotionFix数据集。==基于该数据集，我们训练了==条件扩散模型TMED，该模型以源运动和编辑文本作为联合输入。==此外，我们构建了仅使用文本-运动配对数据集训练的多种基线模型，实验表明==基于三元组训练的模型性能更优。==我们提出了基于检索的新型运动编辑评估指标，并在MotionFix评估集上建立了新基准。实验结果展现出良好前景，为细粒度运动生成研究开辟了新方向。代码、模型及数据详见项目网站。 |                                                              | ![image-20250518113817361](assets/image-20250518113817361.png) | :white_check_mark: |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |
|      |                                                              |                                                              |                                                              |                                                              |                    |                                                              |

其他的挂在ArXiv上的文章，没开源：

- **(ArXiv 2025)** [StableMotion](https://arxiv.org/pdf/2505.03154): Training Motion Cleanup Models with Unpaired Corrupted Data, Mu et al.
- **(ArXiv 2025)** [PRIMAL](https://yz-cnsdqz.github.io/eigenmotion/PRIMAL/): Physically Reactive and Interactive Motor Model for Avatar Learning, Zhang et al.
- **(ArXiv 2025)** [Dai et al](https://arxiv.org/pdf/2503.08180): Towards Synthesized and Editable Motion In-Betweening Through Part-Wise Phase Representation, Dai et al.