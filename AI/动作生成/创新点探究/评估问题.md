好的谢谢老师，对于场景来说，现有的数据集可能不支持，感觉可以考虑用现成的数据集+人工标注，比如HumanML3D去做场景标签扩展。

或者在网络上爬取一些视频资源+动作提取，构建一个小的数据集，然后用fid做指标。

clip（motionClip）也可以评估动作和文本（包括风格和场景）的匹配度，或者也可以增加人工评估。

还有可以看模型输出的zc动作和zs风格以及zscene场景 的相似度（比如余弦相似度），如果接近0，可以说明解耦成功。

可以固定这几项的两个，换其中一个，比如固定zc动作，变化zs风格，评估是不是生成的动作不变但风格变了。

还有就是比较普遍的看物理合理性，脚打滑，关节有没有在阈值内那些。