Synthesizing Physically Plausible Human Motions in 3D Scenes 合成3D场景中物理上合理的人类动作

任务：输入一段文字，生成一段人物跑向椅子并坐下的动画。
场景中提供椅子和椅子坐标。
然后能生成动画

现在的方法可以做到文字生成人物动画，但是文字里没办法指定人物跟某个物体的交互

怎么让生成的动画满足物体交互的描述







### 合成3D场景中物理上合理的人类动作

主页：

file:///D:/_LabProj/MotionGenerate/TryTest3DSceneHumanMotion/InterScene/docs/index.html

效果

可以去不同的物体分别交互

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240904122524986.png" alt="image-20240904122524986" style="zoom: 33%;" />

可以避障

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240904122506539.png" alt="image-20240904122506539" style="zoom:50%;" />

code：有

![image-20240904122133890](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904122133890.png)

Figure 1. Our framework enables physically simulated characters to perform long-term interaction tasks in diverse and complex 3D scenes via composing reusable skills that include sitting (gray), getting up (blue), and avoding obstacles (red)  



**摘要**

​	在3D场景中合成物理上合理的人类动作是一个具有挑战性的问题。基于运动学的方法由于缺乏物理约束，无法避免固有的伪影（例如，穿透和脚滑）。与此同时，现有的基于物理的方法由于使用强化学习训练的策略具有有限的建模能力，无法推广到多物体场景。在本研究中，我们提出了一个框架，使得物理模拟角色能够在多样、杂乱和未知的场景中执行长期交互任务。关键思想是将人类与场景的交互分解为两个基本过程：**交互和导航**，这激励我们构建两个可重用的控制器，即**InterCon和NavCon**。具体而言，InterCon包含两个互补的策略，使角色能够**进入和离开交互状态**（例如，坐在椅子上和起身）。为了在不同位置与物体生成交互，我们进一步设计了NavCon，一个轨迹跟踪策略，以保持角色在3D场景自由空间中的运动。得益于**分而治之**的策略，我们能够在简单环境中训练这些策略，并推广到复杂的多物体场景。

​	实验结果表明，我们的框架能够在复杂的3D场景中合成物理上合理的长期人类动作。代码将在https://github.com/liangpan99/InterScene上公开发布。

### 1. 引言

​	在日常生活场景中创建具有多样运动技能（例如，行走、坐下、从坐姿站起）和丰富人类与场景交互的虚拟角色，仍然是计算机视觉和图形学中的一项基本任务。尽管之前的研究 [5, 9, 20, 27, 28, 31, 33] 在人类与场景交互的长期动作生成方面取得了进展，但这些模型仍然存在物理伪影的问题，例如表面穿透和脚滑。最近的工作 [1, 6] 开始结合物理仿真器来合成物理上合理的动作。然而，它们的框架仍然局限于与单个对象的交互，因为使用强化学习训练的策略具有有限的建模能力，因此导致生成的动作与复杂3D场景中的现实人类行为之间存在差距。

​	为了弥补这种差距，我们的工作致力于使物理模拟的角色能够在杂乱的多物体环境中执行长期交互任务。我们框架的关键见解是构建两个可重用的控制器，即InterCon和NavCon，以建模人类与场景交互中的两个基本过程。**InterCon学习与物体交互的技能**，而**NavCon则控制角色沿无障碍路径的移动**。我们的框架的优势在于：1）它将长期交互任务分解为两个控制器的调度问题；2）两个控制器可以在相对简单的环境中进行训练，而不依赖于昂贵的3D场景数据；3）训练好的控制器可以直接推广到复杂的多物体场景，而无需额外的训练。

​	尽管现有的工作 [1, 6] 提出了用于执行交互任务的控制器，但由于交互建模的不完整性，这些控制器无法应用于多物体场景。为了解决这个问题，我们的InterCon采用两种互补的控制策略来学习完整的交互技能。与之前的工作 [1, 6] 不同，InterCon不仅涉及到接触和与物体交互，还包括离开交互对象。如图1（左）所示，利用这两种策略确保InterCon是一个闭环控制器，可以实现与多个对象的交互。在如图1（右）所示的有障碍物的杂乱环境中，我们进一步引入NavCon，使角色能够在复杂的多物体场景中导航并避开障碍物。InterCon和NavCon提供完整的交互技能，包括坐下、起身和无障碍轨迹跟随。因此，我们可以利用有限状态机来调度这两个控制器，使角色能够在复杂的3D场景中执行长期交互任务，而无需额外的训练。

​	我们通过目标条件强化学习和对抗性动作先验（AMP）[17] 来训练所有策略。因为策略需要协调角色与物体之间的细粒度移动，并且奖励稀疏，所以稳定地训练InterCon是一项挑战。之前的工作 [6] 将AMP的判别器条件化于场景上下文，以构造密集的奖励。在本研究中，我们提出了交互早期终止的方法，以通过平衡训练样本的数据分布实现稳定训练。为了确保我们的InterCon能够推广到未见过的对象，需要大量形状各异的对象用于训练 [6]。然而，起身策略依赖于与物体接触的各种坐姿，这些坐姿必须是合理的（例如，不应漂浮或穿透），而获得这些姿势是困难的。为了解决这个问题，我们引入了坐姿采样，其中训练好的坐下策略将被用来生成合理的坐姿，而不会增加动作捕捉的额外成本。

​	总之，我们的主要贡献包括：

1. 我们提出了一个框架，使得物理模拟的角色能够在多样、杂乱和未知的3D场景中执行长期交互任务。
2. 我们提出了两个可重用的控制器，用于建模交互和导航，从而分解具有挑战性的人类与场景的交互。
3. 我们利用有限状态机，使用户能够通过直观的指令获得期望的人类与场景交互，而无需额外训练。

### 2. 相关工作

##### 2.1 Human motion generation in 3D scenes.   

​	在3D场景中生成人体运动的研究在计算机视觉和图形学领域得到了广泛关注。创建能够与周围环境互动的虚拟角色是该领域的重要目标之一。方法主要有两个方向：一种是利用大规模动作捕捉数据集构建数据驱动的运动模型 [13]；另一种是基于神经网络的方法，其中**阶段性神经网络（Phase-based neural networks）**被广泛用于生成自然且逼真的人类动作 [7, 20-22]。例如，Holden等人 [7] 提出了阶段功能神经网络，能够生成适应粗糙地形的角色运动。Strake等人 [20] 扩展了阶段变量的概念，在人类与场景交互的情境下（如坐在椅子上和搬运箱子）生成运动。Strake等人 [21] 还提出了一种基于局部运动阶段模型的方案，用于合成丰富接触的交互。

​	另一种方法是使用**生成模型**，比如**条件变分自编码器（cVAE）**，来模拟人类交互行为。Wang等人 [27] 提出了一种层次化生成框架，通过3D场景结构来合成长期3D运动。Hassan等人 [5] 提出了一个随机场景感知运动生成框架，使用两个cVAE模型学习目标位置和人类运动流形。Wang等人 [28] 提出的框架则聚焦于合成多样的场景感知人类运动。Taheri等人 [24] 和Wu等人 [29] 设计了类似的框架，以生成全身交互运动。最近的研究开始采用**强化学习（RL）**来开发运动生成的控制策略。MotionVAE [10] 是一个具有代表性的工作，提出了一种基于RL和生成模型的新范式用于运动生成。Zhang等人 [32] 将MotionVAE扩展到合成多样的数字人类在3D场景中的表现。Zhao等人 [33] 则提出了交互策略和移动策略，用于合成3D室内场景中的人类运动。Lee等人 [9] 同样使用强化学习与运动匹配相结合来解决3D场景中的运动、动作和操控任务。

​	本研究旨在合成与日常室内物体（如椅子、沙发和小凳子）进行交互的3D运动。最相关的前期工作包括基于阶段的神经网络 [20]、基于cVAE的生成模型 [5, 27, 28] 以及基于RL的方法 [9, 33]。这些研究为我们当前的工作提供了重要的基础和背景，使我们能够在复杂的3D场景中实现更加自然的人类与物体交互的模拟。

##### 2.2 物理驱动的运动生成

​	物理驱动的范式使用运动控制和物理模拟器 [14, 26] 来保证生成运动的物理合理性。大多数努力集中在提高物理模拟角色运动的自然性上。基于追踪的运动模仿技术 [3, 11, 12, 16] 使得模拟角色能够模仿多样化、具有挑战性且自然的运动技能。然而，基于追踪的方法依赖于高质量的参考动作，而在运动生成任务中获取如此高质量的数据往往是具有挑战性的。

​	最近，采用对抗性运动先验 [17] 的研究提出了一种生成运动模仿学习框架，其中训练了一个运动鉴别器，以取代复杂的基于追踪的目标。基于AMP（Adversarial Motion Prior）的方法 [8, 18, 25] 在面向任务的运动生成中取得了显著成果。最新的研究 [6] 首次扩展了AMP框架以合成角色与场景的交互，例如坐在椅子上、躺在沙发上以及搬运箱子等。

​	此外，类似的基于追踪的工作 [1, 30] 也专注于角色与场景的交互。这些研究为物理驱动的运动生成提供了强有力的支持，强调了如何通过结合物理模拟与机器学习方法，提升生成动作的自然性和真实性。这样的融合不仅能够改善角色表现，还能有效地处理复杂的环境交互，使虚拟角色在动态场景中更具真实感。

### 3. 方法

#### 3.1. 概述



​	如图2所示，我们的框架集成了两个可重用的控制器：**交互控制器（Interaction Controller, InterCon）和导航控制器（Navigation Controller, NavCon）**。前者作为低级执行器，后者作为高层规划器，通过**有限状态机（Finite State Machine, FSM）**调度这两个执行器，根据**用户指令**合成人体运动。InterCon由两个控制策略组成，包括坐下策略 ( $\pi_s$ ) 和起立策略 ( $\pi_g$ )。每个策略都基于目标物体特征 ( $g_{t}^{bbox} $) 和角色根部的目标位置 ( $g_{t}^{pos} $ ) 进行条件设置。NavCon 包含一个轨迹跟随策略 ( $\pi_f$ )，其条件基于目标轨迹特征 ( $g_{t}^{traj} $ )。这些输入条件可以看作是显式的控制信号。

​	我们使用图2中展示的杂乱3D场景 ( W )，该场景包含三个可交互物体 ( $W = {w_1, w_2, w_3} $)，作为描述我们框架工作流程的例子。基于三种控制策略，我们的FSM为用户提供了三种可重用技能，包括坐下 ($ k_s$ )、起立 ($ k_g $) 和轨迹跟随 ( $k_f $)。

​	为了合成描述为“一个人首先坐在第一个椅子上，然后坐在第二个椅子上，最后在沙发上休息”的人体运动，用户需要构建以下指令：
$$
[ I = {(k_s, w_1), (k_g, w_1), (k_f, h_1), (k_s, w_2), (k_g, w_2), (k_f, h_2), (k_s, w_3)} ]
$$
​	其中，($ (k_s/k_g, w) $) 表示角色执行坐下技能 ($ k_s $) 在物体 ( w ) 上就坐，或执行起立技能 ($ k_g$ ) 离开物体 ( w )；而 ($ (k_f, h) $) 则表示角色沿着一条无障碍的轨迹 ( h ) 移动，该轨迹可以通过**A*路径规划算法** [4] 生成或由用户定义。FSM将此指令转换为一系列显式控制信号，然后调度控制策略执行指令，而无需额外的训练。FSM还决定何时在技能之间进行切换。例如，当角色的根部与目标位置重叠的时间超过特定时间时，FSM将开始执行下一个技能。引入这样一个简单的基于规则的FSM使用户能够在复杂的3D场景中获得所需的长期人体运动。



![image-20240904151839594](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904151839594.png)

图2. 我们框架的概述

- **交互控制器（Interaction Controller）**：
  - 由两个独立的控制策略组成。
  - 提供两种涉及交互的技能：坐下和起立。
- **导航控制器（Navigation Controller）**：
  - 使用轨迹跟随策略，控制角色沿特定路径的移动。

通过将这两个可重用的控制器结合在一起，我们能够在复杂的3D场景中合成人体运动，而无需额外的训练。这是通过使用有限状态机（FSM）实现的，该机器接收用户指令，使模拟角色能够执行长期交互任务。

这种集成方法使得在动态和复杂环境中生成自然、连贯的人类动作成为可能。



### 3.2. 强化学习背景

我们将运动合成形式化为目标条件的强化学习。在每个时间步骤 ( t ) 中，策略 ( $\pi(a_t | s_t, g_t)$ ) 根据从环境中观察到的当前状态 ( $s_t$ ) 和任务特定的目标特征 ( $g_t$ ) 输出一个动作 ( $a_t $)。将该动作应用于角色后，环境根据环境的动态变化转移到下一个状态 ( $s_{t+1} $)，这种转移通过 ( $p(s_{t+1} | s_t, a_t)$ ) 表示。模拟状态的轨迹描述了一系列生成的人体姿势，表示为 ( $S = {s_0, \ldots, s_{t-1}, s_t} $)。状态 ( s ) 描述了角色身体的配置，共包含125维特征，包括：

- 根部高度（1D）
- 根部旋转（6D）
- 根部线性和角速度（6D）
- 局部关节旋转（72D）
- 局部关节速度（28D）
- 关键关节位置：手和脚（12D）

根部高度记录在世界坐标系中，其他特征则是在角色的局部坐标系中计算的。旋转采用6D表示法 [34]。模拟角色与文献 [6, 17, 18] 中的角色相同，具有12个可移动的内部关节，总共28个自由度。策略输出的动作 ( $a \in \mathbb{R}^{28} $) 指定了各关节PD控制器的目标方向。

我们的框架包含三种控制策略，即坐下策略 ( $\pi_s$ )、起立策略 ($ \pi_g $) 和轨迹跟随策略 ($ \pi_f $)。为了使它们能够自然且生动地控制物理模拟角色，我们使用了对抗运动先验（Adversarial Motion Priors, AMP）框架 [17]。AMP 使角色能够组合大型非结构化运动数据集中描绘的不同技能，以执行任务。策略 ($ \pi $) 的训练目标是最大化期望折扣回报 ($ J(\pi)$ )，其中奖励 ( r ) 的公式为：

$$[ r = w_G r_G + w_S r_S. ] -------------------(2)$$

任务奖励 ($ r_G $) 鼓励角色完成所需任务，而风格奖励 ($ r_S $) 则鼓励角色产生与数据集相似的行为，该奖励由基于连续10步状态的判别器 ( $D(s_{t-10:t}) $) 计算得出。更多细节可以参考文献 [17]。

我们的框架涉及三个任务场景，即在椅子上坐下、从坐着的位置起立，以及跟随轨迹。在后续部分中，我们将描述每个任务场景的目标特征 ( g ) 和任务奖励函数 ( $r_G$ )。

![image-20240904152437488](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904152437488.png)

图3. InterCon中两个控制策略的训练方案

该训练方案包括三个步骤：

1. **训练坐下策略**：
   - 首先，我们使用对抗运动先验（Adversarial Motion Priors, AMP）框架来训练一个有效的坐下策略。
2. **引入坐姿采样**：
   - 由于缺乏高质量的坐姿数据，我们引入了坐姿采样。在这一阶段，已训练好的坐下策略被用来生成各种坐姿。
3. **训练起立策略**：
   - 最后，我们开始训练起立策略。在这一过程的初始化阶段，利用合成的坐姿来初始化角色，以便在每个回合开始时提供合适的状态。

这个训练方案有效地解决了数据不足的问题，并通过生成多样化的坐姿来增强模型的表现。



### 3.3 交互控制器

给定一个目标物体 ( $w_i \in W$ )，交互控制器（InterCon）使角色能够进入或离开与该物体的交互状态。以往基于物理的方法 [1, 6] 主要集中在发展角色的前者能力，而忽视了后者在长期交互任务中的重要性。通过学习起身的技能，InterCon 可以将坐着的角色恢复到静止站立状态，这为继续执行新的交互任务提供了机会。如图2所示，InterCon 由两个独立的控制策略组成，即坐下策略 ( $\pi_s$ ) 和起身策略 ( $\pi_g$ )。每个策略共享相同的状态 ( $s_t$ )、目标特征![image-20240904160229961](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904160229961.png)和网络结构。为了使策略能够感知目标物体，我们**根据目标物体的 3D 特征 ( $ g_{t}^{bbox}  \in R^{24+2} $) 进行条件设置，该特征包含 8 个边界框的顶点和一个表示朝向的水平向量**。这对于策略有效地学习如何协调角色的运动与目标物体之间的关系至关重要。特别是，我们将角色根部的显式目标位置 ( $g_{t}^{pos} $ ) 添加到目标特征 ($ g_t $) 中。坐下策略的目标位置设置在椅子座面的顶部中心上方 5 厘米处，而起身策略的目标位置则通过以下公式计算：

$$[\Large g_{pos_t} = \left[ \frac{j_{lxfoot} + j_{rxfoot}}{2}, \frac{j_{lyfoot} + j_{ryfoot}}{2}, 0.89 \right] ]$$

![image-20240904155925779](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904155925779.png)

其中 ( j_i ) 表示第 ( i ) 个关节的 3D 位置，我们使用稳定的坐姿来计算起身策略的 ( $g_{t}^{pos} $ )。所有这些目标特征都记录在角色的局部坐标系中。



**训练。**我们两种控制策略的训练方案如图3所示。虽然所提出的 InterCon 应用于复杂的多对象环境中，但我们在相对简单的单对象环境中训练这些策略，无需依赖昂贵的 3D 场景数据。类似于 [6]，我们遵循标准的 AMP 框架，并将其扩展到角色场景交互任务，以训练 InterCon 中的每个策略。我们首先描述坐下策略的训练过程。

- 运动和物体数据集。我们使用来自 SAMP 数据集 [5] 的 20 个序列（chair mo001 ∼ chair mo019）。由于 SAMP 数据集提供通过 SMPL-X [15] 参数表示的动作，我们将动作重定向到我们的模拟角色。为了确保训练的策略能够很好地泛化到新物体，我们按照 [6] 的方法从 **3D-Front 数据集 [2] 中选择 57 把形状和规模各异的椅子**，并随机将 50 把作为训练集，7 把作为测试集。
- 初始化。在每个训练回合开始时，从训练集中随机抽取物体，并将其初始化到 SAMP 数据集记录的固定位置和旋转。对于角色，我们采用两种方法进行初始化。一种是参考状态初始化 [16]，另一种是将角色放置在具有默认姿势的情况下，与物体的 2D 中心保持一到五米的随机全局旋转。
- 奖励。总奖励在公式(2)中定义。样式奖励使用运动鉴别器建模。与 [6] 不同，我们不根据物体上下文来条件鉴别器。给定角色根部的目标位置  ( $g_{t}^{pos} $ ) 和目标标量速度  ( $g_{t}^{vel} $ )，任务奖励定义为：



![image-20240904155832490](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904155832490.png)

![image-20240904160417662](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904160417662.png)

其中 ( $x_{\text{root}, t} $) 是角色根部的三维位置，($ x^{*}$ ) 是物体中心的三维位置，($ d^{*}$ ) 是一个水平单位向量，指向从 ($ x_{\text{root}, t} $) 到 ( $x^{*} $) 的方向，$ d_{\text{facing}, t} $ 是角色面向方向的水平单位向量，$ a \cdot b $ 表示向量的点积。

- **重置和早期终止条件**：每一集（episode）将在固定的时间长度后终止，或当触发早期终止（ET）条件时结束。该集的时间长度设定为10秒。我们将摔倒检测作为ET条件之一。我们引入了一种新的条件称为交互早期终止（IET）。当**角色根部 $ x_{\text{root}, t} $ 和目标位置 $ g_{\text{pos}, t} $ 之间的累积重叠时间超过固定的时间段时**，IET将**终止**当前的集。实验结果表明，这个简单的IET机制可以有效地稳定强化学习的训练过程。



**训练起身策略**.如图3所示，我们在坐下策略训练完成后开始训练起身策略。我们保留上述大部分设置，除了初始化部分。为了在每个集的开始初始化角色，我们依赖于与物体有合理接触的多样化坐姿，确保没有悬浮或穿透。这类高质量数据由于训练集中物体数量庞大而难以获取。因此，我们利用已训练好的坐下策略来生成大量符合物理规律的坐姿。这是成功训练起身策略的关键。

### 3.4 导航控制器

虽然 InterCon 能够合成长期交互动作，但它无法在拥挤的 3D 场景中避免障碍物。为了展示一个能够进行无障碍行走的模拟角色，我们提出了 NavCon，它包含一个轨迹跟随策略 $\pi_f(a_t | s_t, g_{\text{traj,t}})$，如图 2 所示。我们遵循文献 [19] 来构造目标特征，通过一条短轨迹 $g_{\text{traj},t} \in \mathbb{R}^{10 \times 2}$，该轨迹由角色根部在未来 2.5 秒内的多个 2D 位置信息构成，这些位置以 0.25 秒的间隔进行采样。

在以往的研究中，采用了获取无碰撞人类运动的模块 [5, 28, 33]。然而，基于运动学的方法会产生脚滑动伪影。基于物理的轨迹跟随方法 [19] 可以生成角色的物理合理运动。在本工作中，我们首次将这一技术应用于角色与场景的交互任务。使用 NavCon 使我们的框架成为一个闭环框架，可以在与人类场景的交互场景中合成长期人类动作，并为用户提供生成自定义行走轨迹的人类场景交互接口。

这段文字介绍了导航控制器的设计和实现，强调了在复杂环境下角色的移动能力以及与场景交互的必要性。通过引入 NavCon，系统能够更有效地处理动态交互情境，提高了整体的模拟真实感和灵活性。

### 训练

我们依据文献 [19] 提供的设置训练轨迹跟随策略。在我们的情况下，策略是在平坦地面的简单环境中进行训练，而不同于 [19] 中使用复杂地形的方式。

- **运动和轨迹数据集**：我们使用了来自 AMASS 数据集 [13] 的约 200 条序列。我们还将运动序列重新定位到我们使用的角色上。我们遵循 [19] 的方法，程序性地生成用于训练的合成轨迹。完整的轨迹 $\tau = {p_{\tau 0}, \ldots, p_{\tau T -1}, p_{\tau T}}$ 被建模为一组以固定 0.1 秒时间间隔的 2D 点。在每个时间步 $t$ 上，我们通过插值查询未来 2.5 秒内的 10 个点 ${p_{\tau t}, \ldots, p_{\tau t+9}}$，以构建目标特征 $g_{\text{traj}}$。
- **其他设置**：当角色摔倒或偏离既定轨迹过远时，我们会终止训练过程。角色总是初始化为固定的默认状态。当环境重置时，轨迹将被重新计算。任务奖励 $r_G^t$ 测量角色根部 $x_{\text{root}}^t$ 在水平面上与期望位置 $p_{\tau t} \in \tau$ 的距离，其公式如下： $ [ r_G^t = \exp(2.0 |x_{\text{root}}^t - p_{\tau t}|). ] $

这部分内容详细描述了轨迹跟随策略的训练过程，包括数据集的选择、轨迹的生成方法以及奖励机制的设计，强调了在训练阶段对环境和角色状态的控制。这些设置确保了角色能够有效学习在各种情境下的行走和导航能力。

### 4. 实验

#### 4.1 交互控制器的评估

InterCon 包含两个控制策略，分别负责执行坐下和起立任务。我们首先定量评估每个策略在其相应任务中的有效性。随后，我们对交互早期终止（IET）的超参数进行了消融研究。

我们通过测量成功率来评估模型性能，成功率表示角色完成任务的试验百分比。如果**角色根部与目标位置之间**的欧几里得距离小于 20 cm，则该试验被视为成功。此外，我们还使用以下指标评估成功试验的质量：1）执行时间，即角色完成任务所花费的平均时间；2）精度，即角色根部与其目标位置之间的平均距离。所有指标均是在每个任务中进行的 4096 次试验计算得出的。

这部分内容详细阐述了实验设计和评估方法，强调了通过成功率、执行时间和精度等多个维度来全面评估交互控制器的性能。这些评价标准确保了模型不仅能够成功完成任务，而且在效率和准确性上也能取得良好的表现。

### 3D 场景实验

在本节中，我们在单对象环境中进行实验。我们使用来自 3D-Front 数据集 [2] 的 7 张未见椅子。每个试验的环境初始化时，随机从数据集中抽取一个物体。

#### 坐下策略的有效性

我们评估坐下任务中的有效性。角色随机初始化于离物体 1 米至 5 米之间的任意位置，并具有随机方向。最大回合长度设置为 20 秒。我们采用 AMP [17] 作为基线，其中所有训练配置（例如状态、动作、奖励、数据集）与我们的方法相同，但早期终止条件不同。AMP 基线会在角色摔倒时终止回合。我们使用额外条件训练我们的坐下策略，即交互早期终止，将最大交互步骤设置为 30 步。表 1 显示了经验评估结果。我们报告了 NSM [20]、SAMP [5]、Chao 等 [1] 和 InterPhys [6] 提供的数据。我们用不同的随机种子训练我们的方法和基线三次，并报告最佳平均指标。我们的坐下策略实现了高成功率，并显著超过基线，这证明了我们坐下策略的有效性。

#### 起立策略的有效性

我们评估起立任务中的有效性。首先，我们使用训练好的坐下策略在 7 个测试物体上生成各种坐姿。由于角色可以面向三个不同的方向坐在椅子上，我们在每个方向上收集了 30 个样本，总计获得 7×3×30 个姿势。角色随机初始化为从 30 个对应于物体的坐姿中采样而来的坐姿。根据表 2，我们的起立策略在起立任务中可以达到高成功率。

![image-20240904163423937](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904163423937.png)

#### 关于 IET 的消融研究

我们多次训练坐下策略，使用不同的交互早期终止步骤参数。为了减少随机性，我们还对每个实验进行了三次训练。表 3 显示了定量结果，表明随着步骤数量的减少，模型性能逐渐改善。如图 4 所示，使用较小的步骤参数可以显著减少性能曲线的波动，并实现更好的收敛性和稳定性，这证明了我们新颖的交互早期终止方法可以有效稳定 InterCon 的训练过程

![image-20240904163451493](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904163451493.png)

### 4.2 完整系统的评估

尽管每个控制器是在单对象或无对象环境中训练的，但将它们结合起来，使我们的完整系统能够在多样化和复杂的 3D 场景中成功合成长期的人类运动，而无需额外训练。为了展示我们系统的有效性，我们在三个测试 3D 场景中应用了该系统。对于每个场景，我们手动设计了三个脚本，每个脚本包含一系列角色需要执行的交互任务。有限状态机将根据脚本调度两个控制器。

#### 3D 场景构建

我们构建了三个合成场景，以模拟日常室内环境。3D 物体网格来自 3D-Front 数据集 [2]。与第 4.1 节的实验不同，我们使用了更广泛的物体种类，其中一些甚至是分布外的。我们使用了 17 种物体（6 张直椅子、4 张扶手椅、3 张沙发、4 个凳子）。每个场景中都填充了 5 到 6 个可交互物体以及一些障碍物。

#### 结果

如图 5 所示，我们的完整系统能够成功合成期望的长期人类-场景交互运动。结果表明，我们的方法可以推广到多样化、杂乱和完全未见过的 3D 场景。我们还验证了 InterCon 能够将其学习到的交互技能推广到新物体，甚至对沙发等分布外物体也能适用。此外，NavCon 提供的避障能力有效地帮助模拟角色在不同地点与多个物体进行交互。

![image-20240904163652529](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904163652529.png)

### 4.3 导航控制器的消融研究

我们的系统依赖 NavCon 确保角色能够在杂乱环境中导航并避免障碍物。我们通过比较完整系统与没有 NavCon 的变体产生的结果来验证其重要性。如图 6 所示，在给定相同目标的情况下，完整系统首先使用 NavCon 提示角色通过跟踪无障碍路径接近物体，然后使用 InterCon 控制角色成功与物体交互。然而，变体只能利用 InterCon，这导致角色总是倾向于沿最短路径前往目标物体，从而使角色被障碍物卡住。因此，将 NavCon 引入我们的系统对于在复杂环境中合成动作至关重要。

![image-20240904164249349](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240904164249349.png)

### 5. 限制与未来工作

InterCon 的目的是学习涉及交互的技能。我们训练的 InterCon 主要集中在坐下行为。然而，人类与场景的交互还包括其他行为，例如躺下和触摸。目前的研究未涵盖这些动作。由于 InterCon 是一个即插即用模块，我们可以通过为每种动作添加新的交互控制器来扩展我们的框架，以容纳更多动作。根据以往的工作 [6]，为每个动作训练独立控制器是可行的。

NavCon 的轨迹跟随策略追踪预先规划的无障碍路径。然而，由于追踪误差，角色仍然可能因障碍物卡住，从而中断已建立的路径。我们将在未来的工作中结合动态路径规划算法 [23] 来解决这个问题。此外，我们使用固定的骨骼人形模型对虚拟角色进行建模。为了实现更逼真的表面接触效果，我们将在未来的工作中使用带皮肤的人体模型。

### 6. 结论

在本文中，我们提出了一种基于物理的框架，用于合成长时间的人类-场景交互运动。这是通过联合使用两个可重用的控制器，即 InterCon 和 NavCon 来简化长期运动生成任务。我们物理模拟的角色在多样化、杂乱和未见过的 3D 场景中，真实且自然地展示了交互和移动行为。



## 实验

![image-20240905150407544](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240905150407544.png)

但是接下来

![image-20240905150433524](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240905150433524.png)

这个似乎只有linux？

![image-20240905150447191](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240905150447191.png)