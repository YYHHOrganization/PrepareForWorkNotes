# 动作生成 运动生成



可以考虑的方向

更进一步优化或者综合化当前已有的多人运动+交互框架

希望最后是给一个剧本，多个人可以演绎，且场景中有物体



方向更新为 自然语言和任务驱动的动作生成（后面可以加上多人，环境等）



讨论

任务是输入一段文字：生成一段人物跑向椅子并坐下的动画。
场景中提供椅子和椅子坐标。
然后能生成动画

现在的方法可以做到文字生成人物动画，但是文字里没办法指定人物跟某个物体的交互
解决这个问题-怎么让生成的动画满足物体交互的描述



（过场动画，技能动画）

想法：可编辑，的动画

策划：要生成从站着到坐在椅子上，靠着躺下

编辑1：左脚往外伸直

编辑2：



或者说 用线稿+语言描述去生成一个动作



给定一个草图 / 火柴人  / 分镜 （火柴人这个有人做了 可以看论文）

生成动作



或者给定几帧，（前面的生成好了），通过语言去生成后面的帧



先给文本生成动作，然后再看看



在思考，先通过线稿/火柴人，生成单帧动作，然后结合文本和单帧动作进行插值动作生成



因为现在动作生成基本依赖于现有数据集

是否可以先通过2Dai 比如stable diffusion生成一张对应图片（骨骼），然后再根据图片生成骨骼

然后再生成动作

| 数据集  | 描述                             | 时间/ 投稿            | 论文名                                              | 本地笔记记录地址                                             |
| ------- | -------------------------------- | --------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| TRUMANS | 全新的人物-场景交互数据集TRUMANS | CVPR 2024 (highlight) | Scaling Up Dynamic Human-Scene Interaction Modeling | D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\Scaling Up Dynamic Human-Scene Interaction Modeling\Scaling Up Dynamic Human-Scene Interaction Modeling.md |
|         |                                  |                       |                                                     |                                                              |
|         |                                  |                       |                                                     |                                                              |



做成小程序，可以操作，比如可以拖动删帧，

可以通过拖拽（daz那种）控制关键帧



综合的

比如使用网上的图片，识别生成骨骼 / 或者文本先生成对应图片，然后图片识别骨骼（利用图片数据集更大，sd更好的优点）

然后 将骨骼作为中间帧输入



做一个数字人，然后GPT会生成他应该说的话，以及它的行为（动作），然后他去生成

比如数字人是个厨师，

然后跟他说，想吃苹果，他就会去冰箱拿苹果，然后削苹果之类的动作生成



或者说 其实一个人的动作只有 头，左右手，左右腿，身体，可以通过提示词，或者勾选页面，让其mask一部分，重新生成另一部分





##### 思考增加模块

以motionlcm为基础

1、轨迹生成与场景

目前的模型motionlcm+controlnet：更改了轨迹，可以生成适配轨迹的动作

思考可行性：

​	第一步，根据场景和文本，**生成**对应的**轨迹**，然后输入到motionlcm+controlnet，生成符合（场景）轨迹的动作。

​	需要场景编码。

​	这可能需要train轨迹生成。

​	也可以 更快即可	

目前：

已经测试了更改轨迹，动作可以适配上去

目前就是需要根据场景和文本 生成轨迹



有涉及到轨迹生成的论文



HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes https://zju3dv.github.io/text_scene_motion/

场景生成，是有数据集的，就是比如从A椅子走到B桌子，是由数据的，他就拿着这个去训练轨迹生成，所以也有损失函数

此工作： 分阶段独立训练轨迹生成模型和动作生成模型

D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\S Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本.md



Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models  https://zju3dv.github.io/hghoi/index.html

首先生成一组**里程碑**（中间的关键帧），然后沿着这些里程碑合成运动

首先生成轨迹，然后合成动作。具体而言，在两个连续的里程碑之间，我们首先完成轨迹（在两个连续的里程碑之间，我们生成长度为2秒的轨迹）

DDPM $(f_r)$ 来生成轨迹

Hierarchical Generation of Human-Object.md



Scaling Up Dynamic Human-Scene Interaction Modeling https://jnnan.github.io/trumans/

CVPR 2024 (highlight)

>可参考这里的ik进行优化
>
>**优化运动轨迹**  
>为消除第一步或逆向运动学(IK)计算导致的轨迹突变，我们对关节偏移量进行时间平滑处理，通过迭代调整相邻帧的权重来实现。**重点** 该优化对维持多物体交互场景中的运动连贯性至关重要，理论细节详见附录B.5。  
>
>**基于IK重新计算运动**  
>最后使用增强的CCD算法[24] IK求解器，根据平滑后的轨迹重新计算人体运动。该求解器通过以下方式提升效果：  
>
>- 对骨骼运动施加剪裁和正则化约束  
>- **重点** 设置层级化旋转限制（离根关节越远的骨骼允许更大旋转）  
>- 显著减少抖动并增强运动真实感  
>  完整算法描述见附录B.5。  
>
>$$ \text{关节偏移量} = \sum_{t=i-k}^{i+k} w_t \cdot \Delta p_t \quad \text{(平滑计算公式)} $$  



Synthesizing Physically Plausible Human Motions in 3D Scenes

这篇轨迹是通过A*寻路做的



https://scenediffuser.github.io/



2、增加情绪

 MotionReFit 

 Diffusion  

CVPR 2025  

[link](https://awfuact.github.io/motionrefit/) 

 开源,暂无Training  

[demo 编辑已有的一些动作](https://huggingface.co/spaces/Yzy00518/motionReFit)   error了  

动作可精确编辑.给动作增加情绪



3、不同风格动作

不同体型的人的动作生成

https://carstenepic.github.io/humos/



4、**物理增强**的

IK,FK

引入环境碰撞，生成角色与场景的交互

（扩展MotionLCM的ControlNet）



5、

跨域动作迁移（不同骨架标准）



6、

实时NPC动作生成系统

ControlNet中新增 **环境状态** 来作为控制的信号（比如玩家距离，障碍物位置）

\实时NPC动作生成系统.md

一句话描述课题：

实现 生成实时交互动作的NPC / NPC实时交互动作生成

详细描述

方法：



7、

文本→图片→骨骼→动作生成

利用生图大模型的强大

似乎暂时没看到类似工作，但是有可能没人做是因为效果不如直接生成动作



8、

网络上的增强

![image-20250513112619798](assets/image-20250513112619798.png)

loss 比如 即有全局也有关节空间，会不会更好 呢



9、

引入IK



## 问题

1、 是直接去找一个结合文本和场景交互的demo跑通（看懂有难度），还是优先找只有文本到动作生成的



## todo



1、codebook VQ-VAE





## 综述或总结

github总结text2motion网站

https://github.com/haofanwang/awesome-conditional-content-generation

https://github.com/layumi/Awesome-Text2Motion-Generation?tab=readme-ov-file



HumanML3D数据集上各个模型的效果

https://paperswithcode.com/sota/motion-synthesis-on-humanml3d?p=executing-your-commands-via-motion-diffusion

### 综述

https://blog.csdn.net/Arachis_X/article/details/136691012

==文件：**[动作生成综述 人体动作生成综述：方法、资源与挑战.md](.\动作生成综述 人体动作生成综述：方法、资源与挑战.md)**==



AI 动作生成的现状与畅想 - Reuben的文章 - 知乎
https://zhuanlan.zhihu.com/p/28333273901



### 论文



人体动作生成的需求通常包括一个条件信号，如**文本描述、背景音频或周围环境**，如图1所示。因此生成的动作不仅本身应该合理，还应与条件信号协调一致。

![image-20240715105627856](动作生成 运动生成.assets/image-20240715105627856.png)





## 论文



非人形骨骼动画生成



https://animatabledreamer.github.io/

从单目视频中提取的骨架上生成各种类别的非刚性物体。



https://nju-3dv.github.io/projects/STAG4D/

感觉是生成视频那种，NERF形式



### 多人



##### Interactive Humanoid

Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting

https://yunzeliu.github.io/iHuman/

专注于人与虚拟人的**互动**任务，特别是涉及物体的互动任务， NPC

https://mp.weixin.qq.com/s/d9c0YirPTxw9_SpmulvYrQ



##### InterGen

文本控制双人/多人互动

InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions

论文：https://arxiv.org/pdf/2304.05684

https://github.com/tr3e/InterGen?tab=readme-ov-file



##### ContactGen

**ContactGen****Contact-Guided Interactive 3D Human Generation for Partners**

https://dongjunku.github.io/contactgen/

**ContactGen****合作伙伴接触引导交互式3D人体生成** 好像没动作，只有一帧



##### ActFormer

ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation

- 主页：https://liangxuy.github.io/actformer/
- 论文：https://arxiv.org/abs/2203.07706
- 代码：https://github.com/Szy-Young/actformer

https://blog.csdn.net/AITIME_HY/article/details/132074368

可多人

提供了GTA Combat Dataset





##### Neural Categorical Priors for Physics-Based Character Control

SIGGRAPH Asia 2023 最佳论文提名

Neural Categorical Priors for Physics-Based Character Control 让虚拟人动作更逼真，基于**物理**的角色控制

有code  https://github.com/Tencent-RoboticsX/NCP

https://arxiv.org/pdf/2308.07200

![image-20240606155257508](动作生成 运动生成.assets/image-20240606155257508.png)

https://blog.csdn.net/y80gDg1/article/details/135233967

具体笔记：

**[基于物理的角色控制Neural Categorical Priors for Physics-Based Character Control.md](基于物理的角色控制Neural Categorical Priors for Physics-Based Character Control.md)**



##### TIMotion

https://hub.baai.ac.cn/view/44623





### 单人



#### 文本->动作



##### MDM: Human Motion Diffusion Model

早年baseline 感觉是

ICLR2023 (Top-25%)

开源 star3.5k



##### MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model

22年比较早了 但是开源

论文链接：

github地址：https://github.com/mingyuan-zhang/MotionDiffuse?tab=readme-ov-file

demo：https://huggingface.co/spaces/mingyuan/MotionDiffuse  好像打不开了

https://zhuanlan.zhihu.com/p/685288565

输入：文字

>多样化和细粒度
>
>第一个基于[扩散模型](https://zhida.zhihu.com/search?content_id=240416616&content_type=Article&match_order=1&q=扩散模型&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDQ5Njc0MTMsInEiOiLmianmlaPmqKHlnosiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyNDA0MTY2MTYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.vlq5xc_Nc06Mlu0Nvj7FFSEjUJtKLSvqudCsgXvx8n4&zhida_source=entity)的[文本驱动动作生成](https://zhida.zhihu.com/search?content_id=240416616&content_type=Article&match_order=1&q=文本驱动动作生成&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDQ5Njc0MTMsInEiOiLmlofmnKzpqbHliqjliqjkvZznlJ_miJAiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyNDA0MTY2MTYsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.ML6lgq5aDqh9-0EYSl_9RUDbEzrQLIiQYHhMUCl0ZC0&zhida_source=entity)框架

![img](https://mingyuan-zhang.github.io/static/images/MotionDiffuse/pipeline.png)



##### MotionGPT: Human Motion as Foreign Language

最后更新 2023/09/22 开源

项目地址：https://motion-gpt.github.io/

github地址：https://github.com/OpenMotionLab/MotionGPT	

有demo 过期了

[NeurIPS 2023](https://nips.cc/)

输入：文字

方法:VAE

笔记:

D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\MotionGPT\MotionGPT Human Motion as Foreign Language.md



##### HumanTOMATO: Text-aligned Whole-body Motion Generation 文本对齐的全身人体动作生成

ICML-2024 (ccfa)

有手

https://github.com/IDEA-Research/HumanTOMATO	

![img](https://github.com/IDEA-Research/HumanTOMATO/raw/main/assets/system.png)

 【摘要】：本研究旨在解决一项新颖任务：文本驱动的全身动作生成。该任务以文本描述作为输入，旨在同时生成高质量、多样化和连贯的面部表情、手势和身体动作。以往的文本驱动动作生成研究主要存在两个缺陷：它们忽视了在生动的全身动作生成中精细的手部和面部所扮演的关键角色，同时文本和动作之间缺乏良好的对齐。为了解决这些缺陷，我们提出了一种名为HumanTOMATO的文本对齐全身动作生成框架。据了解，这是第一个尝试在这一研究领域实现可行的全身动作生成的工作。为了应对这一具有挑战性的任务，我们的解决方案包括两个关键设计：

（1）一种用于精细重建、生成身体和手部动作的**全面层级VQ-VAE**（Holistic Hierarchical H²VQ）和Hierarchical-GPT分层GPT，具有两个结构化码本；

以及（2）一个预训练的文本-动作对齐模型，以帮助生成的动作明确地与输入的文本描述对齐。全面的实验证明，我们的模型在生成动作的质量和其与文本的对齐方面具有显著优势。

【

在VQ-VAE中，"codebook"（码本）指的是一种用于存储离散编码的集合，这些编码用来表示输入数据的离散化版本。具体来说，它是由一组向量组成的集合，每个向量对应着编码空间中的一个离散点。在训练过程中，VQ-VAE通过量化器（quantizer）将输入数据映射到最接近的码本向量，从而实现信息的离散化压缩。

这种码本的存在使得VQ-VAE能够有效地将连续的输入数据转换为离散的表示形式，从而在保留重要信息的同时，减少了存储和处理上的复杂性。在生成过程中，码本的向量被用来重构原始输入或生成类似输入的新样本，从而支持模型在高维空间中进行有效的表示和生成。

总结来说，码本在VQ-VAE中充当了存储和量化离散信息的重要角色，是模型能够实现高效数据压缩和生成的关键组成部分。

】

第一步，将motion压缩成离散代码



##### Taming Diffusion Probabilistic Models for Character Control 

windows可执行.exe demo下载地址：https://github.com/AIGAnimation/CAMDM 

论文：[SIGGRAPH 2024]Taming Diffusion Probabilistic Models for Character Control  https://arxiv.org/pdf/2404.15121

作者：Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, Xuelin Chen

项目地址：https://github.com/AIGAnimation/CAMDM

【[SIGGRAPH] 100种**运动风格**切换实机测试！**unity**中使用diffusion model实时生成动作并控制角色运动】 https://www.bilibili.com/video/BV1cs421K71o/?share_source=copy_web&vd_source=2c8847c41a6c97560a277330511f1edd





##### MoMask 文本一键转3D数字人骨骼动画

CVPR 2024

==[MoMask 文本一键转3D数字人骨骼动画.md](D:\YanYi1\xia\AI\MotionGenerate\MoMask\MoMask 文本一键转3D数字人骨骼动画.md)==

![图片](https://image.jiqizhixin.com/uploads/editor/67b517f5-16d2-43ed-bdd8-d4f92b9c7678/640.png)

- 论文题目：MoMask: Generative Masked Modeling of 3D Human Motions
- 论文链接：https://arxiv.org/abs/2312.00063
- 代码链接：https://github.com/EricGuo5513/momask-codes
- Huggingface Space 链接：https://huggingface.co/spaces/MeYourHint/MoMask
- 方法：VQ-VAE

讲解：**https://www.jiqizhixin.com/articles/2024-04-29-3**

没有手指，还是会有抖动

![image-20240715104041805](动作生成 运动生成.assets/image-20240715104041805.png)





MoMask: 生成式遮蔽建模 3D 人体动作

https://arxiv.org/pdf/2312.00063

https://ericguo5513.github.io/momask/



##### Motion Generation from Fine-grained Textual Descriptions 细粒度文本 FineMotionDiffuse

D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\FineMotionDiffuse

[写了文章md](D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\MotionGenerate\动作生成 motion generate Motion Generation from Fine-grained Textual Descriptions.md)

方法：Diffusion

grained 高度详细的

- 简介

  本文的任务是从给定的文本描述生成人体运动序列，模型探索从自然语言指令到人体运动的多样映射。虽然大多数现有的工作都限于粗粒度的运动描述，例如“一个人蹲下”，但几乎没有探索指定相关身体部位运动的**细粒度描述**。使用粗粒度文本训练的模型可能无法学习从细粒度运动相关单词到运动基元的映射，导致无法从未见过的描述中生成运动。因此，本文通过向 **GPT-3.5-turbo** 提供具有伪代码强制检查的逐步说明来构建一个专门针对细粒度文本描述的大规模语言-运动数据集 FineHumanML3D。相应地，我们设计了一种新的文本到运动模型 **FineMotionDiffuse**，充分利用细粒度文本信息。我们的定量评估表明，FineMotionDiffuse 在 FineHumanML3D 上训练相比竞争基线大幅提高了 FID 0.38。根据定性评估和案例研究，我们的模型在生成空间或时间复合运动方面优于 MotionDiffuse，通过学习从细粒度描述到相应基本运动的隐式映射。我们在 https://github.com/KunhangL/finemotiondiffuse 上发布了我们的数据。

  收起 

- 解决问题

  该论文旨在解决text2motion中细粒度文本描述的问题，即如何从细粒度的文本描述中生成人体运动序列。

- 关键思路

  论文提出了一种新的text2motion模型FineMotionDiffuse，该模型利用细粒度文本信息进行训练，并学习从细粒度描述到相应基本动作的隐式映射，从而生成空间或时间上复合的动作。

- 其它亮点

  论文构建了一个大规模的FineHumanML3D数据集，使用GPT-3.5-turbo对其进行训练，并设计了FineMotionDiffuse模型，该模型在FineHumanML3D上训练，相对于竞争基线模型，FID提高了0.38。论文还开源了数据集和代码。

- 相关研究

  最近的相关研究包括：1. Learning to Generate 3D Human Pose and Shape via Multiview Pose Estimation and 3D Morphable Model. 2. Learning to Generate Human Motion with Adversarial Training. 3. Text2Action: Generative Adversarial Synthesis from Language to Action.

https://hub.baai.ac.cn/paper/8fcbf49d-695c-4184-b42d-8a5f02318a9c



##### TAAT: Think and Act from Arbitrary Texts in Text to Motion 任意文本

https://arxiv.org/pdf/2404.14745v3



##### 元象推出国内首个基于物理的3D动作生成模型MotionGen

【MotionGen 输入文本👉生成动作，3D动画GPT时代来了】 https://www.bilibili.com/video/BV1zXs5ecEDH/?share_source=copy_web

https://www.jiqizhixin.com/articles/2024-08-28-7

不过用不了 也不开源

**强化学习+物理仿真：让动作自然逼真**

使用动捕数据进行动作生成模型的训练是目前业界普遍采用的方法。但原始动捕数据通常会带有抖动和误差等问题，使其不够自然和符合物理规律。我们结合深度强化学习，通过在仿真环境中模仿人类动捕数据的模仿学习[1,2,3,4]可以生成更自然的动作。由于使用动捕数据，该方法无需设计特定的目标函数，而是利用统一的模仿误差作为优化目标。通过在训练过程中引入扰动和改变目标，训练出的控制器可以适应目标motion的变化，呈现自然的过渡动作。

**VQ-VAE：提取特征并复用**

基于基于矢量量化变分自编码器（VQ-VAE）[5]。近几项研究表明，**VQ-VAE提供了一个有效且紧凑的动作表征**，许多运动学中的动作生成模型[6,7,8]已经证明了VQ-VAE这种离散但紧凑的隐空间表示适用于在大型数据集上训练，并可应用到各种下游任务中。受此启发，我们将VQ-VAE训练得来的动作表征和GPT结合起来，实现基于物理模拟的文本生成动作。

**Transformer：文本与动作的高效连接**

在文本生成动作的任务中，Transformer 结构发挥了关键作用。受序列模型近两年发展的启发，我们通过 Transformer 的注意力机制[9]，在文本与动作表征之间建立了复杂而精准的联系。通过双 Transformer 设计，模型不仅生成了基础动作编码，还进一步细化了这些编码，捕捉到运动的微妙细节。这种多层次编码方式使生成的动作既符合物理规律，又展现出自然的流动性和多样性，极大提升了文本驱动动作生成的表现力。该设计使 MotionGenGPT 能够从文本中生成多样且高度逼真的运动序列，实现了自然语言与运动控制的无缝连接。

![图片](https://image.jiqizhixin.com/uploads/editor/b17e407d-f8d5-43a8-9611-1058883bbb1e/640.png)



##### MotionReFit 动作可精确编辑

CVPR 2025

介绍：https://mp.weixin.qq.com/s/SWXhiDUFpWnU2kCIIGaIiw

https://awfuact.github.io/motionrefit/

demo 编辑已有的一些动作 https://huggingface.co/spaces/Yzy00518/motionReFit

增加情绪还行，比如加伤心 就会垂头丧气

不过比如增加手的动作之类的可能不太行



**1. 项目开源的内容**

- **✅ 开源了代码**
- **❌ 没有开源训练数据**
  - 项目仅提供了预训练模型（checkpoints）和必要的资源文件（如 SMPL-X 人体模型），这些文件托管在 Hugging Face，需通过 `git-lfs` 下载。
  - 训练部分的代码标注为 *“Coming soon!”*，尚未公开。



##### [ECCV 2024] MotionLCM：实时动作生成与控制模型

MotionLCM，一个支持实时动作生成和可控生成的单步扩散模型！[论文](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2404.19759)、[代码](https://link.zhihu.com/?target=https%3A//github.com/Dai-Wenxun/MotionLCM)、[demo](https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/wxDai/MotionLCM)、[项目主页](https://link.zhihu.com/?target=https%3A//dai-wenxun.github.io/MotionLCM-page)、[视频展示](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1uT421y7AN/)等均已公开。

https://zhuanlan.zhihu.com/p/693836738

基于diffusion

D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\FineMotionDiffuse 细粒度文本描述\MotionLCM：实时动作生成与控制模型.md

效果还行



#####  FlowMDM 无缝人类动作组合（HMC）的方法

https://barquerogerman.github.io/FlowMDM/

条件人类动作生成是一个重要课题，在虚拟现实、游戏和机器人技术中具有广泛应用。虽然先前的研究主要集中在通过文本、音乐或场景指导生成动作，但这些方法通常只能生成短时孤立的动作片段。我们致力于解决基于系列变化文本描述生成长时连续动作序列的挑战。在此背景下，我们提出了FlowMDM——首个基于扩散模型的无需后处理或冗余去噪步骤即可生成无缝人类动作组合（HMC）的方法。为此，我们开发了混合式位置编码技术，该技术在去噪链中同时利用绝对和相对位置编码。具体而言，全局动作连贯性在绝对阶段恢复，而平滑自然的过渡效果则在相对阶段构建。实验表明，我们的方法在Babel和HumanML3D数据集上实现了最先进的准确度、真实性和平滑性指标。

FlowMDM的独特优势在于其"以姿态为中心的交叉注意力机制"，即使每个动作序列仅使用单一描述进行训练，也能保持优异性能。该机制使得模型在推理时能够有效应对变化的文本描述输入。最后，针对现有HMC评估指标的局限性，我们提出了两个新指标：峰值急动度（Peak Jerk）和急动度曲线下面积（Area Under the Jerk），专门用于检测动作序列中的突变过渡。这些创新使FlowMDM在长时序动作生成的连续性和自然度方面实现了突破性进展，为虚拟角色动画和机器人运动规划等领域提供了更可靠的解决方案。



#### 线稿/火柴人 ->动作

##### StickMotion : CVPR 2025 | 基于手绘火柴人的人体动作生成算法

介绍：https://mp.weixin.qq.com/s?__biz=MzUxMDE4MzAzOA==&mid=2247833749&idx=4&sn=3d7a1c9e9564c9ec002a6ef4b84ea060&chksm=f8025c5ec50f12373daeb8acdd8b7451b57babf074b71f3a549b35859dd4492ce9daa2380377&scene=27

论文 ： https://arxiv.org/pdf/2503.04829



#### 生成与编辑 MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion **Paradigm**

2025.3月刚发

主页:https://diouo.github.io/motionlab.github.io/

论文: https://arxiv.org/pdf/2502.02358





#### 音频->动作：舞蹈 



##### Lodge 单人跳舞

论文标题：Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives

机构：清华大学，鹏城实验室，Meshcapade，北京师范大学

论文链接：https://arxiv.org/pdf/2403.10518.pdf

项目地址：https://li-ronghui.github.io/lodge

https://www.bilibili.com/read/cv33342660/



#### 音频->动作：演讲 



##### EMAGE  面部+肢体动画，一个框架搞定从音频生成数字人表情与动作

2401

- 论文地址：https://arxiv.org/abs/2401.00374
- 项目主页：https://pantomatrix.github.io/EMAGE/
- 视频结果：https://www.youtube.com/watch?v=T0OYPvViFGE
- hugging face space 链接：https://huggingface.co/spaces/H-Liu1997/EMAGE

**机器之心：https://www.jiqizhixin.com/articles/2024-04-01-3**

用了BEAT2数据集



##### 本科毕设研究过的beat

D:\YanYi1\xia\AI\MotionGenerate\本科毕设 手势生成\毕设论文 写在模板上0422T1930.docx





### 场景互动



##### （**SceneDiffuser**）Diffusion-based Generation, Optimization, and Planning in 3D Scenes

早年 CVPR23

https://scenediffuser.github.io/

https://scenediffuser.github.io/paper.pdf

涉及轨迹生成



##### 分层 单人长文与物体交互 Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models

[详细笔记.md](D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\S 3D场景交互Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models\Hierarchical Generation of Human-Object.md)

https://www.bilibili.com/video/BV1xH4y1973x/?vd_source=8365f01b9a38920137caac09dd55836f

方法：diffusion生成里程碑和中间帧 （也用到了**VQ-VAE**来建模数据分布）采用分层框架，依次生成目标姿势、里程碑和动作

思路：

**长距离运动生成可以简化为合成若干个由里程碑引导的短动作序列**

基于扩散概率模型的人 - 物交互的分层生成

Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models

![image-20240604154849435](动作生成 运动生成.assets/image-20240604154849435.png)

https://arxiv.org/pdf/2310.02242

https://zju3dv.github.io/hghoi/index.html

局限：似乎只有坐在沙发上 躺床上之类的



##### 抓取 Controllable Human-Object Interaction Synthesis 可控人-物交互综合

ECCV 2024 (Oral)

https://arxiv.org/pdf/2312.03913

主页https://lijiaman.github.io/projects/chois/

可以看的讲解等

GAMES Webinar 321-模拟与动画专题-李佳蔓-面向物理交互场景的人类运动建模与仿真

与场景交互的运动生成

https://www.bilibili.com/video/BV1SE421G7h7/?spm_id_from=333.788&vd_source=8365f01b9a38920137caac09dd55836f

![image-20240604160855799](动作生成 运动生成.assets/image-20240604160855799.png)





先预测手的位置，然后再预测整个人体的位置

![image-20240604161608743](动作生成 运动生成.assets/image-20240604161608743.png)

![image-20240604161849551](动作生成 运动生成.assets/image-20240604161849551.png)





![image-20240604161924274](动作生成 运动生成.assets/image-20240604161924274.png)

![image-20240604162222873](动作生成 运动生成.assets/image-20240604162222873.png)

![image-20240604162237308](动作生成 运动生成.assets/image-20240604162237308.png)







GAMES Webinar 321-模拟与动画专题-蒋一峰-面向物理交互场景的人类运动建模与仿真

https://www.bilibili.com/video/BV1Qb42187nd/?spm_id_from=333.788&vd_source=8365f01b9a38920137caac09dd55836f



##### Synthesizing Physically Plausible Human Motions in 3D Scenes 合成3D场景中物理上合理的人类动作

此论文的方法可以归类为**生成模型与强化学习的结合**，但其核心更偏向于**任务特定模型**（如动作图）的扩展，而非传统的生成模型（如GANs、VAEs等）。以下

有论文笔记

[note在此](./Synthesizing Physically Plausible Human Motions in 3D Scenes 动作生成 人物与3D场景交互.md)

[更新note位置](D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\S 场景交互 物理合理的动作\Synthesizing Physically Plausible Human Motions in 3D Scenes 动作生成 人物与3D场景交互.md)

主页：https://github.com/liangpan99/InterScene

**InterCon学习与物体交互的技能**，而**NavCon则控制角色沿无障碍路径的移动**。

通过**有限状态机（Finite State Machine, FSM）**调度这两个执行器

我们通过目标条件强化学习和对抗性动作先验（AMP）来训练所有策略。

![image-20240905110331504](D:\myNote\Postgraduate\MotionGenerate\assets\image-20240905110331504.png)



##### Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本

[note](D:\myNote\ppNotes\PrepareForWorkNotes\AI\动作生成\论文\S Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本.md)

主页：https://zju3dv.github.io/text_scene_motion/

github：https://zju3dv.github.io/text_scene_motion/

我们提出了一个大规模且富含语义的人-场景交互**数据集** HUMANISE。每个 HSI 都包含语言描述。HUMANISE 实现了一项新任务：在 3D 场景中进行语言条件的人类动作生成。

![image-20250416111955322](assets/image-20250416111955322.png)



##### GMD: Guided Motion Diffusion for Controllable Human Motion Synthesis

![image-20240910135145754](assets/image-20240910135145754.png)

https://github.com/korrawe/guided-motion-diffusion

研究 [38] 可以在行走时避免障碍物

>#### 去噪扩散模型在文本驱动人体运动合成中的进展与挑战
>
>**去噪扩散模型**在基于自然语言描述的人体运动合成领域展现出巨大潜力。然而，现有方法在整合**空间约束**（如预设运动轨迹与障碍物）方面仍存在明显局限，这种能力缺失导致生成的孤立人体动作难以与真实环境有效交互，成为阻碍实际应用的关键瓶颈。
>
>#### 解决方案：引导式运动扩散模型（GMD）
>
>为突破上述限制，我们提出**引导式运动扩散模型（Guided Motion Diffusion, GMD）**，通过两大核心技术实现空间约束的精准控制：
>1. **特征投影增强机制**：开发高效的跨模态特征对齐方案，显著提升*空间信息（如全局轨迹）与局部肢体姿态的连贯性*
>2. **动态插补公式**：提出新型运动补全算法，确保生成动作严格遵循给定的空间约束条件，数学表达为：
>   > *生成动作 = 扩散模型先验 × 空间约束条件*
>
>#### 稀疏信号到密集引导的创新转换
>面对**稀疏空间约束**（如少量关键帧），我们设计**密集引导策略**，利用扩散模型的去噪器将稀疏信号转化为密集运动指导信号。该过程通过迭代优化实现：
>$$
>G_{dense} = \sum_{t=1}^T \mathcal{D}(G_{sparse}, \epsilon_t)
>$$
>其中𝒟为去噪网络，εₜ表示第t步的噪声扰动，最终输出高精度的连续运动轨迹。
>
>#### 实验结果与性能突破
>在跨数据集测试中，GMD在**文本-运动对齐精度**（R-Precision提升18.7%）和**空间约束合规率**（轨迹误差降低32.4%）两项核心指标上均显著超越现有最优方法。特别在复杂场景（如绕过动态障碍物+同步完成指定动作）中展现出更强的环境适应能力。
>
>---
>
>### **核心贡献总结**
>**重点突破1**：首创*空间约束与扩散模型的耦合架构*，通过特征投影与动态插补实现动作生成的可控性  
>**重点突破2**：开发*稀疏到密集的信号转换器*，仅需少量关键帧即可生成符合物理规律的真实运动  
>
>---
>
>### **两句话技术总结**
>**①** GMD通过**空间-姿态联合优化框架**，首次在扩散模型中实现*毫米级轨迹精度*与*自然肢体动作*的同步生成。  
>**②** 提出的**密集引导机制**将稀疏约束的利用率提升3.2倍，使合成动作既能贴合复杂环境，又能准确响应文本指令。



##### Synthesizing Diverse Human Motions in 3D Indoor Scenes

https://arxiv.org/pdf/2305.12411

code:https://github.com/zkf1997/DIMOS

主页：https://zkf1997.github.io/DIMOS/



##### PartwiseMPC: Interactive Control of Contact-Guided Motions

 https://www.youtube.com/watch?v=wdL9EvkIWZo

![image-20240910141458489](assets/image-20240910141458489.png)









### 工业级

https://news.nweon.com/119109



### vlm

https://blog.csdn.net/shebao3333/article/details/139065434

https://www.jiqizhixin.com/articles/2024-06-11-15

![image-20240715111708327](动作生成 运动生成.assets/image-20240715111708327.png)



### 草图生成

https://patentimages.storage.googleapis.com/2a/0d/57/600d5efe1f506a/CN110310351A.pdf