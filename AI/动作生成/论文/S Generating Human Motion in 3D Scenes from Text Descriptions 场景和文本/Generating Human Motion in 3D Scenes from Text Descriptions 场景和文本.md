

Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本

https://zju3dv.github.io/text_scene_motion/

#### CVPR 2024

本地测试位置

D:\_LabProj\MotionGenerate\SceneAndText2Motion\text_scene_motion



这篇论文的baseline

HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes

直接渲染出一段视频

**摘要**

在 3D 场景中学习生成多样的、场景感知和目标导向的人类动作仍然具有挑战性，这主要是由于现有的人-场景交互（HSI）数据集的特点较为平庸；这些数据集在规模和质量上均有限，并且缺乏语义信息。为了填补这一空白，我们提出了一个大规模且丰富语义的合成 HSI 数据集，命名为 HUMANISE，通过将捕获的人类运动序列与各种 3D 室内场景对齐来实现。我们自动为对齐的动作添加语言描述，描绘动作及场景中独特的交互对象，例如：“坐在桌子旁的扶手椅上”。因此，HUMANISE 使得一种新的生成任务成为可能，即在 3D 场景中进行语言条件的人类动作生成。该任务具有挑战性，因为它需要对 3D 场景、人类动作和自然语言进行联合建模。为了解决这一任务，我们提出了一种新颖的场景和语言条件生成模型，能够生成与指定对象交互的理想动作的人类 3D 动作。我们的实验表明，我们的模型能够在 3D 场景中生成多样且语义一致的人类动作。

**简而言之：**我们提出了一个大规模且富含语义的人-场景交互数据集 HUMANISE。每个 HSI 都包含语言描述。HUMANISE 实现了一项新任务：在 3D 场景中进行语言条件的人类动作生成。

![image-20240909151600504](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\assets\image-20240909151600504.png)

实际上这个motionlcm自带了一个轨迹编码器，输入轨迹，它可以生成符合轨迹的动作，我想通过拓展它这部分，就是引入场景，实现轨迹生成器，然后再输入这个motionlcm实现符合场景的动作，这样好做吗，请问这个可行性以及请给我一些参考文献。

**摘要**

​	从文本描述生成人体运动引起了越来越多的研究兴趣，因为它具有广泛的应用。然而，考虑人与场景交互以及文本条件的工作仍然较少，而这对于视觉和物理现实主义至关重要。本文专注于在给定人-场景交互的文本描述下，生成3D室内场景中的人体运动。由于文本、场景和动作的多模态特性以及对空间推理的需求，这项任务面临着挑战。为了解决这些挑战，我们提出了一种新方法，将复杂问题分解为两个更易于管理的子问题：(1)目标对象的语言基础和(2)以对象为中心的运动生成。在目标对象的语言基础方面，我们利用大型语言模型的强大能力。对于运动生成，我们设计了一种面向对象的场景表示，使生成模型能够专注于目标对象，从而减少场景复杂性，并促进人体运动与对象之间关系的建模。实验结果表明，我们的方法在运动质量上优于基线，并验证了我们的设计选择。代码将会在链接中提供。

**1. 引言**

​	人体运动生成一直是一个长期存在的问题，因其在游戏开发、虚拟现实和电影制作等广泛应用中具有重要意义。近年来，这一领域经历了一场范式转变，从基于丰富用户输入的角色动画 [29] 转向基于高层次语言提示（例如，关于期望运动的文本描述）进行学习的运动生成 [2, 3, 15, 18, 19, 39, 56, 74, 75]。然而，大多数之前关于文本驱动运动合成的研究并未考虑人与场景之间的交互 [39, 56, 74, 75]，而场景上下文和环境的物理约束在很大程度上决定了生成的人体运动的真实性。

​	在本文中，我们专注于从3D室内场景的文本描述中生成运动。具体而言，给定目标场景的3D扫描和描述人类与场景交互的动作的文本描述，我们旨在生成与文本描述一致的自然人体运动。这个问题面临几个挑战，主要由于文本、场景和人体运动的多模态特性。与之前仅关注人类如何移动的文本描述的方法 [2, 3, 56, 74, 75] 相比，我们的任务还包括额外描述给定场景中的空间细节的文本（例如，坐在桌子旁的扶手椅上）。因此，这项任务需要空间推理能力 [1]，模型应该建立文本-对象映射，以定位与自然语言描述相一致的3D场景中特定对象。此外，生成的运动还应与场景上下文保持一致。

​	作为开创性工作，**HUMANISE** [83] 构建了一个条件变分自编码器（cVAE） [41, 69]，为多模态理解设计了独立的场景和文本编码器。为了使空间推理能力得以实现，他们引入了直接回归对象中心的辅助任务，以隐式方式学习3D视觉基础。但是，他们并没有明确利用预测的中心，因此视觉基础的归纳偏差无法充分结合。此外，HUMANISE [83] 用单个点变换器 [99] 编码整个场景。使用这样的模型直接生成运动是具有挑战性的，因为3D点云本质上是嘈杂和复杂的 [48, 88]，这导致无法精确定位目标对象。正如 [51] 所建议的那样，并不是场景中的每个点都与最终的人体运动相关。因此，**开发一种更有针对性的方法，专注于场景中的相关部分以提高运动生成质量也是必要的。**

​	为了解决上述问题，我们提出了一种新方法，利用大型语言模型（LLMs）的强大能力 [6, 42, 52]。我们的关键思想是通过将**生成场景中基于文本提示的运动**这一具有挑战性的任务分解为两个较小的问题来解决： **(1) 3D场景中对象的语言基础和 (2) 专注于目标对象的运动生成。**在语言基础方面，我们并不直接学习文本-对象映射，而是将其表述为问答形式，并利用LLMs的丰富先验知识。具体而言，我们首先构建3D场景的场景图并生成其文本描述。然后，我们使用ChatGPT [52] 分析场景描述与输入指令之间的关系，并以3D视觉基础回答。实验证明了这一策略的有效性。

​	对于运动生成，我们设计了一种以对象为中心的表示方法，以帮助生成模型专注于目标对象。具体而言，我们**将目标对象周围的点云转换为体积传感器** [70]，**以构建对象中心表示**。然后，我们采用扩散模型 [57, 75] 来合成给定对象中心表示和文本的人体运动。与可能具有不同尺度的原始场景点云相比，对象中心表示更紧凑并且对尺度更具鲁棒性，因为同一类别中的对象尺寸相似。因此，这种表示降低了场景复杂性，并促进了运动与对象之间关系的建模。

​	我们在HUMANISE数据集上进行了全面的比较和消融实验。结果证明，我们的方法优于基线，反映在更准确的对象基础结果和更好的人体运动上，这些运动与文本描述和场景保持一致。我们进一步显示，我们的方法能够在不进行任何微调的情况下推广到PROX数据集 [22]。

**2. 相关工作**

**2.1. 动作合成**

​	*深度学习在动作合成中的应用。*近年来，深度学习方法在动作合成方面受到越来越多的关注 [14, 21, 25, 28, 29, 50, 71, 91]。各种技术，包括多层感知器（MLP） [29]、专家混合模型（MoE） [72, 91] 和循环神经网络（RNN） [21]，被用于解决这一任务。为了生成多样的动作结果，之前的研究探索了条件变分自编码器（cVAE） [46, 96]、生成对抗网络（GANs） [44, 61]、归一化流 [26] 和扩散模型 [75]。

​	*以文本驱动的动作生成。*最近，对以文本驱动的动作生成的兴趣日益增长 [2, 56, 74]。这个任务以自然语言为输入，合成与指令一致的人类动作。KIT-ML [58] 是该任务的第一个基准。一些研究进一步为 AMASS 数据集 [49] 添加了动作标签 [59] 和文本 [18]。为了解决这个任务，[2, 15, 74] 提出了学习文本和动作共享潜在空间的方法。[56] 使用变换器变分自编码器（transformer VAE） [55] 来生成多样的结果。[19, 93] 通过使用 VQ-VAE [65, 78] 的离散表示实现了更好的性能。[12, 39, 75, 94] 成功地在此方向上应用了扩散模型。[5, 43] 进一步探索了潜在扩散模型 [67]。基于 [75]，[38, 85] 引入了稀疏空间控制。[3, 60, 68] 还研究了长序列动作生成。其他现有工作 [4, 37, 82, 98] 在动作领域利用了大型语言模型（LLM） [6, 42, 53, 77]。[4] 通过结合不同的身体部位与 LLM 构建组合动作。[36, 98] 将动作视为一种语言，并用运动标记 [19, 93] 来微调 LLM [63]。

​	*场景感知的动作生成。*这一方向旨在在 3D 场景中生成人体动作 [70, 80]。为了合成行走和坐下的动作，[70] 构建了体积传感器以编码角色周围的物体信息。 [23] 将其扩展到合成具有多样坐姿和躺卧风格的动作。此外，[95] 提出了通过手部接触来控制坐姿风格。为了提高性能，[57] （Hierarchical generation of human-object interactions with diffusion probabilistic models.  ）采用分层框架，依次生成目标姿势、里程碑和动作。[73, 84] 合成了与小物体的全身（身体和手指）抓取动作。[16, 45, 71, 86] 探索了与动态物体的交互，包括操控 [16, 45] 和搬运 [86]。 [8, 24] 通过物理模拟角色探索人类与物体的交互。这些工作主要集中在一个或两个物体上，而其他工作 [80, 81] 考虑了更复杂的场景输入（例如，包括墙壁和地板的场景点云）。[80, 81] 采用分层框架，分别生成轨迹和动作。[103] 引入注视以帮助生成。[33] 进一步使用扩散模型，并且 [51] 通过将长序列分割成多个短序列合成非常长时间的动作。[101] 设计了一种强化学习管道，以便在复杂场景中进行导航和交互样式控制。

​	***以文本驱动的场景感知动作生成**。*只有少数工作同时考虑了文本和场景 [38, 83, 101]。尽管 [101] 通过 [100] 实现了对坐姿风格的文本控制，但在我们的设置中，文本描述用于选择杂乱场景中的对象。研究 [38] 可以在行走时避免障碍物，而我们的任务需要处理各种动作。与我们最相关的工作是 **HUMANISE [83]，它采用了带有双流条件模块的变换器 VAE 架构，用于处理文本和场景**。为了准确定位目标对象，他们设计了辅助任务，如直接回归对象中心。与 HUMANISE 相比，我们提出了一个两阶段的 pipeline，首先**借助 ChatGPT [52] 定位目标对象，然后使用以对象为中心的表示生成人类动作**。

**3. 问题设置与基础知识**                                                      

​	在本节中，我们讨论任务的定义和相关基础知识。我们的目标是生成与文本描述和给定场景一致的人类动作。

​	**文本描述。** 文本描述遵循 Sr3D [1] 中的模板（例如，“<坐在> <椅子上> [<在> <桌子和书架之间>]”）。有四个动作（走、坐、站起和躺下）。目标表示代理需要交互的对象，而锚对象则有助于确定目标。某种类型的目标家具类别通常在一个场景中有许多实例，而锚家具应当是唯一的。为了精确指定目标对象，目标与锚之间存在五种空间关系[1]：水平接近、垂直接近、在...之间、以自我为中心（allocentric）和支撑。

​	**场景表示。** **场景被表示为一个由 N 个点组成的点云**：$S ∈ R^N×6$，包含每个点的位置和法向量信息。

​	**动作表示。** 输出的动作序列表示为一系列 **SMPL-X [54] 身体网格 M**。SMPL-X 是一种参数化的人体模型。在本研究中，身体参数包括身体形状参数 β ∈ R^10、全局平移 r ∈ R^3、6D 全局方向 γ ∈ R^6 以及 J 个关节的 6D 姿态参数 $ θ ∈ R^{(J×6)}$ [104]。根据 [83]，β 被视为建模身体形状影响的条件，为了简化符号表示我们省略它。请注意，起始位置和姿势也由模型生成。

​	**扩散模型。** 扩散 [27] 被定义为一个马尔可夫噪声过程$ {xt}^T_{t=0}$，满足 q(xt | x0)，其中 x0 ∼ q(x0) 是数据，xt 是噪声步骤 t 的噪声数据。其正式定义为：

$q(xt | x0) = N(xt; \sqrt{\bar{\alpha}_t} x0, (1 - \bar{\alpha}_t) I),$

其中 $\bar{\alpha}_t$ 是单调递减的常数。当 $\bar{\alpha}_t$ 足够小的时候，我们可以近似得出

$x_T ∼ N(0, I).$

在我们的背景下，我们使用条件扩散模型，如 [64, 75]。训练损失被定义为：

$L = E_{t∈[1,T], x0∼q(x0)} [∥x0 - G(xt, t, C)∥],$

其中 G 是生成模型，C 是条件。

**4. 方法**

我们方法的概述如图 2 所示。在第 4.1 节中，我们利用 ChatGPT 在给定 3D 场景和文本描述的情况下定位目标对象。基于准确的定位，我们可以聚焦于目标对象，并采用以对象为中心的生成管道分别合成轨迹（第 4.2 节）和动作（第 4.3 节）。实现细节将在第 4.4 节中讨论。

![image-20240909160551815](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\assets\image-20240909160551815.png)

Figure 2. Overview of our two-stage pipeline. In the first stage, given an input scene and a text description (a), we use ChatGPT to locate the target object (b). In the second stage, human motions are synthesized by first producing human trajectories (c) and then generating local poses (d).  

图 2. 我们两阶段管道的概述。

在第一阶段，给定一个输入场景和文本描述 (a)，我们使用 ChatGPT 来定位目标对象 (b)。

在第二阶段，通过首先生成人体轨迹 (c) 和然后生成局部姿势 (d) 来合成人体动作。



**4.1. 3D场景中的对象语言基础**

从文本描述生成场景感知运动需要场景理解能力，并建立场景与文本之间的关系。由于指令描述了角色如何移动并与单个目标对象进行交互，场景的大部分可能与最终动作具有微不足道的相关性 [51]。基于此动机，我们提出定位目标对象，以识别最相关的信息。受到 LLM [42, 52, 53] 的最新进展的启发，我们利用 ChatGPT [52] 来查找给定文本中的特定对象。我们首先通过**构建场景图获取场景的文本描述**。然后，我们将文本指令输入到使用特别设计的提示词的 ChatGPT 中，并解析响应以获取目标对象。

**空间场景图提取。** 为了利用 LLM，**初始步骤涉及将 3D 场景转换为文本**。这是通过构建空间场景图来实现的。我们利用预训练的 3D 目标检测模型 [47] 提供 3D 边框提议。随后，我们遵循 **[1]（看下面【1】）** 的方法来获取对象之间的关系。具体而言，对于每一对对象，我们可以**根据其边界框推断三种类型的关系：水平接近、垂直接近和支撑**，如第 3 节所述；对于每组三个对象，我们可以推断其中一个是否位于其他两个对象之间。由于检测结果不包含对象姿态，我们不构建以自我为中心的关系（例如，“在沙发后面的架子”）。然后，我们构建一个场景**图**，其中**每个对象被分配为图中的一个节点**，**节点之间的边表示对象之间的关系**。通过将 3D 场景转换为文本，我们可以应用 ChatGPT 从数据中提取有意义的见解。



【1】：

![image-20240909161555238](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\assets\image-20240909161555238.png)



**利用 ChatGPT 定位目标对象。**

​	一种简单的方法是直接将整个场景图输入到 ChatGPT，并要求其选择目标对象。然而，场景中可能包含许多对象，导致文本描述极其冗长。我们观察到，在这种情况下，ChatGPT 经常感到困惑，无法给出正确的答案。为了缩小搜索空间，我们首先利用 ChatGPT 识别与提供文本相关的对象。然后，我们从场景图中排除不相关的对象，仅专注于那些包含相关信息的对象，从而有效地定位目标对象。这种方法的优点在于减少了需要考虑的对象数量，使 ChatGPT 更容易识别目标对象。

​	如图 3 所示，我们依次构建两个提示词。以“坐在板子和边桌中间的椅子上”为例，我们首先需要缩小对象类别的搜索空间。为了找到我们关心的对象类型，我们构建第一个提示词，要求 ChatGPT 查找目标对象和锚对象。目标对象被定义为我们希望代理人交互的最终对象，在本例中是“桌子”。**锚对象是帮助确定目标对象的对象**，因为一个场景中可能有许多椅子。根据目标对象“椅子”和锚对象“板子”和“边桌”，我们可以过滤掉场景中所有不相关的对象，只保留椅子、板子和边桌在我们的场景图中。接下来，根据简化后的场景图，我们可以用文本描述对象关系：场景图中的每一条边都可以转换为类似“椅子 4 离边桌 0 很远”的边句子。将所有边转换为这样的边句子能够全面描述当前场景。最后，我们通过询问 ChatGPT 从累积的边句子推断目标对象来构建第二个提示词。





**Input ：**  

文本描述： 坐在位于桌子和边桌之间的椅子上。

**Prompt 1  ：**

房间包含： 椅子、背包、显示器、边桌、桌子、脚凳、黑板、打印机。 

文本描述（来自输入） 

指令： 你是一个助手，帮助人们在房间中找到物体 ... 

少量示例： {example_1}; {example_2}。 

问题： 根据文本推断目标对象和锚对象。请按照以下格式回答 ...

**Prompt 2  ：**

对象关系（来源于第一阶段的简化场景图）： 

椅子 0 靠近边桌； 

椅子 2 离桌子远；

 ... 

椅子 0 位于边桌和桌子之间；

文本描述（来自输入）

目标对象（来自第一阶段） 

锚对象（来自第一阶段） 

指令： 你是一个助手，根据给定的对象关系确定目标对象 ...

few-shot（少样本）示例： {example_1}; {example_2}。【“few-shot” 通常翻译为 “少量示例” 或 “少样本”，指的是在机器学习和自然语言处理等领域中，仅使用少量的训练示例来进行学习或推断。】

问题： 根据给定的对象关系和文本描述，推断目标对象。请按照以下格式回答 ... 检测到的对象边界框

![image-20240909161928771](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\assets\image-20240909161928771.png)

图 3. 定位目标对象的管道。

在第一阶段，缩小对象搜索空间。给定输入的文本描述和检测到的对象边界框 (bbx)，我们构建第一个提示，询问 ChatGPT 目标对象和锚对象的类别。根据响应，场景图可以被简化。

在第二阶段，推断目标对象。我们使用第一阶段的输入和结果构建第二个提示，包括从简化场景图中得出的对象关系。第二个提示旨在请求 ChatGPT 推断目标对象。最后，我们可以从 ChatGPT 的响应中获取目标对象的边界框。

【【【感觉还是有点鸡肋了，如果就是想让他坐到哪里去，直接点击那个物体，训练的时候就改为编号，感觉会更好，然后就寻路过去，坐椅子上啥的】，这样的话这个GPT感觉就没啥用。

就是改为，从椅子1站起，并走到桌子4的位置坐下这种】】】





**4.2. 基于扩散的轨迹生成**

在从 ChatGPT 获取到定位对象后，我们首先根据指令生成轨迹，然后合成局部人类姿态。**轨迹被定义为角色的平移和方向序列**。如 [51] 所建议的，并不是场景中的每一个点都与最终的人类运动相关。受到 NSM [70] 和 ManipNet [92] 的启发，我们在目标对象周围使用**体积传感器（如图 4 所示）来表示场景。**

![image-20240909163611579](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\image-20240909163611579.png)

图 4. 环境传感器、目标传感器和轨迹传感器的可视化。

目标传感器 (b) 提供了目标对象的详细几何信息。

环境传感器 (c) 提供了目标对象周围的粗略空间信息。

轨迹传感器 (d) 位于人体周围。



​	**以对象为中心的场景表示法。** 将目标对象的中心位置表示为 $c_o = (c_x, c_y, c_z)$。我们将场景的点云 $S$ 转换为以 $c_o$ 为中心的对象坐标轴。为了识别目标对象周围的几何形状，我们创建名为环境传感器和目标传感器的体积传感器。

​	**环境传感器。** 环境传感器以 $c_o$ 为中心，呈立方体形状，体积为 $4 \times 4 \times 4 , m^3$，包含 $8 \times 8 \times 8$ 个立方体体素，如图 4 (c) 所示。它通过收集每个体素的所有占用率 $o_s$、中心位置 $c_v$ 和法向方向 $n_v$ 来形成特征向量 $E$。与 [70] 类似，每个体素中的场景占用率 $o_s \in R^1$ 是基于给定的场景网格定义的：

![image-20240909164319165](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\assets\image-20240909164319165.png)

$o_s = \begin{cases} 1 & \text{if } d_s < 0 \ 0 & \text{if } d_s > a_s \ 1 - \frac{d_s}{a_s} & \text{otherwise} \end{cases}$

​	其中 $d_s$ 是场景网格与体素中心之间的符号距离，$a_s$ 是体素边长。$n_v$ 是离体素中心最近的场景点的法向方向。环境传感器提供了目标对象周围粗略的场景几何信息。

​	**目标传感器。** 为了捕捉目标对象的详细几何形状，我们进一步构建了目标传感器。由于我们在第 4.1 节中已经获得了目标对象的 3D 边界框，因此我们根据边界框裁剪点云，并构建覆盖该边界框的 $8 \times 8 \times 8$ 立方体体积，如图 4 (b) 所示。目标传感器 $T$ 的形式与环境传感器 $E$ 相同，只是目标传感器具有不同的体素大小，如图 4 (b) 和 (c) 所示。

​	给定构建的以对象为中心的表示，我们遵循 [57] 使用变压器解码器架构 [79]，这使得生成任意长度的运动成为可能。对于文本输入，我们使用 CLIP [62] 文本编码器将输入文本编码为文本特征 $L$。时间步骤 $t$ 以正弦位置嵌入的形式注入到解码器中 [79]。总之，该生成模型的条件为：

$C_t = {L, E, T},$

​	其中 $L$ 是文本特征，$E$ 是环境传感器，$T$ 是目标传感器。所有条件通过 MLP 投影到相同的维度 $D = 512$，并与位置嵌入相加形成标记。我们使用第 2 式中描述的简单目标来训练轨迹生成模型 $G_r$，以生成长度为 $N$ 的轨迹 $r_{1:N}$。

**4.3. 基于扩散的运动补全**

​	给定第 4.2 节中的轨迹，下一步是完成整个动作。基于生成的轨迹，我们构建了一个轨迹传感器 $O$，以明确推理角色与场景之间的交互。

​	**轨迹传感器。** 轨迹传感器也是一种体积传感器，其形式与环境传感器类似，但设计用于自我中心感知 [95]，如图 4 (d) 所示。具体来说，轨迹传感器被放置在每帧的角色周围。这个传感器 $O_i$ 位于第 $i$ 帧的预测根位置中心，并面向第 $i$ 帧的预测根方向，包含 $8 \times 8 \times 8$ 个存储场景占用率的立方体体素。占用率的计算与式 (3) 相同。

​	另一种基于变压器的条件扩散模型用于合成沿着轨迹的局部姿态。其条件定义为：

​	$C_m = {L, E, T, O_1, \ldots, O_N},$

​	其中 $L$、$E$ 和 $T$ 在式 (4) 中具有相同的含义。使用第 2 式中描述的简单目标来训练运动生成模型 $G_m$，以生成长度为 $N$ 的全局方向 $\gamma_{1:N}$ 和局部姿态 $\theta_{1:N}$。

**4.4. 实施细节**

​	遵循 [38, 57, 80, 81]，我们对轨迹生成和运动生成使用单独的扩散模型。由于 HUMANISE 数据集 [83] 包含来自 AMASS 数据集（3772 分钟） [49] 的相对较少数量（51 分钟）的纯运动样本，因此我们首先在整个 AMASS 数据集上进行 200 个周期的预训练，然后在 HUMANISE 数据集上进行 200 个周期的微调，以提高运动质量。在预训练期间，文本特征 $L$ 被设置为全零。两个模型均使用 AdamW 优化器 [40] 进行训练，在单个 Nvidia RTX 3090 GPU 上使用学习率 0.0001。批量大小设置为 128。ChatGPT 的版本是 **gpt-3.5-turbo**。更多细节可以在补充材料中找到。

**5. 实验**

**5.1 评估指标**

我们从三个方面评估生成的动作：场景条件、动作条件和纯运动质量。

**场景条件运动质量**。为了评估生成的动作与场景的对齐程度，我们计算身体到目标的距离（goal dist.）[83]来测量角色与目标物体交互的准确性。然而，goal dist. 并未考虑整个动作与场景的一致性（例如，以不正确的方向坐在沙发上）。为了解决 goal dist. 的不足，进行了一项人类感知研究（通过场景评分表示），随机抽样每个模型的20个场景，需要十名工作者对每个样本进行评分。

**动作条件运动质量**。为了衡量生成的动作与文本的对齐程度，我们遵循 [17] 的方法评估动作识别准确率（accuracy）、多样性和结果的多模态性。这些指标的计算依赖于一个预训练的动作识别模型 [87]，并且我们在 HUMANISE 数据集上训练识别模型。

**纯运动质量**。我们使用 [17] 提出的指标来评估生成动作的真实感，即弗雷歇距离（Frechet Inception Distance, FID）。更低的 FID 表明生成的动作更接近真实动作。我们还进行了一项人类感知研究（通过质量评分表示）来衡量纯运动质量。

**5.2 从文本和场景生成人体动作**

我们的实验是在 HUMANISE 数据集 [83] 上进行的。按照之前的设置 [83]，训练集中有 16.5k 个动作序列，涵盖 543 个场景，测试集中有 3.1k 个动作序列，涵盖 100 个场景。我们将我们的方法与四个基线进行了比较：（1）MDM∗ [75]：基于扩散的动作生成器。（2）GMD∗ [38]：GMD 提出了多种增强 MDM 控制和质量的技术。我们采用 GMD∗ 的单阶段设置。类似于 HUMANISE，我们使用点转换器为 MDM∗ 和 GMD∗ 提供场景特征。（3）GMDHC：我们提供 HUMANISE 预测的物体中心（记作 HC）以指导 GMD 中的动作生成过程，通过添加邻近损失鼓励动作靠近预测的物体中心。邻近损失定义为 HC 到交互帧中预测的人体骨盆的距离。（4）HUMANISE：我们直接使用他们发布的模型。

定量结果如表 1 所示。我们的方法在 goal dist.、场景评分、准确率、FID 和质量评分等方面优于基线，并在多样性和多模态性方面取得了竞争性结果。定量结果表明，我们的方法可以生成更高质量的动作，并且在所有条件下更加一致。定性结果展示在图 5 中。更多可视化内容请参见补充材料。

**7. 结论**

在本研究中，我们提出了一种从场景中的文本生成动作的新方法。为了解决这个问题，我们提出了一个两步法。第一步涉及 3D 视觉定位，在这一过程中我们识别目标对象。在第二步中，针对目标对象，我们构建了一种基于扩散的动作生成方法。我们的方法具有多个优点，包括提高了 3D 视觉定位的准确性和动作质量。



## 其他相关工作

##### GMD: Guided Motion Diffusion for Controllable Human Motion Synthesis

![image-20240910135145754](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\assets\image-20240910135145754.png)

https://github.com/korrawe/guided-motion-diffusion

研究 [38] 可以在行走时避免障碍物



##### Synthesizing Diverse Human Motions in 3D Indoor Scenes

https://arxiv.org/pdf/2305.12411

code:https://github.com/zkf1997/DIMOS

主页：https://zkf1997.github.io/DIMOS/



##### PartwiseMPC: Interactive Control of Contact-Guided Motions

 https://www.youtube.com/watch?v=wdL9EvkIWZo

![image-20240910141458489](D:\myNote\Postgraduate\MotionGenerate\Generating Human Motion in 3D Scenes from Text Descriptions 场景和文本\assets\image-20240910141458489.png)





任务：跑起来

if跑起来 午可洗

