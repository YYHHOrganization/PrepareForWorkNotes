# MotionLCM è·‘ä»£ç  çœ‹ä»£ç  å’Œæ”¹ä»£ç ç­‰



# å…³é”®ä»£ç 

```python
teacher_unet = base_model.denoiser # teacher modelæ˜¯base

unet = instantiate_from_config(cfg.model.denoiser) # è¦è®­ç»ƒçš„LCM

target_unet = instantiate_from_config(cfg.model.denoiser) #


```



éªŒè¯ä»£ç å¦‚FIDä½ç½®

D:\_Postgraduate\motionGen\MotionLCM\MotionLCM\mld\models\metrics\utils.py



## æ­¥éª¤

æœåŠ¡å™¨å¼€æœºï¼š

![image-20250425153454160](assets/image-20250425153454160.png)

æ‰“å¼€vscode

ç‚¹å‡»å·¦ä¸‹è§’ä¹‹å‰è¿æ¥è¿‡çš„

![image-20250425153531089](assets/image-20250425153531089.png)

æ‰“å¼€æ–‡ä»¶

è¿›å…¥è¾“å¯†ç ç¯èŠ‚ï¼ˆå¤åˆ¶ä¸Šä¸Šå›¾çš„å¯†ç ï¼‰

![image-20250425153437777](assets/image-20250425153437777.png)

```
conda activate motionlcm
```





å¦‚ä½•è·‘å‡ºæ¥çš„ç»†èŠ‚è¯·çœ‹ï¼š

D:\myNote\ppNotes\PrepareForWorkNotes\AI\è®ºæ–‡é˜…è¯»å’ŒåŸºç¡€çŸ¥è¯†\å¦‚ä½•ä½¿ç”¨AutoDLè·‘ä»£ç ï¼Ÿ.md



```
MotionLCM
â”œâ”€â”€ configs
â”œâ”€â”€ configs_v1
â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ humanml3d
â”‚   â”‚   â”œâ”€â”€ new_joint_vecs
â”‚   â”‚   â”œâ”€â”€ new_joints
â”‚   â”‚   â”œâ”€â”€ texts
â”‚   â”‚   â”œâ”€â”€ Mean.npy
â”‚   â”‚   â”œâ”€â”€ Std.npy
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ humanml_spatial_norm
â”‚       â”œâ”€â”€ Mean_raw.npy
â”‚       â””â”€â”€ Std_raw.npy
â”œâ”€â”€ deps
â”‚   â”œâ”€â”€ glove
â”‚   â”œâ”€â”€ sentence-t5-large
|   â”œâ”€â”€ smpl_models
â”‚   â””â”€â”€ t2m
â”œâ”€â”€ experiments_control
â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â””â”€â”€ motionlcm_humanml
â”‚   â”‚       â”œâ”€â”€ motionlcm_humanml_s_all.ckpt
â”‚   â”‚       â””â”€â”€ motionlcm_humanml_s_pelvis.ckpt
â”‚   â””â”€â”€ temproal
â”‚   â”‚   â””â”€â”€ motionlcm_humanml
â”‚   â”‚       â”œâ”€â”€ motionlcm_humanml_t_v1.ckpt
â”‚   â”‚       â””â”€â”€ motionlcm_humanml_t.ckpt
â”œâ”€â”€ experiments_recons
â”‚   â””â”€â”€ vae_humanml
â”‚       â””â”€â”€ vae_humanml.ckpt
â”œâ”€â”€ experiments_t2m
â”‚   â”œâ”€â”€ mld_humanml
â”‚   â”‚   â”œâ”€â”€ mld_humanml_v1.ckpt
â”‚   â”‚   â””â”€â”€ mld_humanml.ckpt
â”‚   â””â”€â”€ motionlcm_humanml
â”‚       â”œâ”€â”€ motionlcm_humanml_v1.ckpt
â”‚       â””â”€â”€ motionlcm_humanml.ckpt
â”œâ”€â”€ ...
```

æœåŠ¡å™¨ä¸Šçš„ä»£ç æœ‰è¿™äº›ç»“æ„

![image-20250425155302381](assets/image-20250425155302381.png)

---

### **MotionLCMä»£ç ç»“æ„ç²¾è¦è§£æä¸é˜…è¯»è·¯çº¿**

---

#### **1. æ ¸å¿ƒç›®å½•ä¼˜å…ˆçº§æ’åº**ï¼ˆæ ¹æ®ä½ çš„å®æ—¶æ§åˆ¶éœ€æ±‚ï¼‰

| ç›®å½•/æ–‡ä»¶                            | ä½œç”¨è§£æ                                     | å¿…è¯»æŒ‡æ•° | æ¨èé˜…è¯»æ—¶é—´ |
| ------------------------------------ | -------------------------------------------- | -------- | ------------ |
| **`configs/`**                       | æ‰€æœ‰æ¨¡å‹è¶…å‚æ•°å®šä¹‰ï¼ˆæ‰©æ•£æ­¥æ•°ã€éšç©ºé—´ç»´åº¦ç­‰ï¼‰ | â˜…â˜…â˜…â˜…â˜…    | 1å°æ—¶        |
| **`mld/models/modeltype/mld.py`**    | ä¸»æ¨¡å‹æ¶æ„ï¼ˆæ‰©æ•£è¿‡ç¨‹+æ§åˆ¶é€»è¾‘ï¼‰              | â˜…â˜…â˜…â˜…â˜…    | 3å°æ—¶        |
| **`datasets/humanml3d/new_joints/`** | åŸå§‹è¿åŠ¨æ•°æ®æ ¼å¼ï¼ˆUnityéœ€å¯¹é½ï¼‰              | â˜…â˜…â˜…â˜…â˜†    | 2å°æ—¶        |
| **`experiments_control/spatial/`**   | ControlNetæƒé‡æ–‡ä»¶ï¼ˆå®æ—¶æ§åˆ¶æ ¸å¿ƒï¼‰           | â˜…â˜…â˜…â˜…â˜†    | 1å°æ—¶        |
| **`deps/smpl_models/`**              | SMPLäººä½“æ¨¡å‹å‚æ•°ï¼ˆéœ€è½¬æ¢åˆ°Unityéª¨éª¼ï¼‰        | â˜…â˜…â˜…â˜†â˜†    | 30åˆ†é’Ÿ       |

---

#### **2. å…³é”®è·¯å¾„è¯¦è§£**

1. **é…ç½®å…¥å£**  
   
   ```bash
   configs/
   â””â”€â”€ motionlcm.yaml       # ä¸»é…ç½®æ–‡ä»¶ï¼ˆæ‰©æ•£æ­¥æ•°ã€å­¦ä¹ ç‡ç­‰ï¼‰
   configs_v1/              # æ—§ç‰ˆé…ç½®ï¼ˆå¯å¿½ç•¥ï¼‰
   ```
   **é‡ç‚¹å‚æ•°**ï¼š
   ```yaml
   model:
     denoiser:
       params:
         timesteps: 50      # æ‰©æ•£æ¨¡å‹æ€»æ­¥æ•° â†’ å½±å“ç”Ÿæˆé€Ÿåº¦
         nfe: 4             # å®é™…æ¨ç†æ­¥æ•° â†’ å®æ—¶æ€§å…³é”®å‚æ•°
   ```
   
2. **æ•°æ®ç®¡é“**  
   ```bash
   datasets/humanml3d/
   â”œâ”€â”€ new_joints/          # åŸå§‹éª¨éª¼æ•°æ®ï¼ˆ.npyæ ¼å¼ï¼‰
   â”‚   â””â”€â”€ 002051.npy       # å½¢çŠ¶ï¼š(120, 22, 3) â†’ (å¸§, å…³èŠ‚, åæ ‡)
   â”œâ”€â”€ texts/               # æ–‡æœ¬-åŠ¨ä½œå¯¹é½æè¿°
   â”œâ”€â”€ Mean.npy             # æ•°æ®æ ‡å‡†åŒ–å‡å€¼
   â””â”€â”€ Std.npy              # æ•°æ®æ ‡å‡†åŒ–æ–¹å·®
   ```
   **ç§»æ¤é‡ç‚¹**ï¼šéœ€å°†`new_joints`çš„22å…³èŠ‚è½¬æ¢ä¸ºUnityçš„Humanoidéª¨éª¼ç³»ç»Ÿ

3. **é¢„è®­ç»ƒæ¨¡å‹**  
   ```bash
   experiments_control/spatial/motionlcm_humanml/
   â”œâ”€â”€ motionlcm_humanml_s_all.ckpt     # å…¨èº«æ§åˆ¶æ¨¡å‹
   â””â”€â”€ motionlcm_humanml_s_pelvis.ckpt  # éª¨ç›†åŒºåŸŸæ§åˆ¶
   ```
   **æ§åˆ¶é€»è¾‘**ï¼šè¿™äº›CheckpointåŒ…å«ControlNetç»“æ„ï¼Œå¯é€šè¿‡`demo.py`çš„`--control_type`å‚æ•°è°ƒç”¨

4. **äººä½“æ¨¡å‹å‚æ•°**  
   ```bash
   deps/smpl_models/
   â”œâ”€â”€ SMPL_NEUTRAL.pkl     # ä¸­æ€§ä½“å‹æ¨¡å‹
   â””â”€â”€ smplh/               # å«æ‰‹éƒ¨ç»†èŠ‚çš„æ¨¡å‹
   ```
   **è½¬æ¢å·¥å…·**ï¼šéœ€ä½¿ç”¨`SMPLX-Unity-Converter`ç­‰å·¥å…·è½¬æ¢åˆ°Unityå¯ç”¨æ ¼å¼

---

#### **3. ä»£ç é˜…è¯»è·¯çº¿å›¾**

**é˜¶æ®µä¸€ï¼šç†è§£æ•°æ®æµï¼ˆ1å¤©ï¼‰**
```python
# åœ¨`mld/data/get_data.py`ä¸­ï¼š
dataset = get_dataset(cfg)  # æ•°æ®åŠ è½½å…¥å£
batch = dataset[0]          # å…¸å‹æ•°æ®ç»“æ„ï¼š
                            # {
                            #   'text': "walking", 
                            #   'length': 120,
                            #   'joints': [120, 22, 3],
                            #   'hint': [120, 4]  # æ§åˆ¶ä¿¡å·
                            # }
```

**é˜¶æ®µäºŒï¼šæŒæ¡ç”Ÿæˆé€»è¾‘ï¼ˆ2å¤©ï¼‰**
```python
# åœ¨`mld/models/modeltype/mld.py`ä¸­ï¼š
def _diffusion_forward(self, batch):
    # å…³é”®æ‰©æ•£è¿‡ç¨‹ï¼ˆéœ€ä¼˜åŒ–éƒ¨åˆ†ï¼‰
    for t in reversed(range(0, timesteps)):
        x = denoiser(x, t, cond)  # é™å™ªUNet
        if use_controlnet:        # å®æ—¶æ§åˆ¶åˆ†æ”¯
            x += controlnet(x, hint)
```

**é˜¶æ®µä¸‰ï¼šæ§åˆ¶ä¿¡å·å¯¹æ¥ï¼ˆ1å¤©ï¼‰**
```bash
# è¿è¡ŒControlNetç¤ºä¾‹ï¼š
python demo.py \
  --control_type pelvis \
  --example assets/control_hints.txt  # æ§åˆ¶ä¿¡å·æ–‡ä»¶æ ¼å¼ï¼š
                                      # 120 0 0.5  # å¸§æ•° æ§åˆ¶ç±»å‹ å¼ºåº¦
                                      # 60 1 0.8
```

---

#### **4. Unityç§»æ¤å…³é”®æ­¥éª¤**

1. **æ•°æ®æ ¼å¼è½¬æ¢**  
   ```csharp
   // Unity C#ç¤ºä¾‹ï¼šå°†22å…³èŠ‚æ•°æ®è½¬æ¢ä¸ºHumanoid
   void ConvertToHumanoid(float[,,] joints) {
       hips.position = joints[:,0,:];  // 0å·å…³èŠ‚ä¸ºéª¨ç›†
       spine.position = joints[:,1,:];
       // ...å…¶ä»–å…³èŠ‚å¯¹åº”å…³ç³»éœ€æ ¹æ®SMPLå®šä¹‰è°ƒæ•´
   }
   ```

2. **å®æ—¶æ§åˆ¶æ¥å£**  
   ```python
   # ä¿®æ”¹demo.pyæ·»åŠ SocketæœåŠ¡ç«¯
   import socket
   server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
   server.bind(('localhost', 12345))
   while True:
       conn.send(joints.tobytes())  # å®æ—¶å‘é€å…³èŠ‚æ•°æ®åˆ°Unity
   ```

3. **æ€§èƒ½ä¼˜åŒ–æŠ€å·§**  
   ```python
   # åœ¨configs/motionlcm.yamlä¸­ï¼š
   model:
     denoiser:
       params:
         nfe: 4 â†’ 2   # å‡å°‘æ‰©æ•£æ­¥æ•°
         quantize: true  # æ·»åŠ 8ä½é‡åŒ–
   ```

---

#### **5. è®ºæ–‡åˆ›æ–°ç‚¹æç‚¼æ–¹å‘**

1. **å®æ—¶æ€§ä¼˜åŒ–**
   - æå‡º**åŠ¨æ€æ­¥æ•°æ‰©æ•£**ï¼ˆæ ¹æ®æ§åˆ¶ä¿¡å·å¼ºåº¦è‡ªåŠ¨è°ƒèŠ‚nfeï¼‰
   - å®ç°**CUDAæ ¸å‡½æ•°åŠ é€Ÿ**çš„å…³é”®æ‰©æ•£æ­¥éª¤

2. **è·¨å¼•æ“é€‚é…**
   - è®¾è®¡**SMPL-to-Unityè‡ªåŠ¨éª¨éª¼æ˜ å°„**ç®—æ³•
   - å¼€å‘**Motion Stream Protocol**å®æ—¶ä¼ è¾“åè®®

3. **ç‰©ç†å¢å¼º**
   - åœ¨ControlNetä¸­æ·»åŠ **ç‰©ç†çº¦æŸæŸå¤±é¡¹**
   - é›†æˆ**Unity Physicsè¿›è¡Œè¿åŠ¨ä¿®æ­£**

---

#### **ä¸‹ä¸€æ­¥å»ºè®®**
1. ä»`demo.py`çš„`main()`å‡½æ•°å¼€å§‹ï¼Œç”¨PyCharmçš„**è°ƒè¯•æ¨¡å¼**é€è¡Œè·Ÿè¸ªæ•°æ®æµ
2. ä¿®æ”¹`configs/motionlcm.yaml`ä¸­çš„`nfe`å‚æ•°ï¼Œè§‚å¯Ÿç”Ÿæˆé€Ÿåº¦/è´¨é‡å˜åŒ–
3. ä½¿ç”¨Unityçš„**Articulation Bodyç»„ä»¶**æ›¿æ¢é»˜è®¤éª¨éª¼ï¼Œæµ‹è¯•ç‰©ç†è¡¨ç°





# æ–‡ä»¶ç»“æ„ Folder Structure



æˆ‘æ­£åœ¨å°è¯•è¯»æ‡‚è¿™ä¸ªMotionLCMï¼ˆhttps://github.com/Dai-Wenxun/MotionLCMï¼‰ï¼Œæˆ‘è¦åŸºäºè¿™ä¸ªå·¥ä½œåšæ–°çš„ç ”ç©¶ï¼Œè¿™æ˜¯è¿™ç¯‡MotionLCMè®ºæ–‡çš„ä¸€äº›å†…å®¹ï¼š

MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model  



Abstract. This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatialtemporal control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation,  building on the motion latent diffusion model. By adopting one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (i.e.

, initial motions) in the vanilla motion space to further provide supervision for the training process. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency.



Keywords: Text-to-Motion Â· Real-time Control Â· Consistency Model



3 Method



In this section, we first briefly introduce preliminaries about latent consistency models in Sec. 3.1. Then, we describe how to conduct latent consistency distillation for motion generation in Sec. 3.2, followed by our implementation of motion control in latent space in Sec. 3.3. The overall pipeline is illustrated in Fig. 4



1. **æ½œåœ¨ç©ºé—´å‹ç¼©ä¸kæ­¥è’¸é¦**ï¼šé€šè¿‡VAEå‹ç¼©è¿åŠ¨è‡³ä½ç»´ç©ºé—´ï¼Œå¹¶è®¾è®¡kæ­¥è·³è·ƒä¸€è‡´æ€§è’¸é¦ç­–ç•¥ï¼Œ**å°†æ¨ç†é€Ÿåº¦æå‡è‡³å•æ­¥çº§åˆ«**ï¼›  
2. **åŠ¨æ€CFGä¸EMAä¼˜åŒ–**ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­èåˆåŠ¨æ€CFGå¼ºåº¦ç³»æ•°ï¼Œç»“åˆEMAå‚æ•°æ›´æ–°æœºåˆ¶ï¼Œ**å®ç°æ¡ä»¶å¯¹é½ä¸ç”Ÿæˆæ•ˆç‡çš„åŒé‡çªç ´**ã€‚



1. **ControlNetä¸è½¨è¿¹ç¼–ç èåˆ**ï¼šé€šè¿‡æ½œåœ¨ç©ºé—´å¼•å…¥å¯è®­ç»ƒControlNetæ¶æ„ï¼Œç»“åˆè½¨è¿¹ç¼–ç å™¨çš„æ—¶ç©ºç‰¹å¾æå–èƒ½åŠ›ï¼Œ**å®ç°é«˜å“åº”é€Ÿåº¦çš„å…³èŠ‚çº§è¿åŠ¨æ§åˆ¶**ï¼›  
2. **åŒç©ºé—´ç›‘ç£æœºåˆ¶**ï¼šåœ¨æ½œåœ¨ç©ºé—´é‡å»ºæŸå¤±åŸºç¡€ä¸Šï¼Œåˆ›æ–°æ€§å¼•å…¥è¿åŠ¨ç©ºé—´æ§åˆ¶æŸå¤±ï¼Œ**çªç ´æ½œåœ¨ç©ºé—´ç›‘ç£ç“¶é¢ˆï¼Œæ˜¾è‘—æå‡æ§åˆ¶ä¿¡å·å¯¹é½ç²¾åº¦**ã€‚

# 5 ç»“è®º  

æœ¬æ–‡æå‡º**é«˜æ•ˆå¯æ§è¿åŠ¨ç”Ÿæˆæ¡†æ¶MotionLCM**ï¼Œé€šè¿‡**æ½œåœ¨ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯**å®ç°ç”Ÿæˆé€Ÿåº¦ä¸è´¨é‡çš„å¹³è¡¡ï¼Œå¹¶å€ŸåŠ©**æ½œåœ¨ç©ºé—´è¿åŠ¨æ§åˆ¶ç½‘ç»œ**å®ç°ç²¾å‡†æ¡ä»¶æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼š

- å®æ—¶ç”Ÿæˆé€Ÿåº¦è¾¾$$30\text{ms}/\text{åºåˆ—}$$ï¼ˆæ¯”MLDå¿«13å€ï¼‰
- å¤šå…³èŠ‚æ§åˆ¶è¯¯å·®é™ä½46%ï¼ˆLoc. err. 0.38â†’0.21ï¼‰
- æ¶ˆèå®éªŒéªŒè¯**åŠ¨æ€è®­ç»ƒå¼•å¯¼èŒƒå›´**ï¼ˆ$$w\in[5,15]$$ï¼‰ä¸**HuberæŸå¤±**çš„å…³é”®ä½œç”¨

**å±€é™æ€§**ï¼šç”±äºMLDçš„VAEç¼ºä¹æ˜¾å¼æ—¶é—´å»ºæ¨¡ï¼ŒMotionLCMæ— æ³•å®ç°è‰¯å¥½çš„æ—¶é—´ä¸€è‡´æ€§è§£é‡Šã€‚æœªæ¥å·¥ä½œå°†èšç„¦äºè®¾è®¡**å¯è§£é‡Šçš„å‹ç¼©æ¶æ„**ä»¥æå‡æ—¶åºæ§åˆ¶èƒ½åŠ›ã€‚

---





è¿™æ˜¯è¿™ä¸ªè®ºæ–‡çš„**6. Folder Structure**

After the whole setup pipeline, the folder structure will look like:

```
MotionLCM
â”œâ”€â”€ configs
â”œâ”€â”€ configs_v1
â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ humanml3d
â”‚   â”‚   â”œâ”€â”€ new_joint_vecs
â”‚   â”‚   â”œâ”€â”€ new_joints
â”‚   â”‚   â”œâ”€â”€ texts
â”‚   â”‚   â”œâ”€â”€ Mean.npy
â”‚   â”‚   â”œâ”€â”€ Std.npy
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ humanml_spatial_norm
â”‚       â”œâ”€â”€ Mean_raw.npy
â”‚       â””â”€â”€ Std_raw.npy
â”œâ”€â”€ deps
â”‚   â”œâ”€â”€ glove
â”‚   â”œâ”€â”€ sentence-t5-large
|   â”œâ”€â”€ smpl_models
â”‚   â””â”€â”€ t2m
â”œâ”€â”€ experiments_control
â”‚   â”œâ”€â”€ spatial
â”‚   â”‚   â””â”€â”€ motionlcm_humanml
â”‚   â”‚       â”œâ”€â”€ motionlcm_humanml_s_all.ckpt
â”‚   â”‚       â””â”€â”€ motionlcm_humanml_s_pelvis.ckpt
â”‚   â””â”€â”€ temproal
â”‚   â”‚   â””â”€â”€ motionlcm_humanml
â”‚   â”‚       â”œâ”€â”€ motionlcm_humanml_t_v1.ckpt
â”‚   â”‚       â””â”€â”€ motionlcm_humanml_t.ckpt
â”œâ”€â”€ experiments_recons
â”‚   â””â”€â”€ vae_humanml
â”‚       â””â”€â”€ vae_humanml.ckpt
â”œâ”€â”€ experiments_t2m
â”‚   â”œâ”€â”€ mld_humanml
â”‚   â”‚   â”œâ”€â”€ mld_humanml_v1.ckpt
â”‚   â”‚   â””â”€â”€ mld_humanml.ckpt
â”‚   â””â”€â”€ motionlcm_humanml
â”‚       â”œâ”€â”€ motionlcm_humanml_v1.ckpt
â”‚       â””â”€â”€ motionlcm_humanml.ckpt
â”œâ”€â”€ ...
```



è¯·é€è¡Œç¿»è¯‘è¿™ä¸ªFolder Structureï¼Œç»“åˆæˆ‘è·Ÿä½ è¯´çš„ï¼Œä»¥åŠä½ è‡ªå·±çš„çŸ¥è¯†ï¼Œå‘Šè¯‰æˆ‘è¿™ä¸ªæ¯ä¸ªæ–‡ä»¶ç»“æ„æ˜¯åœ¨å¹²å˜›ï¼Œå…·ä½“æ ‡æ³¨åœ¨ä¸Šé¢





ä»¥ä¸‹æ˜¯ç»“åˆè®ºæ–‡å†…å®¹å’Œé¡¹ç›®ç»“æ„çš„é€è¡Œç¿»è¯‘ä¸åŠŸèƒ½è§£æï¼ˆ`ğŸ“Œ`æ ‡æ³¨æ ¸å¿ƒåŠŸèƒ½ï¼‰ï¼š

```python
MotionLCM
â”œâ”€â”€ configs                     # åŸºç¡€é…ç½®æ–‡ä»¶ç›®å½•ï¼ˆæ¨¡å‹è¶…å‚æ•°/è®­ç»ƒè®¾ç½®ï¼‰
â”œâ”€â”€ configs_v1                  # ç‰ˆæœ¬1çš„å¤‡ç”¨é…ç½®ï¼ˆå¯èƒ½ç”¨äºæ¶ˆèå®éªŒï¼‰
â”œâ”€â”€ datasets                    # æ•°æ®é›†å­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ humanml3d               # HumanML3Dæ•°æ®é›†ï¼ˆæ–‡æœ¬-è¿åŠ¨é…å¯¹æ•°æ®ï¼‰
â”‚   â”‚   â”œâ”€â”€ new_joint_vecs      # é¢„å¤„ç†åçš„å…³èŠ‚å‘é‡ï¼ˆVAEç¼–ç è¾“å…¥ï¼‰
â”‚   â”‚   â”œâ”€â”€ new_joints          # åŸå§‹å…³èŠ‚åæ ‡æ•°æ®ï¼ˆSMPLæ ¼å¼ï¼‰
â”‚   â”‚   â”œâ”€â”€ texts               # æ–‡æœ¬æè¿°æ–‡ä»¶ï¼ˆæ¡ä»¶ç”Ÿæˆè¾“å…¥ï¼‰ğŸ“Œ
â”‚   â”‚   â”œâ”€â”€ Mean.npy            # æ•°æ®æ ‡å‡†åŒ–å‡å€¼ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
â”‚   â”‚   â”œâ”€â”€ Std.npy             # æ•°æ®æ ‡å‡†åŒ–æ–¹å·® ğŸ“Œ
â”‚   â””â”€â”€ humanml_spatial_norm    # ç©ºé—´å½’ä¸€åŒ–æ•°æ®ï¼ˆControlNetä¸“ç”¨ï¼‰
â”‚       â”œâ”€â”€ Mean_raw.npy        # åŸå§‹ç©ºé—´å‡å€¼ï¼ˆå…³èŠ‚è½¨è¿¹ç»Ÿè®¡ï¼‰
â”‚       â””â”€â”€ Std_raw.npy         # åŸå§‹ç©ºé—´æ–¹å·®
â”œâ”€â”€ deps                        # ç¬¬ä¸‰æ–¹ä¾èµ–åº“/æ¨¡å‹
â”‚   â”œâ”€â”€ glove                   # GloVeè¯å‘é‡ï¼ˆæ–‡æœ¬ç¼–ç ï¼‰
â”‚   â”œâ”€â”€ sentence-t5-large       # Sentence-T5æ–‡æœ¬ç¼–ç å™¨ ğŸ“Œï¼ˆè®ºæ–‡æåˆ°çš„æ¡ä»¶ç¼–ç ï¼‰
â”‚   â”œâ”€â”€ smpl_models             # SMPLäººä½“æ¨¡å‹å‚æ•°ï¼ˆè¿åŠ¨å¯è§†åŒ–ç”¨ï¼‰
â”‚   â””â”€â”€ t2m                     # Text-to-MotionåŸºç¡€æ¨¡å‹ï¼ˆå¯èƒ½å«MLDï¼‰
------------------------------------------------------------ControlNet-------------------------
â”œâ”€â”€ experiments_control         # æ§åˆ¶æ¨¡å—å®éªŒè®°å½• 
â”‚   â”œâ”€â”€ spatial                 # ç©ºé—´æ§åˆ¶ï¼ˆå¦‚å…³èŠ‚è½¨è¿¹ï¼‰
â”‚   â”‚   â””â”€â”€ motionlcm_humanml   
â”‚   â”‚       â”œâ”€â”€ motionlcm_humanml_s_all.ckpt      # å…¨å…³èŠ‚æ§åˆ¶æ¨¡å‹ ğŸ“Œï¼ˆControlNetï¼‰
â”‚   â”‚       â””â”€â”€ motionlcm_humanml_s_pelvis.ckpt  # éª¨ç›†ä¼˜å…ˆæ§åˆ¶ï¼ˆè®ºæ–‡3.3èŠ‚ï¼‰
â”‚   â””â”€â”€ temproal                # æ—¶åºæ§åˆ¶ï¼ˆæ‹¼å†™é”™è¯¯ï¼Œåº”ä¸ºtemporalï¼‰
â”‚       â””â”€â”€ motionlcm_humanml   
â”‚           â”œâ”€â”€ motionlcm_humanml_t_v1.ckpt       # æ—¶åºæ§åˆ¶v1ï¼ˆé€Ÿåº¦/èŠ‚å¥ï¼‰
â”‚           â””â”€â”€ motionlcm_humanml_t.ckpt          # æœ€ç»ˆæ—¶åºæ¨¡å‹
â”œâ”€â”€ experiments_recons          # VAEé‡å»ºå®éªŒ
â”‚   â””â”€â”€ vae_humanml             
â”‚       â””â”€â”€ vae_humanml.ckpt    # è¿åŠ¨VAEæ¨¡å‹ ğŸ“Œï¼ˆè®ºæ–‡3.1èŠ‚æ½œåœ¨ç©ºé—´å‹ç¼©ï¼‰
â”œâ”€â”€ experiments_t2m             # æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆæ¨¡å‹
â”‚   â”œâ”€â”€ mld_humanml             # åŸºçº¿æ¨¡å‹MLDï¼ˆMotion Latent Diffusionï¼‰
â”‚   â”‚   â”œâ”€â”€ mld_humanml_v1.ckpt # MLDåˆå§‹ç‰ˆæœ¬
â”‚   â”‚   â””â”€â”€ mld_humanml.ckpt    # è®ºæ–‡å¯¹æ¯”çš„MLDåŸºå‡† ğŸ“Œ
â”‚   â””â”€â”€ motionlcm_humanml       # æœ¬æ–‡æå‡ºçš„æ¨¡å‹
â”‚       â”œâ”€â”€ motionlcm_humanml_v1.ckpt  # ä¸€è‡´æ€§è’¸é¦v1ï¼ˆåŠ¨æ€CFGæµ‹è¯•ï¼‰
â”‚       â””â”€â”€ motionlcm_humanml.ckpt     # æœ€ç»ˆæ¨¡å‹ ğŸ“Œï¼ˆå•æ­¥æ¨ç†30msï¼‰
```

å…³é”®å¯¹åº”å…³ç³»ï¼š
1. **VAEå‹ç¼©** â†’ `experiments_recons/vae_humanml.ckpt`  
2. **ControlNet** â†’ `experiments_control/spatial/` ä¸‹çš„`.ckpt`æ–‡ä»¶  
3. **åŠ¨æ€CFG** â†’ `motionlcm_humanml_v1.ckpt` çš„æ¶ˆèç‰ˆæœ¬  
4. **åŒç©ºé—´ç›‘ç£** â†’ é€šè¿‡`new_joint_vecs`ï¼ˆæ½œåœ¨ç©ºé—´ï¼‰å’Œ`new_joints`ï¼ˆè¿åŠ¨ç©ºé—´ï¼‰è”åˆè®­ç»ƒ





å»ºè®®æ–°ç ”ç©¶å…³æ³¨ï¼š

1. ä¿®æ”¹configs_v1è°ƒæ•´åŠ¨æ€CFGèŒƒå›´ï¼ˆè®ºæ–‡æåˆ°çš„wâˆˆ[5,15]ï¼‰
2. åœ¨experiments_controlä¸­æ·»åŠ æ–°æ§åˆ¶ä¿¡å·ç±»å‹
3. æ›¿æ¢deps/sentence-t5-largeå°è¯•ä¸åŒæ–‡æœ¬ç¼–ç å™¨
4. ä¿®æ”¹experiments_reconsä¸­çš„VAEç»“æ„ä»¥æ”¹å–„æ—¶é—´ä¸€è‡´æ€§ï¼ˆè®ºæ–‡å±€é™æ€§ï¼‰



å…³é”®è®¾è®¡å¯¹åº”è®ºæ–‡æ–¹æ³•ï¼š

- **æ½œåœ¨ç©ºé—´å‹ç¼©**ï¼š`experiments_recons/vae_humanml.ckpt`å®ç°VAEç¼–ç ï¼ˆå¯¹åº”è®ºæ–‡Sec3.2ï¼‰
- **kæ­¥è’¸é¦**ï¼š`experiments_t2m/motionlcm*`æ¨¡å‹é€šè¿‡EMAä¼˜åŒ–å®ç°åŠ é€Ÿï¼ˆå¯¹åº”åŠ¨æ€CFGæŠ€æœ¯[7](https://link.springer.com/chapter/10.1007/978-3-031-72640-8_22)[9](https://zhuanlan.zhihu.com/p/693836738)ï¼‰
- **ControlNetèåˆ**ï¼š`experiments_control/`ä¸‹æ¨¡å‹å®ç°å…³èŠ‚çº§æ§åˆ¶ï¼ˆåŒç©ºé—´ç›‘ç£æœºåˆ¶è§Sec3.3ï¼‰
- **å®æ—¶ç”Ÿæˆ**ï¼š`.ckpt`æ–‡ä»¶å‡ç»è¿‡å•æ­¥æ¨ç†ä¼˜åŒ–ï¼Œå®ç°30ms/åºåˆ—ï¼ˆè§è®ºæ–‡ç»“è®ºï¼‰[10](https://github.com/52CV/ECCV-2024-Papers)



# mld.py

## è§’åº¦1ï¼š

ä»¥ä¸‹æ˜¯`mld.py`çš„é€è¡Œç¿»è¯‘ä¸ç»“æ„åŒ–è§£æï¼Œç»“åˆMotionLCMè®ºæ–‡å†…å®¹è¿›è¡ŒåŠŸèƒ½æ ‡æ³¨ï¼š

---

### **æ–‡ä»¶æ¦‚è§ˆ**
```python
import time
import inspect
import logging
from typing import Optional

import tqdm
import numpy as np
from omegaconf import DictConfig

import torch
import torch.nn.functional as F
from diffusers.optimization import get_scheduler

# è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥
from mld.data.base import BaseDataModule
from mld.config import instantiate_from_config
from mld.utils.temos_utils import lengths_to_mask, remove_padding
from mld.utils.utils import count_parameters, get_guidance_scale_embedding, extract_into_tensor, control_loss_calculate
from mld.data.humanml.utils.plot_script import plot_3d_motion

from .base import BaseModel
```

---

### **æ ¸å¿ƒç±» `MLD` è§£æ**
#### **1. åˆå§‹åŒ– (`__init__`)**
```python
class MLD(BaseModel):
    def __init__(self, cfg: DictConfig, datamodule: BaseDataModule) -> None:
        super().__init__()
        self.cfg = cfg 
        self.nfeats = cfg.DATASET.NFEATS  # è¿åŠ¨ç‰¹å¾ç»´åº¦
        self.njoints = cfg.DATASET.NJOINTS  # å…³èŠ‚æ•°é‡
        self.latent_dim = cfg.model.latent_dim  # æ½œåœ¨ç©ºé—´ç»´åº¦
        self.guidance_scale = cfg.model.guidance_scale  # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç³»æ•°ï¼ˆCFGï¼‰
        self.datamodule = datamodule  # æ•°æ®æ¨¡å—

        # åŠ¨æ€CFGè®¾ç½®ï¼ˆè®ºæ–‡2.2èŠ‚ï¼‰
        if cfg.model.guidance_scale == 'dynamic':
            self.guidance_scale = s_cfg.cfg_step_map[s_cfg.num_inference_steps]

        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–ï¼ˆè®ºæ–‡3.1-3.3èŠ‚ï¼‰
        self.text_encoder = instantiate_from_config(cfg.model.text_encoder)  # æ–‡æœ¬ç¼–ç å™¨ï¼ˆT5/Sentence-T5ï¼‰
        self.vae = instantiate_from_config(cfg.model.motion_vae)  # è¿åŠ¨VAEï¼ˆæ½œåœ¨ç©ºé—´å‹ç¼©ï¼‰
        self.denoiser = instantiate_from_config(cfg.model.denoiser)  # å»å™ªç½‘ç»œï¼ˆU-Netï¼‰
        self.scheduler = instantiate_from_config(cfg.model.scheduler)  # æ‰©æ•£è°ƒåº¦å™¨ï¼ˆDDIM/LCMï¼‰

        # ControlNetç›¸å…³ï¼ˆè®ºæ–‡3.3èŠ‚ï¼‰
        self.is_controlnet = cfg.model.get('is_controlnet', False)
        if self.is_controlnet:
            self.controlnet = instantiate_from_config(c_cfg)  # æ§åˆ¶ç½‘ç»œ
            self.traj_encoder = instantiate_from_config(cfg.model.traj_encoder)  # è½¨è¿¹ç¼–ç å™¨
            self.vaeloss = cfg.model.get('vaeloss', False)  # åŒç©ºé—´ç›‘ç£æ ‡å¿—
            self.control_loss_func = cfg.model.get('control_loss_func', 'l2')  # æŸå¤±å‡½æ•°ç±»å‹
```

#### **2. å…³é”®æ–¹æ³•**
##### **(1) å‰å‘ä¼ æ’­ (`forward`)**
```python
def forward(self, batch: dict) -> tuple:
    # è¾“å…¥å¤„ç†
    texts = batch["text"]  # æ–‡æœ¬æ¡ä»¶
    feats_ref = batch.get("motion")  # å‚è€ƒè¿åŠ¨æ•°æ®
    hint = batch.get('hint')  # æ§åˆ¶ä¿¡å·ï¼ˆå¦‚åˆå§‹å…³èŠ‚è½¨è¿¹ï¼‰
	
    # æ–‡æœ¬ç¼–ç ä¸æ½œåœ¨ç©ºé—´å™ªå£°åˆå§‹åŒ–
    text_emb = self.text_encoder(texts)
    latents = torch.randn((len(lengths), *self.latent_dim), device=text_emb.device)
	
    # ControlNetæ¡ä»¶ç”Ÿæˆï¼ˆè®ºæ–‡3.3èŠ‚ï¼‰
    if self.is_controlnet:
        controlnet_cond = self.traj_encoder(hint_reshaped, hint_mask_reshaped)

    # æ‰©æ•£é€†è¿‡ç¨‹ï¼ˆå»å™ªï¼‰
    latents = self._diffusion_reverse(latents, text_emb, controlnet_cond=controlnet_cond)
    
    # VAEè§£ç å›è¿åŠ¨ç©ºé—´ 
    feats_rst = self.vae.decode(latents / self.vae_scale_factor, mask)
    joints = self.feats2joints(feats_rst)  # ç‰¹å¾è½¬å…³èŠ‚åæ ‡
```

##### **(2) æ‰©æ•£é€†è¿‡ç¨‹ (`_diffusion_reverse`)**
```python
def _diffusion_reverse(self, latents, text_emb, controlnet_cond=None):
    # è°ƒåº¦å™¨è®¾ç½®ï¼ˆè®ºæ–‡3.2èŠ‚ï¼‰
    self.scheduler.set_timesteps(self.cfg.model.scheduler.num_inference_steps)
    
    # åˆ†æ­¥å»å™ª
    for i, t in enumerate(timesteps):
        # ControlNetæ®‹å·®è®¡ç®—
        if self.is_controlnet:
            controlnet_residuals = self.controlnet(
                sample=latent_model_input,
                timestep=t,
                encoder_hidden_states=text_emb,
                controlnet_cond=controlnet_cond
            )
        
        # å»å™ªç½‘ç»œé¢„æµ‹
        model_output = self.denoiser(
            sample=latent_model_input,
            timestep=t,
            encoder_hidden_states=text_emb,
            controlnet_residuals=controlnet_residuals
        )
        
        # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆCFGï¼‰
        if self.do_classifier_free_guidance:
            model_output = model_output_uncond + self.guidance_scale * (model_output_text - model_output_uncond)
        
        # è°ƒåº¦å™¨æ›´æ–°æ½œåœ¨å˜é‡
        latents = self.scheduler.step(model_output, t, latents).prev_sample 
```

##### **(3) è®­ç»ƒè¿‡ç¨‹ (`train_diffusion_forward`)**
```python
def train_diffusion_forward(self, batch: dict) -> dict:
    # VAEç¼–ç è¿åŠ¨æ•°æ®åˆ°æ½œåœ¨ç©ºé—´
    z, _ = self.vae.encode(feats_ref, mask)
    
    # æ·»åŠ å™ªå£°ï¼ˆæ‰©æ•£è¿‡ç¨‹ï¼‰
    noisy_latents = self.scheduler.add_noise(z, noise, timesteps)
    
    # é¢„æµ‹å™ªå£°/æ½œåœ¨å˜é‡
    model_output = self.denoiser(noisy_latents, timesteps, text_emb)
    
    # æŸå¤±è®¡ç®—ï¼ˆè®ºæ–‡3.2èŠ‚ï¼‰
    if self.is_controlnet and self.vaeloss:
        # åŒç©ºé—´ç›‘ç£æŸå¤±ï¼ˆæ½œåœ¨ç©ºé—´+è¿åŠ¨ç©ºé—´ï¼‰
        cond_loss = control_loss_calculate(joints_rst, hint, hint_mask)
        rot_loss = F.mse_loss(feats_rst, feats_ref)
        loss_dict = {'diff_loss': diff_loss, 'cond_loss': cond_loss, 'rot_loss': rot_loss}
```

---

### **å…³é”®æµç¨‹æ€»ç»“**
1. **æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆæµç¨‹**  
   ```mermaid
   graph LR
   A[æ–‡æœ¬è¾“å…¥] --> B(T5æ–‡æœ¬ç¼–ç )
   B --> C[æ½œåœ¨ç©ºé—´å™ªå£°åˆå§‹åŒ–]
   C --> D{æ˜¯å¦ControlNet?}
   D -->|æ˜¯| E[è½¨è¿¹ç¼–ç å™¨ç”Ÿæˆæ§åˆ¶ä¿¡å·]
   D -->|å¦| F[ç›´æ¥å»å™ª]
   E --> F
   F --> G[æ‰©æ•£é€†è¿‡ç¨‹å»å™ª]
   G --> H[VAEè§£ç ä¸ºè¿åŠ¨]
   H --> I[å…³èŠ‚åæ ‡è¾“å‡º]
   ```

2. **è®­ç»ƒæµç¨‹**  
   ```mermaid
   graph TB
   A[è¿åŠ¨æ•°æ®] --> B[VAEç¼–ç åˆ°æ½œåœ¨ç©ºé—´]
   B --> C[æ·»åŠ å™ªå£°]
   C --> D[å»å™ªç½‘ç»œé¢„æµ‹]
   D --> E{æ˜¯å¦ControlNet?}
   E -->|æ˜¯| F[è®¡ç®—åŒç©ºé—´æŸå¤±]
   E -->|å¦| G[ä»…æ‰©æ•£æŸå¤±]
   F --> H[æ€»æŸå¤±åå‘ä¼ æ’­]
   G --> H
   ```

---

### **ä¸è®ºæ–‡çš„å¯¹åº”å…³ç³»**
| ä»£ç æ¨¡å—             | è®ºæ–‡å¯¹åº”ç« èŠ‚ | æ ¸å¿ƒåˆ›æ–°ç‚¹                    |
| -------------------- | ------------ | ----------------------------- |
| `self.controlnet`    | 3.3èŠ‚        | æ½œåœ¨ç©ºé—´ControlNetæ¶æ„        |
| `_diffusion_reverse` | 3.2èŠ‚        | kæ­¥ä¸€è‡´æ€§è’¸é¦ï¼ˆå•æ­¥æ¨ç†åŠ é€Ÿï¼‰ |
| `vaeloss`            | 3.3èŠ‚        | åŒç©ºé—´ç›‘ç£æœºåˆ¶                |
| `dynamic guidance`   | 2.2èŠ‚        | åŠ¨æ€CFGè®­ç»ƒç­–ç•¥               |

---

### **éœ€è¦è¡¥å……çš„ä¿¡æ¯**
å¦‚æœéœ€è¦è¿›ä¸€æ­¥åˆ†æä»¥ä¸‹å†…å®¹ï¼Œè¯·æä¾›ç›¸å…³æ–‡ä»¶ï¼š
1. **VAEç»“æ„**ï¼š`configs/model/motion_vae.yaml`  
2. **ControlNeté…ç½®**ï¼š`configs/model/denoiser_controlnet.yaml`  
3. **è®­ç»ƒè„šæœ¬**ï¼š`train.py`ï¼ˆäº†è§£å®Œæ•´è®­ç»ƒæµç¨‹ï¼‰



---------------

## è§’åº¦2ï¼š

### MLD.py é€è¡Œè§£æ (æ ¸å¿ƒéƒ¨åˆ†)

```python
# æ ¸å¿ƒæ¨¡å—å®šä¹‰
class MLD(BaseModel):
    def __init__(self, cfg: DictConfig, datamodule: BaseDataModule) -> None:
        # åˆå§‹åŒ–å…³é”®ç»„ä»¶
        self.text_encoder = instantiate_from_config(cfg.model.text_encoder)  # æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚T5-Largeï¼‰
        self.vae = instantiate_from_config(cfg.model.motion_vae)             # è¿åŠ¨VAEï¼ˆæ½œåœ¨ç©ºé—´å‹ç¼©ï¼‰
        self.denoiser = instantiate_from_config(cfg.model.denoiser)          # å»å™ªç½‘ç»œï¼ˆUNetæ¶æ„ï¼‰
        
        # ControlNetç›¸å…³ç»„ä»¶
        if self.is_controlnet:
            self.controlnet = instantiate_from_config(c_cfg)                # æ§åˆ¶ç½‘ç»œï¼ˆå…³èŠ‚è½¨è¿¹å»ºæ¨¡ï¼‰
            self.traj_encoder = instantiate_from_config(cfg.model.traj_encoder) # è½¨è¿¹ç¼–ç å™¨
            
        # æ‰©æ•£è°ƒåº¦å™¨ï¼ˆLCMä¼˜åŒ–ç‰ˆï¼‰
        self.scheduler = instantiate_from_config(cfg.model.scheduler)        # ä¸€è‡´æ€§è’¸é¦è°ƒåº¦å™¨
```

### æ ¸å¿ƒæ–¹æ³•æ€»ç»“
```markdown
1. æ½œåœ¨ç©ºé—´æ‰©æ•£æµç¨‹ï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ æ–‡æœ¬ç¼–ç        â”‚ â–¶ â”‚ å™ªå£°ä¼˜åŒ–       â”‚ â–¶ â”‚ æ‰©æ•£åå‘è¿‡ç¨‹    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   (T5/Large)          (Latent Optimizer)   (Consistency Scheduler)

2. æ§åˆ¶ä¿¡å·èåˆï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ è½¨è¿¹ç¼–ç        â”‚ â–¶ â”‚ ControlNet     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   (SMPLå…³èŠ‚æ•°æ®)     (æ®‹å·®è¿æ¥è‡³UNet)
```

### æ—¶åºå›¾ (Typoraå¯ç”¨)
```mermaid
sequenceDiagram
    participant Text as è¾“å…¥æ–‡æœ¬
    participant Hint as æ§åˆ¶ä¿¡å·(hint)
    participant VAE as è¿åŠ¨VAE
    participant LCM as æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹
    participant CN as ControlNet
    
    Text->>+VAE: ç”Ÿæˆåˆå§‹æ½œåœ¨å‘é‡z0
    Hint->>+CN: è½¨è¿¹ç¼–ç 
    loop æ‰©æ•£æ­¥éª¤
        LCM->>LCM: å•æ­¥å»å™ª (å…¬å¼1)
        CN-->>LCM: æ³¨å…¥æ®‹å·®æ§åˆ¶ä¿¡å·
    end
    VAE->>-VAE: è§£ç æ½œåœ¨å‘é‡
    Note right of VAE: è¾“å‡ºå…³èŠ‚è¿åŠ¨åºåˆ—
```

### å…³é”®ä»£ç æ®µè§£è¯»
```python
def _diffusion_reverse(self, latents, text_emb, controlnet_cond):
    # LCMæ ¸å¿ƒæ‰©æ•£è¿‡ç¨‹
    for t in timesteps:
        # æ³¨å…¥ControlNetæ®‹å·®
        controlnet_residuals = self.controlnet(...) 
        # ä¸€è‡´æ€§æ¨¡å‹å•æ­¥é¢„æµ‹
        model_output = self.denoiser(latent_model_input, t, ...)
        # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
        model_output = guidance_scale * (text_cond - uncond) + uncond
        # æ›´æ–°æ½œåœ¨å‘é‡
        latents = scheduler.step(model_output, t, latents)
    return latents
```

### éœ€è¦è¡¥å……çš„ä¿¡æ¯
```markdown
â— éœ€è¦ä»¥ä¸‹æ–‡ä»¶çš„å…·ä½“å®ç°æ‰èƒ½ç»˜åˆ¶å®Œæ•´æ¶æ„å›¾ï¼š
1. `configs/` ä¸­çš„è°ƒåº¦å™¨å‚æ•°é…ç½®ï¼ˆç‰¹åˆ«æ˜¯LCMçš„stepæ˜ å°„ï¼‰
2. `deps/smpl_models` çš„å…³èŠ‚å®šä¹‰æ–‡ä»¶
3. `controlnet.py` çš„æ®‹å·®è¿æ¥å®ç°ç»†èŠ‚

å»ºè®®ç ”ç©¶æ­¥éª¤ï¼š
1. ä¿®æ”¹ configs_v1 è°ƒæ•´åŠ¨æ€CFGèŒƒå›´ï¼ˆw_min=5, w_max=15ï¼‰
2. åœ¨ experiments_control/spatial æ·»åŠ æ–°çš„æ§åˆ¶ä¿¡å·ç±»å‹
3. æ›¿æ¢ deps/sentence-t5-large æµ‹è¯•ä¸åŒæ–‡æœ¬ç¼–ç å™¨æ•ˆæœ
```

> å®Œæ•´ä»£ç è§£ææ–‡æ¡£å·²ç”Ÿæˆï¼ŒåŒ…å«è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šä¸è®ºæ–‡æ–¹æ³•å¯¹åº”å…³ç³»ã€‚å»ºè®®ç»“åˆ`configs_v1/motionlcm.yaml`åˆ†æåŠ¨æ€è®­ç»ƒç­–ç•¥çš„å…·ä½“å‚æ•°è®¾ç½®ã€‚



# åœºæ™¯äº’åŠ¨ controlNet

æƒ³è¦å’Œåœºæ™¯äº¤äº’ç»“åˆï¼Œæ˜¯ä¸æ˜¯å¯¹controlNetåšæ–‡ç« å°±è¡Œï¼Ÿ

## å»ºè®®1



æ˜¯çš„ï¼Œ**é€šè¿‡æ”¹é€ ControlNetå®ç°ä¸åœºæ™¯äº¤äº’**æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”æ¨¡å—åŒ–çš„æ–¹æ¡ˆã€‚ä»¥ä¸‹æ˜¯å…·ä½“ç­–ç•¥å’Œå®ç°å»ºè®®ï¼Œç»“åˆMotionLCMçš„æ¶æ„ç‰¹ç‚¹ï¼š

---

### **1. ControlNetçš„æ”¹é€ æ–¹å‘**
#### **(1) è¾“å…¥æ¡ä»¶æ‰©å±•**
| åŸè¾“å…¥                | æ–°å¢åœºæ™¯äº¤äº’è¾“å…¥       | å®ç°æ–¹å¼                                                     |
| --------------------- | ---------------------- | ------------------------------------------------------------ |
| åˆå§‹å…³èŠ‚è½¨è¿¹ (`hint`) | åœºæ™¯éšœç¢ç‰©çƒ­å›¾         | ç”¨Open3D/Blenderç”Ÿæˆåœºæ™¯æ·±åº¦å›¾ï¼Œè½¬ä¸ºè·ç¦»åœºçƒ­å›¾ä½œä¸ºControlNetçš„é¢å¤–æ¡ä»¶è¾“å…¥ |
| -                     | äº¤äº’ç›®æ ‡ä½ç½®ï¼ˆå¦‚æ¤…å­ï¼‰ | åœ¨`batch`ä¸­æ·»åŠ ç›®æ ‡åæ ‡ï¼Œé€šè¿‡`traj_encoder`ç¼–ç ä¸ºå‘é‡        |
| -                     | ç‰©ç†çº¦æŸï¼ˆå¦‚æ‘©æ“¦åŠ›ï¼‰   | ç”¨PyBulletæ¨¡æ‹Ÿç‰©ç†å‚æ•°ï¼Œç¼–ç ä¸ºä½ç»´å‘é‡æ‹¼æ¥è‡³ControlNetçš„æ¡ä»¶è¾“å…¥ |

#### **(2) ç½‘ç»œç»“æ„ä¿®æ”¹**
```python
# ä¿®æ”¹ControlNetçš„forwardæ–¹æ³•ï¼ˆéœ€åŒæ­¥æ”¹configsï¼‰
class SceneAwareControlNet(nn.Module):
    def forward(self, x, t, text_emb, control_cond, scene_heatmap=None, physics_params=None):
        # åŸControlNetåˆ†æ”¯
        traj_cond = self.traj_proj(control_cond)  
        
        # æ–°å¢åœºæ™¯åˆ†æ”¯
        if scene_heatmap is not None:
            scene_feat = self.scene_cnn(scene_heatmap)  # ç”¨3D CNNå¤„ç†çƒ­å›¾
            traj_cond += self.scene_proj(scene_feat)
            
        # ç‰©ç†çº¦æŸåˆ†æ”¯
        if physics_params is not None:
            physics_emb = self.physics_mlp(physics_params)
            traj_cond += physics_emb
            
        return super().forward(x, t, text_emb, traj_cond)
```

---

### **2. æ•°æ®æµæ”¹é€ **
#### **(1) è®­ç»ƒé˜¶æ®µ**
```mermaid
graph TB
A[åœºæ™¯æ•°æ®] --> B[ç”Ÿæˆçƒ­å›¾/ç‰©ç†å‚æ•°]
B --> C{DataLoader}
C --> D[ä¸è¿åŠ¨æ•°æ®batchæ‹¼æ¥]
D --> E[ControlNetå¤šæ¡ä»¶è¾“å…¥]
E --> F[åŒç©ºé—´ç›‘ç£æŸå¤±]
```
- **å…³é”®ç‚¹**ï¼šåœ¨`mld/data/humanml/dataset.py`ä¸­æ‰©å±•`__getitem__`æ–¹æ³•ï¼ŒåŠ è½½åœºæ™¯æ•°æ®ã€‚

#### **(2) æ¨ç†é˜¶æ®µ**
```python
def generate_with_scene(text, scene_heatmap):
    # åŸæµç¨‹
    text_emb = text_encoder(text)  
    latents = torch.randn(...)
    
    # æ–°å¢åœºæ™¯æ¡ä»¶
    control_cond = traj_encoder(scene_heatmap)  # çƒ­å›¾ç›´æ¥ä½œä¸ºhint
    
    # ç”Ÿæˆ
    motion = model(latents, text_emb, control_cond)
    return motion
```

---

### **3. ä¸MotionLCMçš„ååŒä¼˜åŒ–**
#### **(1) åŒç©ºé—´ç›‘ç£å¢å¼º**
åœ¨`train_diffusion_forward`ä¸­å¢åŠ åœºæ™¯ç›¸å…³æŸå¤±ï¼š
```python
# åœ¨loss_dictä¸­æ–°å¢
if scene_heatmap is not None:
    # æ¥è§¦ç‚¹ä¸€è‡´æ€§æŸå¤±ï¼ˆéœ€é¢„å®šä¹‰åœºæ™¯æ¥è§¦åŒºåŸŸï¼‰
    contact_loss = ((joints_rst[:, -1] - scene_contact) ** 2).mean()  # æœ«å¸§å…³èŠ‚éœ€æ¥è§¦ç›®æ ‡
    loss_dict['contact_loss'] = contact_loss
```

#### **(2) åŠ¨æ€CFGè°ƒæ•´**
æ ¹æ®åœºæ™¯å¤æ‚åº¦åŠ¨æ€è°ƒæ•´`guidance_scale`ï¼š
```python
guidance_scale = base_scale * (1 + scene_complexity)  # scene_complexityâˆˆ[0,1]
```

---

### **4. éœ€è¦è¡¥å……çš„ä»£ç /é…ç½®**
1. **ControlNeté…ç½®æ–‡ä»¶**  
   éœ€ä¿®æ”¹`configs/model/denoiser_controlnet.yaml`ï¼Œæ·»åŠ ï¼š
   ```yaml
   scene_cnn:
     channels: [64, 128, 256]  # çƒ­å›¾å¤„ç†CNN
   physics_mlp:
     hidden_dims: [32, 64]     # ç‰©ç†å‚æ•°ç¼–ç å™¨
   ```

2. **åœºæ™¯æ•°æ®å¤„ç†è„šæœ¬**  
   éœ€è¦æä¾›ç”Ÿæˆä»¥ä¸‹æ•°æ®çš„ä»£ç ï¼š
   - åœºæ™¯çƒ­å›¾ï¼ˆ`scripts/generate_heatmap.py`ï¼‰
   - ç‰©ç†å‚æ•°æ¨¡æ‹Ÿï¼ˆ`scripts/physics_sim.py`ï¼‰

---

### **5. æ•ˆæœéªŒè¯æŒ‡æ ‡**
| æŒ‡æ ‡       | è®¡ç®—æ–¹æ³•                         | ç›®æ ‡                         |
| ---------- | -------------------------------- | ---------------------------- |
| åœºæ™¯ç©¿é€ç‡ | å…³èŠ‚ä¸éšœç¢ç‰©çš„ä½“ç§¯äº¤é›†å æ¯”       | <5%                          |
| äº¤äº’æˆåŠŸç‡ | æœ«ç«¯æ‰§è¡Œå™¨åˆ°è¾¾ç›®æ ‡åŠå¾„å†…çš„å¸§å æ¯” | >90% (å¦‚æ‰‹éƒ¨ä¸æ¤…å­è·ç¦»<0.1m) |
| ç‰©ç†åˆç†æ€§ | PyBulletæ¨¡æ‹Ÿçš„èƒ½é‡æ¶ˆè€—æ–¹å·®       | æ¯”åŸºçº¿ä½20%                  |

---

### **æ€»ç»“å»ºè®®**
- **ä¼˜å…ˆæ”¹é€ ControlNet**ï¼šå› å…¶ç‹¬ç«‹äºä¸»å¹²æ‰©æ•£æ¨¡å‹ï¼Œä¿®æ”¹é£é™©ä½ä¸”æ•ˆæœå¯éªŒè¯ã€‚
- **åˆ†é˜¶æ®µå®ç°**ï¼š  
  1. å…ˆå¢åŠ é™æ€çƒ­å›¾è¾“å…¥  
  2. å†å¼•å…¥åŠ¨æ€ç‰©ç†çº¦æŸ  
  3. æœ€åä¼˜åŒ–åŠ¨æ€CFGç­–ç•¥
- **åˆ©ç”¨ç°æœ‰å·¥å…·é“¾**ï¼š  
  - çƒ­å›¾ç”Ÿæˆï¼š`Open3D.screenshot_depth()`  
  - ç‰©ç†æ¨¡æ‹Ÿï¼š`PyBullet.stepSimulation()`  
  - å¯è§†åŒ–éªŒè¯ï¼š`plot_3d_motion`ï¼ˆå·²é›†æˆåœ¨ä»£ç ä¸­ï¼‰



--------------

## å»ºè®®2

deepseek R1:

è¦æ•´åˆåœºæ™¯äº¤äº’åˆ°MotionLCMæ¡†æ¶ä¸­ï¼Œ**å¯¹ControlNetè¿›è¡Œæ‰©å±•ç¡®å®æ˜¯ä¸»è¦çªç ´å£**ï¼Œä½†éœ€è¦ç»“åˆå…¶ä»–æ¨¡å—çš„ååŒè°ƒæ•´ã€‚ä»¥ä¸‹æ˜¯æŠ€æœ¯å®ç°è·¯å¾„åˆ†æï¼š

---

### 1. ControlNetçš„æ ¸å¿ƒæ‰©å±•æ–¹å‘
```python
# åŸå§‹ControlNetè¾“å…¥ç»“æ„
class ControlNet(nn.Module):
    def forward(sample, timestep, controlnet_cond):
        # controlnet_cond: è½¨è¿¹æç¤ºä¿¡å·
        return residuals
```

**åœºæ™¯äº¤äº’æ”¹é€ ç­–ç•¥ï¼š**
1. **å¤šæ¨¡æ€æ§åˆ¶ä¿¡å·èåˆ**
```python
# ä¿®æ”¹åçš„è¾“å…¥ç»“æ„
controlnet_cond = {
    'trajectory': trajectory_emb,  # åŸæœ‰è½¨è¿¹
    'scene': scene_emb             # æ–°å¢åœºæ™¯ç¼–ç 
}
```

2. **åœºæ™¯ç¼–ç å™¨è®¾è®¡**
```python
class SceneEncoder(nn.Module):
    def __init__(self):
        self.obj_encoder = PointNet++()  # ç‚¹äº‘ç¼–ç ç‰©ä½“
        self.room_encoder = ResNet3D()   # 3Dåœºæ™¯ç¼–ç 
    
    def forward(scene_data):
        return fused_emb
```

3. **ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶**
```python
# åœ¨ControlNetæ®‹å·®å—ä¸­åŠ å…¥
class SceneAwareBlock(nn.Module):
    def forward(x, scene_emb):
        attn = CrossAttention(x, scene_emb)  # è·¨æ¨¡æ€æ³¨æ„åŠ›
        return x + attn
```

---

### 2. éœ€è¦åŒæ­¥è°ƒæ•´çš„å…³é”®æ¨¡å—

| æ¨¡å—         | æ”¹é€ ç‚¹                    | åœºæ™¯äº¤äº’å…³è”æ€§    |
| ------------ | ------------------------- | ----------------- |
| **æ•°æ®ç®¡é“** | æ·»åŠ åœºæ™¯ç‚¹äº‘/è¯­ä¹‰åœ°å›¾åŠ è½½ | æä¾›åœºæ™¯æ•°æ®æ¥æº  |
| **VAE**      | æ½œåœ¨ç©ºé—´å¢åŠ åœºæ™¯æ¡ä»¶åˆ†æ”¯  | é˜²æ­¢è¿åŠ¨-åœºæ™¯è§£è€¦ |
| **æŸå¤±å‡½æ•°** | æ–°å¢ç¢°æ’æŸå¤±é¡¹            | ç‰©ç†åˆç†æ€§çº¦æŸ    |

---

### 3. å®ç°è·¯çº¿å›¾ï¼ˆåˆ†é˜¶æ®µï¼‰

**é˜¶æ®µä¸€ï¼šåŸºç¡€åœºæ™¯æ§åˆ¶**
1. åœ¨`configs/model/controlnet.yaml`ä¸­æ·»åŠ ï¼š
```yaml
scene_encoder:
  target: model.components.SceneEncoder
  params:
    obj_dim: 256
    room_dim: 512
```

2. ä¿®æ”¹æ•°æ®åŠ è½½å™¨ï¼š
```python
# åœ¨BaseDataModuleä¸­æ–°å¢
def scene_loader(batch):
    return load_pointcloud(batch['scene_id'])
```

**é˜¶æ®µäºŒï¼šç‰©ç†çº¦æŸå¢å¼º**
```python
# åœ¨lossè®¡ç®—ä¸­æ·»åŠ ç¢°æ’æ£€æµ‹
def collision_loss(joints, scene_octree):
    distances = octree_query(joints, scene_octree)
    return torch.exp(-distances/0.1).mean()
```

**é˜¶æ®µä¸‰ï¼šäº¤äº’åŠ¨ä½œç”Ÿæˆ**
```python
# äº¤äº’è¯­ä¹‰æ§åˆ¶ï¼ˆç¤ºä¾‹ï¼šæ¡æ¯åŠ¨ä½œï¼‰
if "hold_cup" in text_prompt:
    controlnet_cond['contact'] = generate_grasp_heatmap(
        cup_position, hand_kinematics
    )
```

---

### 4. æ½œåœ¨æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

| æŒ‘æˆ˜           | è§£å†³æ–¹æ¡ˆ                           | ç›¸å…³è®ºæ–‡å‚è€ƒ             |
| -------------- | ---------------------------------- | ------------------------ |
| å¤šæ¨¡æ€ä¿¡å·å†²çª | ä½¿ç”¨Modulation Networkåè°ƒæ§åˆ¶ä¿¡å· | ã€ŠComposable Diffusionã€‹ |
| å®æ—¶æ€§è¦æ±‚     | é‡‡ç”¨NeRFåœºæ™¯å‹ç¼©ç¼–ç                | ã€ŠInstant-NGPã€‹          |
| æ•°æ®ç¨€ç¼º       | ä½¿ç”¨SMPL-X+åœºæ™¯åˆæˆæ•°æ®            | ã€ŠPROX Datasetã€‹         |

---

### ç»“è®º
**ä»…æ”¹é€ ControlNetå¯è¾¾åˆ°åŸºç¡€åœºæ™¯é€‚åº”**ï¼ˆå¦‚é¿éšœï¼‰ï¼Œä½†è‹¥è¦å®ç°æ·±åº¦äº¤äº’ï¼ˆå¦‚æ¨é—¨ã€åæ¤…ï¼‰ï¼Œéœ€è¦ï¼š
1. åœ¨VAEä¸­å¼•å…¥åœºæ™¯æ¡ä»¶åŒ–ç”Ÿæˆ
2. æ·»åŠ ç‰©ç†å¼•æ“çº¦æŸå±‚ï¼ˆå¦‚PyBulletæ¢¯åº¦å›ä¼ ï¼‰
3. è®¾è®¡äº¤äº’è¯­ä¹‰çš„æç¤ºè¯åµŒå…¥æœºåˆ¶

å»ºè®®ä»æ‰©å±•ControlNetçš„åœºæ™¯ç¼–ç èƒ½åŠ›å…¥æ‰‹ï¼Œé€æ­¥éªŒè¯æ•ˆæœåå†è¿›è¡Œå…¨æµç¨‹ä¼˜åŒ–ã€‚MotionLCMçš„LCMè°ƒåº¦æœºåˆ¶èƒ½æœ‰æ•ˆåŠ é€Ÿå¤šæ¡ä»¶ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿™å¯¹å®æ—¶åœºæ™¯äº¤äº’è‡³å…³é‡è¦ã€‚



## **æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹æˆ–ä»…è®­ç»ƒControlNet** å»ºè®®1

åœ¨ç»“åˆåœºæ™¯äº¤äº’çš„ä»»åŠ¡ä¸­ï¼Œ**æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹æˆ–ä»…è®­ç»ƒControlNet**ï¼Œå–å†³äºå…·ä½“éœ€æ±‚å’Œç°æœ‰æ¨¡å‹çš„é€‚é…æ€§ã€‚ä»¥ä¸‹æ˜¯åˆ†æƒ…å†µè®¨è®ºå’Œå»ºè®®ï¼š

---

### **1. ä¸è®­ç»ƒä»»ä½•éƒ¨åˆ†ï¼ˆç›´æ¥æ¨ç†ï¼‰**
#### **é€‚ç”¨æ¡ä»¶**
- åœºæ™¯äº¤äº’éœ€æ±‚ç®€å•ï¼ˆå¦‚ä»…éœ€é¿éšœï¼‰
- MotionLCMé¢„è®­ç»ƒæ¨¡å‹å·²åŒ…å«ç±»ä¼¼åœºæ™¯æ•°æ®ï¼ˆå¦‚HumanMLæ•°æ®é›†ä¸­çš„ç¯å¢ƒäº¤äº’æ ·æœ¬ï¼‰
- ControlNetçš„æ¡ä»¶è¾“å…¥ï¼ˆå¦‚çƒ­å›¾ï¼‰ä¸è®­ç»ƒæ—¶æ ¼å¼å®Œå…¨ä¸€è‡´

#### **æ“ä½œæ–¹æ³•**
```python
# ç›´æ¥åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œä»…åœ¨å‰å‘ä¼ æ’­æ—¶æ³¨å…¥åœºæ™¯ä¿¡æ¯
model = MLD.load_from_checkpoint("motionlcm_humanml_v1.ckpt")
motion = model.generate(text="åä¸‹", hint=scene_heatmap)  # å°†åœºæ™¯çƒ­å›¾ä½œä¸ºhintè¾“å…¥
```
#### **ä¼˜ç¼ºç‚¹**
| ä¼˜ç‚¹       | ç¼ºç‚¹                               |
| ---------- | ---------------------------------- |
| é›¶è®­ç»ƒæˆæœ¬ | äº¤äº’æ•ˆæœå—é™ï¼ˆæ— æ³•å¤„ç†å¤æ‚åœºæ™¯ï¼‰   |
| å³æ—¶å¯ç”¨   | è‹¥åœºæ™¯è¾“å…¥æ ¼å¼ä¸åŒ¹é…ä¼šå¯¼è‡´ç”Ÿæˆå¼‚å¸¸ |

---

### **2. ä»…è®­ç»ƒControlNetéƒ¨åˆ†**
#### **é€‚ç”¨æ¡ä»¶**
- éœ€è¦**æ–°å¢åœºæ™¯æ¡ä»¶è¾“å…¥**ï¼ˆå¦‚çƒ­å›¾ã€ç‰©ç†å‚æ•°ï¼‰
- å¸Œæœ›ä¿æŒåŸå§‹æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆèƒ½åŠ›ä¸å˜
- æœ‰å°‘é‡åœºæ™¯äº¤äº’æ ‡æ³¨æ•°æ®ï¼ˆâ‰¥100ç»„è¿åŠ¨-åœºæ™¯é…å¯¹æ ·æœ¬ï¼‰

#### **æ“ä½œæ–¹æ³•**
1. **å†»ç»“ä¸»å¹²ç½‘ç»œ**ï¼š
   ```python
   # åœ¨è®­ç»ƒè„šæœ¬ä¸­è®¾ç½®
   for param in model.text_encoder.parameters():
       param.requires_grad = False
   for param in model.vae.parameters():
       param.requires_grad = False
   for param in model.denoiser.parameters():
       param.requires_grad = False
   ```
2. **æ‰©å±•ControlNetè¾“å…¥å±‚**ï¼š
   ```python
   # ä¿®æ”¹ControlNetçš„__init__ (éœ€åŒæ­¥æ”¹configs)
   self.scene_proj = nn.Linear(256, self.latent_dim)  # çƒ­å›¾ç‰¹å¾æŠ•å½±å±‚
   ```

3. **è®­ç»ƒè„šæœ¬è°ƒæ•´**ï¼š
   ```python
   # åœ¨train_diffusion_forwardä¸­è®¡ç®—åœºæ™¯ç›¸å…³æŸå¤±
   if scene_heatmap is not None:
       scene_feat = scene_cnn(scene_heatmap)
       control_cond += self.scene_proj(scene_feat)
       contact_loss = F.mse_loss(joints_rst[:, -1], target_position)
       loss_dict['contact_loss'] = contact_loss * 0.1  # åŠ æƒç³»æ•°
   ```

#### **ä¼˜ç¼ºç‚¹**
| ä¼˜ç‚¹                           | ç¼ºç‚¹                                       |
| ------------------------------ | ------------------------------------------ |
| è®­ç»ƒæˆæœ¬ä½ï¼ˆä»…éœ€å¾®è°ƒéƒ¨åˆ†å‚æ•°ï¼‰ | éœ€ä¿è¯æ–°å¢è¾“å…¥ä¸åŸæ¡ä»¶è¾“å…¥çš„ç»´åº¦å…¼å®¹æ€§     |
| ä¿ç•™åŸæœ‰ç”Ÿæˆèƒ½åŠ›               | è‹¥åœºæ™¯äº¤äº’å¤æ‚ï¼ˆå¦‚åŠ¨åŠ›å­¦çº¦æŸï¼‰ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ |

---

### **3. è”åˆè®­ç»ƒå…¨éƒ¨æ¨¡å‹**
#### **é€‚ç”¨æ¡ä»¶**
- åœºæ™¯äº¤äº’éœ€æ±‚å¤æ‚ï¼ˆå¦‚éœ€è¦åŠ¨åŠ›å­¦ç‰©ç†åˆç†æ€§ï¼‰
- æœ‰å¤§é‡åœºæ™¯æ ‡æ³¨æ•°æ®ï¼ˆâ‰¥1Kç»„è¿åŠ¨-åœºæ™¯é…å¯¹æ ·æœ¬ï¼‰
- éœ€è¦æ¨¡å‹ä»åº•å±‚ç†è§£åœºæ™¯è¯­ä¹‰

#### **æ“ä½œæ–¹æ³•**
1. **è§£å†»æ‰€æœ‰å‚æ•°**ï¼š
   ```python
   model.train()  # å…¨éƒ¨æ¨¡å—è¿›å…¥è®­ç»ƒæ¨¡å¼
   ```

2. **ä¿®æ”¹æ•°æ®åŠ è½½**ï¼š
   ```python
   # åœ¨dataset.pyä¸­è¿”å›åœºæ™¯æ•°æ®
   def __getitem__(self, idx):
       return {
           "text": texts[idx],
           "motion": motions[idx],
           "scene_heatmap": heatmaps[idx],  # æ–°å¢
           "physics": physics_params[idx]   # æ–°å¢
       }
   ```

3. **è°ƒæ•´æŸå¤±å‡½æ•°**ï¼š
   ```python
   # åœ¨BaseModelä¸­æ‰©å±•æŸå¤±è®¡ç®—
   def training_step(self, batch, batch_idx):
       loss_dict = self.train_diffusion_forward(batch)
       if 'scene_heatmap' in batch:
           loss_dict['scene_loss'] = self.scene_aware_loss(batch)
       return sum(loss_dict.values())
   ```

#### **ä¼˜ç¼ºç‚¹**
| ä¼˜ç‚¹               | ç¼ºç‚¹                                   |
| ------------------ | -------------------------------------- |
| å¯å¤„ç†å¤æ‚äº¤äº’     | è®­ç»ƒæˆæœ¬é«˜ï¼ˆéœ€å®Œæ•´æ‰©æ•£æ¨¡å‹è®­ç»ƒèµ„æºï¼‰   |
| ç«¯åˆ°ç«¯ä¼˜åŒ–åœºæ™¯ç†è§£ | éœ€è°¨æ…è®¾è®¡æŸå¤±å‡½æ•°é¿å…ç ´ååŸæœ‰ç”Ÿæˆèƒ½åŠ› |

---

### **å†³ç­–æµç¨‹å›¾**
```mermaid
graph TD
A[éœ€æ±‚åˆ†æ] --> B{æ˜¯å¦éœ€è¦æ–°å¢è¾“å…¥ç±»å‹?}
B -->|å¦| C[ç›´æ¥æ¨ç†]
B -->|æ˜¯| D{æ•°æ®é‡æ˜¯å¦å……è¶³?}
D -->|å°‘é‡æ•°æ®| E[ä»…è®­ç»ƒControlNet]
D -->|å¤§é‡æ•°æ®| F[è”åˆè®­ç»ƒ]
C --> G[éªŒè¯ç”Ÿæˆè´¨é‡]
E --> G
F --> G
G --> H{æ•ˆæœæ˜¯å¦è¾¾æ ‡?}
H -->|å¦| I[å¢åŠ æ•°æ®/è°ƒæ•´æ¶æ„]
H -->|æ˜¯| J[éƒ¨ç½²åº”ç”¨]
```

---

### **å…·ä½“å»ºè®®**
1. **ä¼˜å…ˆå°è¯•æ–¹æ¡ˆ2ï¼ˆä»…è®­ControlNetï¼‰**  
   - ä¿®æ”¹`configs/model/denoiser_controlnet.yaml`ï¼Œæ·»åŠ åœºæ™¯å¤„ç†å±‚
   - ç¤ºä¾‹è®­ç»ƒå‘½ä»¤ï¼š
     ```bash
     python train.py --stage controlnet --batch_size 32 --lr 1e-4
     ```

2. **å…³é”®éªŒè¯æŒ‡æ ‡**  
   - **è¿åŠ¨è´¨é‡**ï¼šè®¡ç®—ç”Ÿæˆè¿åŠ¨çš„FIDï¼ˆä¸æµ‹è¯•é›†å¯¹æ¯”ï¼‰
   - **åœºæ™¯é€‚é…æ€§**ï¼šæ‰‹å·¥æ£€æŸ¥10%æ ·æœ¬çš„é¿éšœ/äº¤äº’åˆç†æ€§
   - **è®­ç»ƒç¨³å®šæ€§**ï¼šç›‘æ§`contact_loss`çš„æ”¶æ•›æƒ…å†µ

3. **å¿…è¦ä»£ç ä¿®æ”¹å¤„**  
   - `mld/data/humanml/dataset.py`ï¼šæ–°å¢åœºæ™¯æ•°æ®åŠ è½½
   - `mld/models/controlnet.py`ï¼šæ‰©å±•æ¡ä»¶å¤„ç†åˆ†æ”¯
   - `configs/train.yaml`ï¼šè°ƒæ•´ä¼˜åŒ–å™¨å‚æ•°ï¼ˆä»…ä¼˜åŒ–ControlNetæ—¶éœ€å‡å°lrï¼‰

---

### **è‹¥é€‰æ‹©ä¸è®­ç»ƒ**
éœ€ç¡®ä¿ï¼š
1. åœºæ™¯è¾“å…¥èƒ½é€šè¿‡ç°æœ‰`hint`æœºåˆ¶æ³¨å…¥ï¼ˆå¦‚å°†çƒ­å›¾ç¼©æ”¾åˆ°ä¸å…³èŠ‚è½¨è¿¹ç›¸åŒç»´åº¦ï¼‰
2. åœ¨æ¨ç†æ—¶æ·»åŠ åå¤„ç†ï¼š
   ```python
   def filter_penetration(motion, scene):
       # ç”¨ç®€å•è§„åˆ™ä¿®æ­£ç©¿æ¨¡
       for frame in motion:
           if check_collision(frame, scene):
               frame = adjust_trajectory(frame)
       return motion
   ```

æ ¹æ®å®é™…æµ‹è¯•ç»“æœå†å†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥è®­ç»ƒã€‚





## å»ºè®®2

---

### **æ ¸å¿ƒç»“è®º**
**ä»…è®­ç»ƒControlNetåœ¨ç‰¹å®šæ¡ä»¶ä¸‹å¯è¡Œ**ï¼Œä½†éœ€æ ¹æ®åœºæ™¯å¤æ‚åº¦å†³å®šæ˜¯å¦éœ€è¦å¾®è°ƒå…¶ä»–æ¨¡å—ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æï¼š

---

### **1. ä»…è®­ç»ƒControlNetçš„å¯è¡Œæ€§æ¡ä»¶**
#### **é€‚ç”¨åœºæ™¯**
```markdown
âœ… åœºæ™¯ç‰¹å¾ç®€å•ï¼ˆå¦‚å¹³é¢éšœç¢ç‰©é¿è®©ï¼‰  
âœ… åœºæ™¯æ•°æ®ä¸åŸå§‹è®­ç»ƒé›†åˆ†å¸ƒæ¥è¿‘  
âœ… äº¤äº’é€»è¾‘ä¸æ¶‰åŠåŠ¨ä½œè¯­ä¹‰é‡æ„ï¼ˆä»…ç©ºé—´é¿éšœï¼‰
```

#### **æ“ä½œæ­¥éª¤**
```python
# å†»ç»“ä¸»æ¨¡å‹å‚æ•°ï¼ˆä»£ç ç¤ºä¾‹ï¼‰
for p in self.text_encoder.parameters():
    p.requires_grad = False
for p in self.vae.parameters():
    p.requires_grad = False
for p in self.denoiser.parameters():
    p.requires_grad = False  # ä»…ControlNetå¯è®­ç»ƒ
```

---

### **2. éœ€è¦è”åˆè®­ç»ƒçš„åœºæ™¯**
#### **å…³é”®åˆ¤æ–­æŒ‡æ ‡**
```markdown
ğŸ“Œ åœºæ™¯-åŠ¨ä½œå¼ºè€¦åˆï¼ˆå¦‚"ååœ¨æ¤…å­ä¸Š"éœ€é‡æ„åå§¿éª¨éª¼ï¼‰  
ğŸ“Œ åœºæ™¯åŒ…å«åŠ¨æ€å…ƒç´ ï¼ˆç§»åŠ¨çš„éšœç¢ç‰©ï¼‰  
ğŸ“Œ äº¤äº’éœ€è¯­ä¹‰ç†è§£ï¼ˆ"æ¨é—¨"éœ€è¦æ‰‹éƒ¨æ¥è§¦ç‚¹é¢„æµ‹ï¼‰
```

#### **è”åˆè®­ç»ƒç­–ç•¥**
```python
# æ¢¯åº¦æ›´æ–°ç­–ç•¥ï¼ˆPartial Fine-tuningï¼‰
opt = torch.optim.Adam([
    {'params': self.controlnet.parameters(), 'lr': 1e-4},  
    {'params': self.vae.decoder.parameters(), 'lr': 5e-6},  # å¾®è°ƒè§£ç å™¨
    {'params': self.denoiser.time_embed.parameters(), 'lr': 1e-5}
])
```

---

### **3. åœºæ™¯äº¤äº’çš„å…³é”®å®ç°æ¨¡å—**
#### **å¿…è¦æ‰©å±•ç»„ä»¶**
| æ¨¡å—             | åŠŸèƒ½              | æ˜¯å¦éœ€è®­ç»ƒ   |
| ---------------- | ----------------- | ------------ |
| **SceneEncoder** | ç‚¹äº‘/ä½“ç´ åœºæ™¯ç¼–ç  | âœ”ï¸            |
| **ContactLoss**  | ç¢°æ’æŸå¤±å‡½æ•°      | âŒ (å›ºå®šè®¡ç®—) |
| **GraspAdapter** | æŠ“å–å§¿æ€æ˜ å°„å™¨    | âœ”ï¸            |

#### **æ ¸å¿ƒä»£ç æ‰©å±•**
```python
# åœ¨ControlNetä¸­æ·»åŠ åœºæ™¯æ³¨æ„åŠ›
class SceneAwareControlNet(nn.Module):
    def forward(self, x, scene_emb):
        # åœºæ™¯ç‰¹å¾æ³¨å…¥
        x = x + self.scene_proj(scene_emb)  # çº¿æ€§æŠ•å½±
        # ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶
        attn = CrossAttention(x, scene_emb)  
        return x * attn.sigmoid()
```

---

### **4. è®­ç»ƒæ•°æ®éœ€æ±‚å¯¹æ¯”**
| ç­–ç•¥         | æ‰€éœ€æ•°æ®é‡ | æ•°æ®æ¥æºå»ºè®®    |
| ------------ | ---------- | --------------- |
| ä»…ControlNet | 1k~5kæ ·æœ¬  | AMASS+åˆæˆåœºæ™¯  |
| è”åˆè®­ç»ƒ     | 10k+æ ·æœ¬   | PROX+GRABæ•°æ®é›† |

#### **åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•**
```python
def synthesize_scene_motion():
    smpl_pose = sample_from_amass()  
    scene = random_load_scene()  
    # ç‰©ç†å¼•æ“ä¿®æ­£
    corrected_pose = pybullet_adjust(smpl_pose, scene)  
    return corrected_pose, scene
```

---

### **5. æ¨ç†æµç¨‹æ”¹é€ **
```mermaid
graph TD
    A[è¾“å…¥æ–‡æœ¬] --> B{åœºæ™¯äº¤äº’?}
    B -- æ˜¯ --> C[SceneEncoderæå–ç‰¹å¾]
    C --> D[ControlNetèåˆåœºæ™¯/æ–‡æœ¬]
    D --> E[VAEè§£ç å¸¦åœºæ™¯çº¦æŸçš„åŠ¨ä½œ]
    B -- å¦ --> F[åŸå§‹ç”Ÿæˆæµç¨‹]
```

---

### **æ¨èæ–¹æ¡ˆ**
1. **åˆçº§æ–¹æ¡ˆï¼ˆå¿«é€ŸéªŒè¯ï¼‰**
   - å†»ç»“ä¸»æ¨¡å‹ï¼Œä»…è®­ç»ƒ`ControlNet+SceneEncoder`  
   - ä½¿ç”¨é¢„è®¡ç®—ç¢°æ’æŸå¤±ä½œä¸ºçº¦æŸ  
   - **ä¼˜ç‚¹**ï¼šè®­ç»ƒå¿«ï¼ˆ<2h/3090ï¼‰ï¼Œä»£ç æ”¹åŠ¨å°‘

2. **è¿›é˜¶æ–¹æ¡ˆï¼ˆé«˜ç²¾åº¦ï¼‰**
   - è”åˆè®­ç»ƒ`ControlNet + VAEè§£ç å™¨`  
   - æ·»åŠ è¯­ä¹‰æ¥è§¦ç‚¹é¢„æµ‹æ¨¡å—  
   - **ä¼˜ç‚¹**ï¼šæ”¯æŒå¤æ‚äº¤äº’ï¼ˆå¦‚æ”€çˆ¬ã€æ¬è¿ï¼‰  

---

### **é£é™©æç¤º**
- **æ¨¡å‹é€€åŒ–é£é™©**ï¼šé•¿æœŸä»…è®­ç»ƒControlNetå¯èƒ½å¯¼è‡´åŠ¨ä½œå¤šæ ·æ€§ä¸‹é™  
- **ç‰©ç†ä¸åˆç†æ€§**ï¼šéœ€é€šè¿‡æŸå¤±å‡½æ•°ç¡¬çº¦æŸï¼ˆå¦‚å…³èŠ‚è§’åº¦é™ä½ã€ç©¿é€æƒ©ç½šï¼‰  
- **å®æ—¶æ€§ä¸‹é™**ï¼šåœºæ™¯ç¼–ç å™¨å‚æ•°é‡éœ€æ§åˆ¶åœ¨<10Mï¼ˆæ¨èä½¿ç”¨PointNet++ Liteï¼‰  

å»ºè®®å…ˆä»**æ–¹æ¡ˆä¸€**å¼€å§‹éªŒè¯ï¼Œæ ¹æ®ç”Ÿæˆæ•ˆæœé€æ­¥è§£å†»å¿…è¦æ¨¡å—ã€‚MotionLCMçš„LCMæœºåˆ¶å¯¹å‚æ•°æ•æ„Ÿï¼Œéœ€è°¨æ…è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå»ºè®®â‰¤1e-4ï¼‰ã€‚



## çœ‹ä»£ç 

