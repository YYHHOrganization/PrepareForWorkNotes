# 综述2 动作生成与交互



A Survey on Human Interaction Motion Generation

Abstract Humans inhabit a world defined by interactions—with other humans, objects, and environments. These interactive movements not only convey our relationships with our surroundings but also demonstrate how we perceive and communicate with the real world. Therefore, replicating these interaction behaviors in digital systems has emerged as an important topic for applications in robotics, virtual reality, and animation. While recent advances in deep generative models and new datasets have accelerated progress in this field, significant challenges remain in modeling the intricate human dynamics and their interactions with entities in the external world. In this survey, we present, for the first time, a comprehensive overview of the literature in human interaction motion generation. We begin by establishing foundational concepts essential for understanding the research background. We then systematically review existing solutions and datasets across three primary interaction tasks—human-human, human-object, and human-scene interactions—followed by evaluation metrics. Finally, we discuss open research directions and future opportunities. The repository listing relevant papers is accessible at: https://github.com/soraproducer/ Awesome-Human-Interaction-Motion-Generation.  

Keywords Human Interaction · Motion Generation · Deep Learning · Literature Survey 

# 人体交互动作生成研究综述  

## 摘要  
人类生活在一个由交互构成的世界中——包括与他人、物体及环境的互动。这些交互动作不仅反映了我们与周围环境的关系，更展现了人类如何感知现实世界并进行信息传递。**重点** 因此，在数字系统中复现这些交互行为，已成为机器人学、虚拟现实和动画应用领域的重要课题。尽管深度生成模型的进展与新数据集的涌现推动了该领域发展，但在建模复杂人体动力学及其与外部实体交互方面仍存在重大挑战。本文首次对**人体交互动作生成**领域的研究成果进行了系统梳理：首先阐释理解研究背景所需的基础概念；随后从三大核心交互任务（**人-人交互**、**人-物体交互**、**人-场景交互**）出发，分类综述现有解决方案与数据集，并解析评估指标；最后探讨开放研究方向与未来机遇。相关论文资源库详见：https://github.com/soraproducer/Awesome-Human-Interaction-Motion-Generation。  

**关键词** 人体交互 · 动作生成 · 深度学习 · 文献综述  

---

### 两句话总结  
1. **重点** 本文首次系统综述了人体交互动作生成领域，涵盖人-人/物体/场景三类核心交互任务的研究方法、数据集与评估体系。  
2. **重点** 指出当前技术瓶颈在于复杂交互动力学建模，并为机器人、VR等应用场景指明未来研究方向。  





1 Introduction 

 Human life is fundamentally characterized by interactions with the external environment through motion [35,159]. These interactions range from everyday actions, such as using a smartphone and cooking, to social gestures, such as handshakes and clapping. Successfully understanding and reproducing such complex behaviors is crucial for developing human-like entities across various domains, including 3D virtual characters in entertainment media [9, 220], humanoid robots [102, 156], and digital avatars [11, 110, 218]. The past decade has witnessed remarkable progress in generative modeling across multiple domains: text [18, 176, 211], images [98, 177, 178], video [190, 214, 223], audio [44, 86, 216], and 3D objects [149, 161, 236]. This advancement has been driven by foundational generative models, including Variational Autoencoders (VAEs) [103], Generative Adversarial Networks (GANs) [66], Diffusion models [38, 252], Large Language Models (LLMs) [150], and Vision-Language Models (VLMs) [123]. These developments have also enhanced our ability to generate diverse and natural 3D human motions from various inputs such as action categories [69,136,165], textual descriptions [29,67,99,166,209,262], audio [7, 63, 121, 192, 213], and so on. However, generating human interaction motions presents distinct challenges beyond standard generative modeling approaches. First, human interaction is inherently stochastic, yet the resulting body movements must maintain spatial and temporal coherence that aligns with specific human intentions. Second, interacting with the external world demands environmental awareness, requiring adaptation to diverse scene layouts, understanding of object properties and affordances, and compliance with physical constraints to prevent intra- and inter-penetration. Last but not least, the collection of human interaction data is resource-intensive and difficult to scale, making it impractical to rely solely on data-driven learning. Therefore, incorporating domain expertise into learning models is essential to complement traditional generative methods. In summary, generating natural human interaction motions requires the ability to model human dynamics, incorporate physical constraints, and understand the spatial semantics and relationships within the holistic environment.  Despite these challenges, research on human interaction generation has advanced rapidly in the last decade, with growing interest over time. Fig. 1 chronicles these developments, highlighting key milestones covered in this survey. We categorize human interaction scenarios in existing motions into four main types: human-human interaction (HHI), human-object interaction (HOI), humanscene interaction (HSI), and human-mix interaction (involving multiple interaction types simultaneously). This survey provides a comprehensive review of interactive human motion generation, addressing recent advances and emerging challenges. The paper is structured as follows. In Section §2, we define the scope of this survey and identify related topics beyond its scope. Section §3 covers the preliminaries, providing foundational knowledge and key concepts essential for understanding the subsequent sections. Section §4 reviews the various methods and techniques employed in interactive human motion generation. In Section §5, we provide an overview of the commonly used datasets in this field, highlighting their distinct features. Section §6 explores the evaluation metrics utilized to measure the performance of these methods. Finally, Section §7 summarizes the current landscape and offers an exploration of future research directions. This survey aims to provide researchers and practitioners with a comprehensive understanding of the state of the art in this rapidly evolving field.



## 1 引言  

人类行为本质上是通过动作与外部环境持续交互的过程[35,159]。这些交互既包含使用手机、烹饪等日常行为，也涵盖握手、鼓掌等社交动作。**重点** 成功理解并复现此类复杂行为，对于开发娱乐媒体中的3D虚拟角色[9,220]、仿人机器人[102,156]和数字孪生体[11,110,218]等类人实体至关重要。过去十年间，生成式模型在文本[18,176,211]、图像[98,177,178]、视频[190,214,223]、音频[44,86,216]和3D对象[149,161,236]等领域取得显著进展，这归功于变分自编码器(VAEs)[103]、生成对抗网络(GANs)[66]、扩散模型[38,252]、大语言模型(LLMs)[150]和视觉语言模型(VLMs)[123]等基础模型的突破。这些进展同样提升了从动作类别[69,136,165]、文本描述[29,67,99,166,209,262]、音频[7,63,121,192,213]等多模态输入生成多样化自然3D人体动作的能力。  

然而，人体交互动作生成存在超越常规生成方法的特殊挑战：  
1. **时空一致性**：交互行为具有内在随机性，但生成动作需保持符合人类意图的时空连贯性；  
2. **环境感知**：需适应不同场景布局，理解物体属性与功能特性，并遵守物理约束以避免穿模问题；  
3. **数据稀缺性**：交互数据采集成本高且难以规模化，需结合领域知识弥补数据驱动方法的不足。  

**重点** 因此，生成自然的人体交互动作需要三大核心能力：人体动力学建模、物理约束整合、全局环境空间语义理解。尽管存在挑战，近十年来人体交互生成研究快速发展（图1呈现了关键里程碑）。现有研究主要聚焦四类场景：  
- 人-人交互(HHI)  
- 人-物体交互(HOI)  
- 人-场景交互(HSI)  
- 混合交互（多类型同时发生）  

本文首次系统梳理该领域研究进展：第2章界定综述范围；第3章阐述基础理论与关键概念；第4章分类解析生成方法；第5章对比主流数据集特性；第6章总结评估指标；第7章展望未来方向。本综述旨在为研究者提供该快速演进领域的全景式认知。  

$$
\text{生成质量} = f(\text{动力学建模}, \text{物理约束}, \text{语义理解})
$$

---

### 两句话总结  
1. **重点** 人体交互动作生成面临时空一致性、环境感知与数据稀缺三重挑战，需融合动力学建模与物理约束的混合方法。  
2. **重点** 现有研究按交互对象分为HHI/HOI/HSI三类，未来需开发能同时处理多模态输入与复杂场景的通用框架。  

（注：公式示例展示了核心要素的数学表达关系，实际使用时可根据具体模型替换$f(\cdot)$函数）





2 Scope

  This survey examines interactive human motion generation, with a focus on generation methods, datasets, and evaluation metrics across four key interaction types illustrated in Fig. 2: human-human, human-object, humanscene, and human-mix interactions. Our investigation encompasses various generation approaches, including interactive motion generation, motion prediction, and physics-based simulation. The scope of this survey specifically excludes human motion tasks that do not involve generation or interactions. Related but distinct research areas include single-person motion generation [285], human motion style transfer [6], human pose estimation [30, 129], and human action recognition [105]. For comprehensive reviews of these topics, the readers can refer to the respective surveys cited above. 3 Preliminaries  This section establishes the fundamental concepts of human interaction motion generation. We examine three key aspects: the entities involved in interactions, the conditions governing interaction motions, and the core methodologies for generating these motions. This foundation provides essential context for understanding the research developments discussed throughout the survey.

## 2 研究范围  

本综述聚焦**交互式人体动作生成**领域，重点围绕图2所示的四类核心交互场景展开：  
- **人-人交互（HHI）**  
- **人-物体交互（HOI）**  
- **人-场景交互（HSI）**  
- **混合交互**（多类型同时发生）  

研究内容涵盖三大方向：  
1. **交互动作生成**（从条件输入合成新动作）  
2. **动作预测**（基于历史帧预测未来动作）  
3. **基于物理的仿真**（通过物理引擎模拟动力学）  

**重点** 本文明确排除以下相关但独立的研究主题：  
- 单人生成（无交互）[285]  
- 动作风格迁移[6]  
- 人体姿态估计[30,129]  
- 行为识别[105]  

（读者可参阅上述引用文献获取这些领域的专项综述）  

## 3 基础理论  

本节构建人体交互动作生成的三大理论支柱，为后续章节提供认知框架：  

1. **交互实体**  
   - 人体（关节链式结构）  
   - 物体（刚体/可变形体）  
   - 场景（静态/动态环境）  

2. **交互条件**  
   $$ \mathcal{C} = \{ \text{空间约束}, \text{时序依赖}, \text{物理规律} \} $$  

3. **核心方法**  
   - 数据驱动（深度学习）  
   - 物理仿真（刚体/柔体动力学）  
   - 混合方法（神经物理融合）  

**重点** 该理论体系揭示了交互生成的本质挑战：如何在满足$$ \mathcal{C} $$约束的前提下，建模多实体间的动态耦合关系。  

---

### 两句话总结  
1. **重点** 本文界定的研究范围明确区分了交互生成与单人动作生成/姿态估计等相邻领域，聚焦四类交互场景与三种生成范式。  
2. **重点** 基础理论提出交互实体-条件-方法的三角框架，其数学表达$$ \mathcal{C} $$为后续方法分析提供了统一评估维度。  

（注：公式中$\mathcal{C}$可扩展为具体约束项的加权组合，例如：$$ \mathcal{C} = \alpha \cdot \text{碰撞约束} + \beta \cdot \text{运动平滑性} $$）





3.1 Interactive Entities  3.1.1 Human Motion  Human motion is a fundamental component of interactions. Accurate motion capture and efficient motion representation are essential for human interaction motion generation models.  Human Motion Capture. Human movements can be captured through several approaches, each with distinct trade-offs. Marker-based optical systems (e.g., Vicon [146] and OptiTrack [55]) track markers attached to key joints using multiple optical cameras, providing the highest precision but at a significant cost. Inertial-based motion capture systems offer an affordable alternative using IMU sensors or Smartsuits [147] to track body segment movements, although they require regular calibration to address sensor drift. RGB-D cameras (e.g., Kinect [273]) enable low-cost motion capture through single or multi-view setups, extracting 3D joint information from RGB and depth data, but typically lack fine motion details. Recent deep learning-based pose estimation methods [104, 232] can reconstruct 3D motions from video footage, although their generalization capabilities remain limited. Additionally, 3D graphics engines provide a flexible option for generating synthetic human motions in a virtual 3D environment, without physical capture equipment.  Representation. In kinematic-based methods, human motion is represented as a sequence of skeletal poses defined by joints or bones in 3D space. These motions can be expressed through either 3D joint positions or bone rotations along kinematic chains (e.g., limbs and spine). Recent studies [222, 284] favor rotation-based representations as they inherently encode skeletal topology. While traditional rotation formats (Euler angles and quaternions) are available, the 6D rotation representation [284] has gained prominence for its continuity and compatibility with deep learning models. Parametric models such as SMPL [134], SMPL-X [163], and GHUM [243] extend beyond rotational pose parameters by incorporating shape parameters. These models parameterize surface vertices and deformations using both pose and shape information, enabling geometry-aware motion representation, which is crucial for fine-grained interactions.

### 3.1 交互实体  

#### 3.1.1 人体动作  

**重点** 人体动作是交互行为的核心载体，其精确捕捉与高效表征直接影响交互生成模型的效果。  

**动作捕捉技术**  
现有方法各具优势与局限：  

- **光学标记系统**（如Vicon[146]、OptiTrack[55]）：  
  通过多摄像头追踪关节标记点，精度最高但成本昂贵  
- **惯性传感器系统**（如Smartsuits[147]）：  
  使用IMU传感器捕捉肢体运动，性价比高但需定期校准以消除漂移误差  
- **RGB-D相机**（如Kinect[273]）：  
  基于单/多视角RGB-D数据重建3D关节，成本低但细节缺失  
- **深度学习姿态估计**[104,232]：  
  从视频中预测3D动作，泛化能力仍受限  
- **3D引擎合成**：  
  虚拟环境生成动作，无需物理捕捉设备  

**动作表征方法**  
运动学方法将人体动作定义为3D空间关节/骨骼的时序姿态：  

1. ==**基础表征**==
   - 关节坐标：$$ \mathbf{P}_t = [x_1,y_1,z_1,...,x_J,y_J,z_J]_t $$  
   - 骨骼旋转：沿运动链（如四肢、脊柱）的旋转序列  

2. **进阶表征**  
   - **6D旋转表示**[284]：  
     $$ \mathbf{R}_{6D} = [\mathbf{u}_1,\mathbf{u}_2] \quad \text{（优于欧拉角/四元数的连续性）} $$  
   - **参数化模型**（SMPL[134]/SMPL-X[163]/GHUM[243]）：  
     $$ \mathbf{V} = W(\mathbf{\beta}, \mathbf{\theta}) $$  
     其中$\mathbf{\beta}$为体型参数，$\mathbf{\theta}$为姿态参数，实现几何感知的精细交互建模  

---

### 两句话总结  
1. **重点** 动作捕捉技术呈现"精度-成本"权衡，6D旋转表示与参数化模型（如SMPL）已成为当前最优动作表征方案。  
2. **重点** 参数化模型通过$$ W(\mathbf{\beta}, \mathbf{\theta}) $$函数统一描述姿态与体型，为衣物变形等细粒度交互提供数学基础。  

（注：公式中$J$表示关节数量，$W(\cdot)$为蒙皮函数，可根据具体模型扩展为$$ \mathbf{V} = \sum_{k=1}^K w_k G_k(\mathbf{\theta}, \mathbf{\beta}) \mathbf{T}_k $$等详细表达式）





3.1.3 Scene  Scenes provide the spatial and contextual foundation for interactions, necessitating accurate acquisition methods and structured representations to model human-scene relationships effectively.  Scene Acquisition. Human-scene interactions can be captured in both real and virtual environments. Real-world scenes are digitized using advanced scanning technologies: LiDAR systems capture high-resolution 3D point clouds [36], while structured light scanning [79, 101] reconstructs detailed surface geometry. Alternatively, existing 3D scene datasets, such as ScanNet [40], provide ready-to-use virtual environments [234]. Recent approaches have expanded scene diversity through synthetic virtual environments [8, 21, 93, 95]. Created using 3D modeling tools like Unity [171], Unreal Engine [126], or Blender [43], these environments offer precise control over scene parameters, including textures, lighting,  and object placement. This approach enables the scalable capture of complex human-scene interactions across diverse settings.  Representation. Point clouds are widely used due to their lightweight nature and their ability to preserve detailed geometric information. Each point contains the 3D spatial coordinates of scene surfaces, with additional features like surface normals or semantic labels. These representations are typically processed using specialized architectures, like PointNet [173] or PointTransformer [275]. Occupancy grids and voxel representations discretize 3D space into regular cells containing binary information. These approaches facilitate efficient collision checking and spatial reasoning, making them particularly valuable for human-scene interaction tasks. Various architectures, including 3D Convolutional Networks (3D-CNNs) [212] and Vision Transformers (ViTs) [47], have been employed to process these representations. Similar to object representation, BPS [170] features also offer a structured encoding of scene geometry by measuring point-wise distances to a predefined set of basis points.

### 3.1.3 场景  

**重点** 场景为交互行为提供空间载体和语义背景，其精确获取与结构化表征直接影响人-场景交互的建模效果。  

#### 场景获取技术  
**真实场景数字化**  
- **激光雷达（LiDAR）**[36]：  
  通过高频激光扫描生成高精度3D点云  
- **结构光扫描**[79,101]：  
  利用光栅投影重建物体表面几何细节  
- **现成数据集**（如ScanNet[40]）：  
  提供开箱即用的预扫描3D场景  

**虚拟场景合成**  
- **3D建模工具**（Unity[171]/Unreal[126]/Blender[43]）：  
  可精确调控纹理、光照、物体布局等参数  
- **优势**：  
  $$ \text{场景多样性} \propto \text{交互数据规模} $$  
  支持大规模生成复杂人-场景交互数据  

#### 场景表征方法  
1. **点云表示**  
   - 基础形式：$$ \mathcal{P} = \{ (x_i,y_i,z_i,\mathbf{f}_i) \}_{i=1}^N $$  
     其中$\mathbf{f}_i$可包含法向量、语义标签等特征  
   - 处理网络：PointNet[173]、PointTransformer[275]  

2. **体素表示**  
   - 空间离散化：$$ \mathcal{V}_{ijk} = \begin{cases} 
   1 & \text{占据} \\ 
   0 & \text{空闲} 
   \end{cases} $$  
   - 适用任务：实时碰撞检测、空间关系推理  
   - 处理网络：3D-CNN[212]、ViT[47]  

3. **结构化编码**  
   - **基础点集特征（BPS）**[170]：  
     $$ \phi_{\text{BPS}}(\mathbf{p}) = [\|\mathbf{p}-\mathbf{b}_1\|,...,\|\mathbf{p}-\mathbf{b}_K\|] $$  
     通过预定义基点的距离向量编码空间几何  

---

### 两句话总结  
1. **重点** 场景获取呈现"虚实融合"趋势，虚拟引擎的$$ \text{参数化控制} $$能力显著提升了交互数据多样性。  
2. **重点** 点云/体素/BPS三类表征形成互补：点云保细节、体素利计算、BPS强结构化，共同支撑$$ \mathcal{V}_{ijk} $$与$$ \phi_{\text{BPS}} $$等空间关系建模。  

（注：公式中$\propto$表示正比关系，BPS的基点集$\{\mathbf{b}_k\}_{k=1}^K$通常采用均匀采样或关键点提取策略）



继续翻译论文《A Survey on Human Interaction Motion Generation》的内容，请翻译成中文，并给出两句话总结，重点的标注**重点**，如果是1 使用##，如果是 3.1使用###，公式都弄好,公式前后加上$$，并帮我可读性写高一点（同时不可以缺少语句，原文该有的都有，可以增加你的理解，让翻译得容易读懂）,我要复制进typora中：

3.2 Conditioning Modalities  Human interaction motion synthesis often conditions on other modalities. These modalities provide additional context or constraints, enabling more controllable and semantically consistent motion generation. Text Textual descriptions have emerged as a popular modality for guiding human interaction motion generation [52, 124, 148, 169, 184, 185, 235, 250]. Text-based guidance enables models to process detailed instructions that define interactions between generated human motions and various entities (humans, objects, scenes, or combinations thereof). These text conditions are typically incorporated either as embeddings—such as CLIP [175] embeddings [52, 148, 169, 184, 185, 235]—as penultimate layer outputs from LLMs [124], or as sequences of discrete word tokens [250].Audio Audio-driven approaches enable models to generate interaction motions synchronized with acoustic cues, typically in HHI scenarios [5, 191, 253]. The audio input manifests either as conversational exchanges between actors and reactors [5, 253] or as musical accompaniment [191] coordinating multiple participants. These acoustic signals are processed into salient features—including prosody, excitation, music intensity, and rhythmic beats—using established tools such as OpenSmile [50] and Librosa [143]. Action Class Action classes serve as a wellestablished conditioning mechanism in huaman inter action motion generation [61, 72, 138, 245]. These categorical descriptors are typically implemented as one-hot encodings [61,72,138] or label token embeddings [59,245], representing basic interactions such as "Shake Hands" or "Combat".  Spatial and Temporal Signal Diverse spatial signals guide interactive motion generation, encompassing goal poses [34, 116, 202, 258, 277], root trajectory [17, 130, 215, 283], root positions [8, 111], orientations [235], object motions [118], and gamepad signals (e.g., instant direction, speed) [195, 197, 198]. These explicit, deterministic signals provide precise control over generated motions, enhancing both accuracy and adaptability while preserving motion fidelity.

### 3.2 条件模态  

**重点** 人体交互动作生成常需多模态条件输入，这些条件提供语义约束或上下文指引，使生成动作更具可控性和一致性。  

#### 文本条件  
- **应用场景**：  
  通过自然语言描述指导人-物/人-场景交互（如"端杯子喝水"）  
- **表征方法**：  
  - CLIP嵌入[52,148]：$$ \mathbf{e}_{\text{text}} = \text{CLIP}(\text{"握手动作"}) $$  
  - 大语言模型输出[124]：取LLM倒数第二层特征  
  - 词元序列[250]：直接处理离散token  

#### 音频条件  
- **交互类型**：  
  - 对话场景[5,253]：基于语音韵律生成回应动作  
  - 音乐场景[191]：根据节拍强度协调多人舞蹈  
- **特征提取**：  
  $$ \mathbf{f}_{\text{audio}} = \text{Librosa}(\text{音频波形}) $$  
  包含语调、音乐强度等特征  

#### 动作类别  
- **编码方式**：  
  - One-hot向量[61,72]：$$ [0,1,0] \rightarrow \text{"拥抱"} $$  
  - 标签嵌入[59,245]：通过Embedding层映射类别语义  

#### 时空信号  
- **空间控制**：  
  - 目标姿态[34,258]：$$ \mathbf{\hat{p}}_t = \text{LSTM}(\mathbf{p}_{t-1}) $$  
  - 根轨迹[17,283]：控制整体移动路径  
- **实时控制**：  
  游戏手柄信号[195]：将方向/速度映射为动作参数  

---

### 两句话总结  
1. **重点** 多模态条件形成层次化控制体系：文本/音频提供**高层语义**，时空信号实现**帧级精确控制**，共同满足$$ \mathbf{e}_{\text{text}} \oplus \mathbf{f}_{\text{audio}} $$的混合条件需求。  
2. **重点** 动作类别编码将离散标签（如"握手"）转化为连续嵌入空间，解决了符号系统与神经网络间的**语义鸿沟**问题。  

（注：公式中$\oplus$表示多模态特征融合，实际实现可采用拼接/注意力等机制）