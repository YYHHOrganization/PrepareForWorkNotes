# 综述2 动作生成与交互



A Survey on Human Interaction Motion Generation

Abstract Humans inhabit a world defined by interactions—with other humans, objects, and environments. These interactive movements not only convey our relationships with our surroundings but also demonstrate how we perceive and communicate with the real world. Therefore, replicating these interaction behaviors in digital systems has emerged as an important topic for applications in robotics, virtual reality, and animation. While recent advances in deep generative models and new datasets have accelerated progress in this field, significant challenges remain in modeling the intricate human dynamics and their interactions with entities in the external world. In this survey, we present, for the first time, a comprehensive overview of the literature in human interaction motion generation. We begin by establishing foundational concepts essential for understanding the research background. We then systematically review existing solutions and datasets across three primary interaction tasks—human-human, human-object, and human-scene interactions—followed by evaluation metrics. Finally, we discuss open research directions and future opportunities. The repository listing relevant papers is accessible at: https://github.com/soraproducer/ Awesome-Human-Interaction-Motion-Generation.  

Keywords Human Interaction · Motion Generation · Deep Learning · Literature Survey 

# 人体交互动作生成研究综述  

## 摘要  
人类生活在一个由交互构成的世界中——包括与他人、物体及环境的互动。这些交互动作不仅反映了我们与周围环境的关系，更展现了人类如何感知现实世界并进行信息传递。**重点** 因此，在数字系统中复现这些交互行为，已成为机器人学、虚拟现实和动画应用领域的重要课题。尽管深度生成模型的进展与新数据集的涌现推动了该领域发展，但在建模复杂人体动力学及其与外部实体交互方面仍存在重大挑战。本文首次对**人体交互动作生成**领域的研究成果进行了系统梳理：首先阐释理解研究背景所需的基础概念；随后从三大核心交互任务（**人-人交互**、**人-物体交互**、**人-场景交互**）出发，分类综述现有解决方案与数据集，并解析评估指标；最后探讨开放研究方向与未来机遇。相关论文资源库详见：https://github.com/soraproducer/Awesome-Human-Interaction-Motion-Generation。  

**关键词** 人体交互 · 动作生成 · 深度学习 · 文献综述  

---

### 两句话总结  
1. **重点** 本文首次系统综述了人体交互动作生成领域，涵盖人-人/物体/场景三类核心交互任务的研究方法、数据集与评估体系。  
2. **重点** 指出当前技术瓶颈在于复杂交互动力学建模，并为机器人、VR等应用场景指明未来研究方向。  





1 Introduction 

 Human life is fundamentally characterized by interactions with the external environment through motion [35,159]. These interactions range from everyday actions, such as using a smartphone and cooking, to social gestures, such as handshakes and clapping. Successfully understanding and reproducing such complex behaviors is crucial for developing human-like entities across various domains, including 3D virtual characters in entertainment media [9, 220], humanoid robots [102, 156], and digital avatars [11, 110, 218]. The past decade has witnessed remarkable progress in generative modeling across multiple domains: text [18, 176, 211], images [98, 177, 178], video [190, 214, 223], audio [44, 86, 216], and 3D objects [149, 161, 236]. This advancement has been driven by foundational generative models, including Variational Autoencoders (VAEs) [103], Generative Adversarial Networks (GANs) [66], Diffusion models [38, 252], Large Language Models (LLMs) [150], and Vision-Language Models (VLMs) [123]. These developments have also enhanced our ability to generate diverse and natural 3D human motions from various inputs such as action categories [69,136,165], textual descriptions [29,67,99,166,209,262], audio [7, 63, 121, 192, 213], and so on. However, generating human interaction motions presents distinct challenges beyond standard generative modeling approaches. First, human interaction is inherently stochastic, yet the resulting body movements must maintain spatial and temporal coherence that aligns with specific human intentions. Second, interacting with the external world demands environmental awareness, requiring adaptation to diverse scene layouts, understanding of object properties and affordances, and compliance with physical constraints to prevent intra- and inter-penetration. Last but not least, the collection of human interaction data is resource-intensive and difficult to scale, making it impractical to rely solely on data-driven learning. Therefore, incorporating domain expertise into learning models is essential to complement traditional generative methods. In summary, generating natural human interaction motions requires the ability to model human dynamics, incorporate physical constraints, and understand the spatial semantics and relationships within the holistic environment.  Despite these challenges, research on human interaction generation has advanced rapidly in the last decade, with growing interest over time. Fig. 1 chronicles these developments, highlighting key milestones covered in this survey. We categorize human interaction scenarios in existing motions into four main types: human-human interaction (HHI), human-object interaction (HOI), humanscene interaction (HSI), and human-mix interaction (involving multiple interaction types simultaneously). This survey provides a comprehensive review of interactive human motion generation, addressing recent advances and emerging challenges. The paper is structured as follows. In Section §2, we define the scope of this survey and identify related topics beyond its scope. Section §3 covers the preliminaries, providing foundational knowledge and key concepts essential for understanding the subsequent sections. Section §4 reviews the various methods and techniques employed in interactive human motion generation. In Section §5, we provide an overview of the commonly used datasets in this field, highlighting their distinct features. Section §6 explores the evaluation metrics utilized to measure the performance of these methods. Finally, Section §7 summarizes the current landscape and offers an exploration of future research directions. This survey aims to provide researchers and practitioners with a comprehensive understanding of the state of the art in this rapidly evolving field.



## 1 引言  

人类行为本质上是通过动作与外部环境持续交互的过程[35,159]。这些交互既包含使用手机、烹饪等日常行为，也涵盖握手、鼓掌等社交动作。**重点** 成功理解并复现此类复杂行为，对于开发娱乐媒体中的3D虚拟角色[9,220]、仿人机器人[102,156]和数字孪生体[11,110,218]等类人实体至关重要。过去十年间，生成式模型在文本[18,176,211]、图像[98,177,178]、视频[190,214,223]、音频[44,86,216]和3D对象[149,161,236]等领域取得显著进展，这归功于变分自编码器(VAEs)[103]、生成对抗网络(GANs)[66]、扩散模型[38,252]、大语言模型(LLMs)[150]和视觉语言模型(VLMs)[123]等基础模型的突破。这些进展同样提升了从动作类别[69,136,165]、文本描述[29,67,99,166,209,262]、音频[7,63,121,192,213]等多模态输入生成多样化自然3D人体动作的能力。  

然而，人体交互动作生成存在超越常规生成方法的特殊挑战：  
1. **时空一致性**：交互行为具有内在随机性，但生成动作需保持符合人类意图的时空连贯性；  
2. **环境感知**：需适应不同场景布局，理解物体属性与功能特性，并遵守物理约束以避免穿模问题；  
3. **数据稀缺性**：交互数据采集成本高且难以规模化，需结合领域知识弥补数据驱动方法的不足。  

**重点** 因此，生成自然的人体交互动作需要三大核心能力：人体动力学建模、物理约束整合、全局环境空间语义理解。尽管存在挑战，近十年来人体交互生成研究快速发展（图1呈现了关键里程碑）。现有研究主要聚焦四类场景：  
- 人-人交互(HHI)  
- 人-物体交互(HOI)  
- 人-场景交互(HSI)  
- 混合交互（多类型同时发生）  

本文首次系统梳理该领域研究进展：第2章界定综述范围；第3章阐述基础理论与关键概念；第4章分类解析生成方法；第5章对比主流数据集特性；第6章总结评估指标；第7章展望未来方向。本综述旨在为研究者提供该快速演进领域的全景式认知。  

$$
\text{生成质量} = f(\text{动力学建模}, \text{物理约束}, \text{语义理解})
$$

---

### 两句话总结  
1. **重点** 人体交互动作生成面临时空一致性、环境感知与数据稀缺三重挑战，需融合动力学建模与物理约束的混合方法。  
2. **重点** 现有研究按交互对象分为HHI/HOI/HSI三类，未来需开发能同时处理多模态输入与复杂场景的通用框架。  

（注：公式示例展示了核心要素的数学表达关系，实际使用时可根据具体模型替换$f(\cdot)$函数）





2 Scope

  This survey examines interactive human motion generation, with a focus on generation methods, datasets, and evaluation metrics across four key interaction types illustrated in Fig. 2: human-human, human-object, humanscene, and human-mix interactions. Our investigation encompasses various generation approaches, including interactive motion generation, motion prediction, and physics-based simulation. The scope of this survey specifically excludes human motion tasks that do not involve generation or interactions. Related but distinct research areas include single-person motion generation [285], human motion style transfer [6], human pose estimation [30, 129], and human action recognition [105]. For comprehensive reviews of these topics, the readers can refer to the respective surveys cited above. 3 Preliminaries  This section establishes the fundamental concepts of human interaction motion generation. We examine three key aspects: the entities involved in interactions, the conditions governing interaction motions, and the core methodologies for generating these motions. This foundation provides essential context for understanding the research developments discussed throughout the survey.

## 2 研究范围  

本综述聚焦**交互式人体动作生成**领域，重点围绕图2所示的四类核心交互场景展开：  
- **人-人交互（HHI）**  
- **人-物体交互（HOI）**  
- **人-场景交互（HSI）**  
- **混合交互**（多类型同时发生）  

研究内容涵盖三大方向：  
1. **交互动作生成**（从条件输入合成新动作）  
2. **动作预测**（基于历史帧预测未来动作）  
3. **基于物理的仿真**（通过物理引擎模拟动力学）  

**重点** 本文明确排除以下相关但独立的研究主题：  
- 单人生成（无交互）[285]  
- 动作风格迁移[6]  
- 人体姿态估计[30,129]  
- 行为识别[105]  

（读者可参阅上述引用文献获取这些领域的专项综述）  

## 3 基础理论  

本节构建人体交互动作生成的三大理论支柱，为后续章节提供认知框架：  

1. **交互实体**  
   - 人体（关节链式结构）  
   - 物体（刚体/可变形体）  
   - 场景（静态/动态环境）  

2. **交互条件**  
   $$ \mathcal{C} = \{ \text{空间约束}, \text{时序依赖}, \text{物理规律} \} $$  

3. **核心方法**  
   - 数据驱动（深度学习）  
   - 物理仿真（刚体/柔体动力学）  
   - 混合方法（神经物理融合）  

**重点** 该理论体系揭示了交互生成的本质挑战：如何在满足$$ \mathcal{C} $$约束的前提下，建模多实体间的动态耦合关系。  

---

### 两句话总结  
1. **重点** 本文界定的研究范围明确区分了交互生成与单人动作生成/姿态估计等相邻领域，聚焦四类交互场景与三种生成范式。  
2. **重点** 基础理论提出交互实体-条件-方法的三角框架，其数学表达$$ \mathcal{C} $$为后续方法分析提供了统一评估维度。  

（注：公式中$\mathcal{C}$可扩展为具体约束项的加权组合，例如：$$ \mathcal{C} = \alpha \cdot \text{碰撞约束} + \beta \cdot \text{运动平滑性} $$）





3.1 Interactive Entities  3.1.1 Human Motion  Human motion is a fundamental component of interactions. Accurate motion capture and efficient motion representation are essential for human interaction motion generation models.  Human Motion Capture. Human movements can be captured through several approaches, each with distinct trade-offs. Marker-based optical systems (e.g., Vicon [146] and OptiTrack [55]) track markers attached to key joints using multiple optical cameras, providing the highest precision but at a significant cost. Inertial-based motion capture systems offer an affordable alternative using IMU sensors or Smartsuits [147] to track body segment movements, although they require regular calibration to address sensor drift. RGB-D cameras (e.g., Kinect [273]) enable low-cost motion capture through single or multi-view setups, extracting 3D joint information from RGB and depth data, but typically lack fine motion details. Recent deep learning-based pose estimation methods [104, 232] can reconstruct 3D motions from video footage, although their generalization capabilities remain limited. Additionally, 3D graphics engines provide a flexible option for generating synthetic human motions in a virtual 3D environment, without physical capture equipment.  Representation. In kinematic-based methods, human motion is represented as a sequence of skeletal poses defined by joints or bones in 3D space. These motions can be expressed through either 3D joint positions or bone rotations along kinematic chains (e.g., limbs and spine). Recent studies [222, 284] favor rotation-based representations as they inherently encode skeletal topology. While traditional rotation formats (Euler angles and quaternions) are available, the 6D rotation representation [284] has gained prominence for its continuity and compatibility with deep learning models. Parametric models such as SMPL [134], SMPL-X [163], and GHUM [243] extend beyond rotational pose parameters by incorporating shape parameters. These models parameterize surface vertices and deformations using both pose and shape information, enabling geometry-aware motion representation, which is crucial for fine-grained interactions.

### 3.1 交互实体  

#### 3.1.1 人体动作  

**重点** 人体动作是交互行为的核心载体，其精确捕捉与高效表征直接影响交互生成模型的效果。  

**动作捕捉技术**  
现有方法各具优势与局限：  

- **光学标记系统**（如Vicon[146]、OptiTrack[55]）：  
  通过多摄像头追踪关节标记点，精度最高但成本昂贵  
- **惯性传感器系统**（如Smartsuits[147]）：  
  使用IMU传感器捕捉肢体运动，性价比高但需定期校准以消除漂移误差  
- **RGB-D相机**（如Kinect[273]）：  
  基于单/多视角RGB-D数据重建3D关节，成本低但细节缺失  
- **深度学习姿态估计**[104,232]：  
  从视频中预测3D动作，泛化能力仍受限  
- **3D引擎合成**：  
  虚拟环境生成动作，无需物理捕捉设备  

**动作表征方法**  
运动学方法将人体动作定义为3D空间关节/骨骼的时序姿态：  

1. ==**基础表征**==
   - 关节坐标：$$ \mathbf{P}_t = [x_1,y_1,z_1,...,x_J,y_J,z_J]_t $$  
   - 骨骼旋转：沿运动链（如四肢、脊柱）的旋转序列  

2. **进阶表征**  
   - **6D旋转表示**[284]：  
     $$ \mathbf{R}_{6D} = [\mathbf{u}_1,\mathbf{u}_2] \quad \text{（优于欧拉角/四元数的连续性）} $$  
   - **参数化模型**（SMPL[134]/SMPL-X[163]/GHUM[243]）：  
     $$ \mathbf{V} = W(\mathbf{\beta}, \mathbf{\theta}) $$  
     其中$\mathbf{\beta}$为体型参数，$\mathbf{\theta}$为姿态参数，实现几何感知的精细交互建模  

---

### 两句话总结  
1. **重点** 动作捕捉技术呈现"精度-成本"权衡，6D旋转表示与参数化模型（如SMPL）已成为当前最优动作表征方案。  
2. **重点** 参数化模型通过$$ W(\mathbf{\beta}, \mathbf{\theta}) $$函数统一描述姿态与体型，为衣物变形等细粒度交互提供数学基础。  

（注：公式中$J$表示关节数量，$W(\cdot)$为蒙皮函数，可根据具体模型扩展为$$ \mathbf{V} = \sum_{k=1}^K w_k G_k(\mathbf{\theta}, \mathbf{\beta}) \mathbf{T}_k $$等详细表达式）





3.1.3 Scene  Scenes provide the spatial and contextual foundation for interactions, necessitating accurate acquisition methods and structured representations to model human-scene relationships effectively.  Scene Acquisition. Human-scene interactions can be captured in both real and virtual environments. Real-world scenes are digitized using advanced scanning technologies: LiDAR systems capture high-resolution 3D point clouds [36], while structured light scanning [79, 101] reconstructs detailed surface geometry. Alternatively, existing 3D scene datasets, such as ScanNet [40], provide ready-to-use virtual environments [234]. Recent approaches have expanded scene diversity through synthetic virtual environments [8, 21, 93, 95]. Created using 3D modeling tools like Unity [171], Unreal Engine [126], or Blender [43], these environments offer precise control over scene parameters, including textures, lighting,  and object placement. This approach enables the scalable capture of complex human-scene interactions across diverse settings.  Representation. Point clouds are widely used due to their lightweight nature and their ability to preserve detailed geometric information. Each point contains the 3D spatial coordinates of scene surfaces, with additional features like surface normals or semantic labels. These representations are typically processed using specialized architectures, like PointNet [173] or PointTransformer [275]. Occupancy grids and voxel representations discretize 3D space into regular cells containing binary information. These approaches facilitate efficient collision checking and spatial reasoning, making them particularly valuable for human-scene interaction tasks. Various architectures, including 3D Convolutional Networks (3D-CNNs) [212] and Vision Transformers (ViTs) [47], have been employed to process these representations. Similar to object representation, BPS [170] features also offer a structured encoding of scene geometry by measuring point-wise distances to a predefined set of basis points.

### 3.1.3 场景  

**重点** 场景为交互行为提供空间载体和语义背景，其精确获取与结构化表征直接影响人-场景交互的建模效果。  

#### 场景获取技术  
**真实场景数字化**  
- **激光雷达（LiDAR）**[36]：  
  通过高频激光扫描生成高精度3D点云  
- **结构光扫描**[79,101]：  
  利用光栅投影重建物体表面几何细节  
- **现成数据集**（如ScanNet[40]）：  
  提供开箱即用的预扫描3D场景  

**虚拟场景合成**  
- **3D建模工具**（Unity[171]/Unreal[126]/Blender[43]）：  
  可精确调控纹理、光照、物体布局等参数  
- **优势**：  
  $$ \text{场景多样性} \propto \text{交互数据规模} $$  
  支持大规模生成复杂人-场景交互数据  

#### 场景表征方法  
1. **点云表示**  
   - 基础形式：$$ \mathcal{P} = \{ (x_i,y_i,z_i,\mathbf{f}_i) \}_{i=1}^N $$  
     其中$\mathbf{f}_i$可包含法向量、语义标签等特征  
   - 处理网络：PointNet[173]、PointTransformer[275]  

2. **体素表示**  
   - 空间离散化：$$ \mathcal{V}_{ijk} = \begin{cases} 
   1 & \text{占据} \\ 
   0 & \text{空闲} 
   \end{cases} $$  
   - 适用任务：实时碰撞检测、空间关系推理  
   - 处理网络：3D-CNN[212]、ViT[47]  

3. **结构化编码**  
   - **基础点集特征（BPS）**[170]：  
     $$ \phi_{\text{BPS}}(\mathbf{p}) = [\|\mathbf{p}-\mathbf{b}_1\|,...,\|\mathbf{p}-\mathbf{b}_K\|] $$  
     通过预定义基点的距离向量编码空间几何  

---

### 两句话总结  
1. **重点** 场景获取呈现"虚实融合"趋势，虚拟引擎的$$ \text{参数化控制} $$能力显著提升了交互数据多样性。  
2. **重点** 点云/体素/BPS三类表征形成互补：点云保细节、体素利计算、BPS强结构化，共同支撑$$ \mathcal{V}_{ijk} $$与$$ \phi_{\text{BPS}} $$等空间关系建模。  

（注：公式中$\propto$表示正比关系，BPS的基点集$\{\mathbf{b}_k\}_{k=1}^K$通常采用均匀采样或关键点提取策略）





3.2 Conditioning Modalities  Human interaction motion synthesis often conditions on other modalities. These modalities provide additional context or constraints, enabling more controllable and semantically consistent motion generation. Text Textual descriptions have emerged as a popular modality for guiding human interaction motion generation [52, 124, 148, 169, 184, 185, 235, 250]. Text-based guidance enables models to process detailed instructions that define interactions between generated human motions and various entities (humans, objects, scenes, or combinations thereof). These text conditions are typically incorporated either as embeddings—such as CLIP [175] embeddings [52, 148, 169, 184, 185, 235]—as penultimate layer outputs from LLMs [124], or as sequences of discrete word tokens [250].Audio Audio-driven approaches enable models to generate interaction motions synchronized with acoustic cues, typically in HHI scenarios [5, 191, 253]. The audio input manifests either as conversational exchanges between actors and reactors [5, 253] or as musical accompaniment [191] coordinating multiple participants. These acoustic signals are processed into salient features—including prosody, excitation, music intensity, and rhythmic beats—using established tools such as OpenSmile [50] and Librosa [143]. Action Class Action classes serve as a wellestablished conditioning mechanism in huaman inter action motion generation [61, 72, 138, 245]. These categorical descriptors are typically implemented as one-hot encodings [61,72,138] or label token embeddings [59,245], representing basic interactions such as "Shake Hands" or "Combat".  Spatial and Temporal Signal Diverse spatial signals guide interactive motion generation, encompassing goal poses [34, 116, 202, 258, 277], root trajectory [17, 130, 215, 283], root positions [8, 111], orientations [235], object motions [118], and gamepad signals (e.g., instant direction, speed) [195, 197, 198]. These explicit, deterministic signals provide precise control over generated motions, enhancing both accuracy and adaptability while preserving motion fidelity.

### 3.2 条件模态  

**重点** 人体交互动作生成常需多模态条件输入，这些条件提供语义约束或上下文指引，使生成动作更具可控性和一致性。  

#### 文本条件  
- **应用场景**：  
  通过自然语言描述指导人-物/人-场景交互（如"端杯子喝水"）  
- **表征方法**：  
  - CLIP嵌入[52,148]：$$ \mathbf{e}_{\text{text}} = \text{CLIP}(\text{"握手动作"}) $$  
  - 大语言模型输出[124]：取LLM倒数第二层特征  
  - 词元序列[250]：直接处理离散token  

#### 音频条件  
- **交互类型**：  
  - 对话场景[5,253]：基于语音韵律生成回应动作  
  - 音乐场景[191]：根据节拍强度协调多人舞蹈  
- **特征提取**：  
  $$ \mathbf{f}_{\text{audio}} = \text{Librosa}(\text{音频波形}) $$  
  包含语调、音乐强度等特征  

#### 动作类别  
- **编码方式**：  
  - One-hot向量[61,72]：$$ [0,1,0] \rightarrow \text{"拥抱"} $$  
  - 标签嵌入[59,245]：通过Embedding层映射类别语义  

#### 时空信号  
- **空间控制**：  
  - 目标姿态[34,258]：$$ \mathbf{\hat{p}}_t = \text{LSTM}(\mathbf{p}_{t-1}) $$  
  - 根轨迹[17,283]：控制整体移动路径  
- **实时控制**：  
  游戏手柄信号[195]：将方向/速度映射为动作参数  

---

### 两句话总结  
1. **重点** 多模态条件形成层次化控制体系：文本/音频提供**高层语义**，时空信号实现**帧级精确控制**，共同满足$$ \mathbf{e}_{\text{text}} \oplus \mathbf{f}_{\text{audio}} $$的混合条件需求。  
2. **重点** 动作类别编码将离散标签（如"握手"）转化为连续嵌入空间，解决了符号系统与神经网络间的**语义鸿沟**问题。  

（注：公式中$\oplus$表示多模态特征融合，实际实现可采用拼接/注意力等机制）







3.3 Fundamental Methods for Interaction Synthesis  In this subsection, we introduce fundamental methods used in human interaction motion generation, ranging from classical approaches to the latest deep generative frameworks.  3.3.1 Motion Graph  Graph-based methods [106] represent a foundational approach in human interaction motion generation [4, 87, 154, 253], leveraging the inherent structure of motion data to synthesize novel sequences. These approaches typically implement motion graphs—directed graphs where nodes represent motion segments or poses, and edges denote viable transitions between segments. Novel motion synthesis occurs through graph traversal, where random walks along connected nodes generate coherent motion sequences. This framework enables the combinatorial fusion of characteristics from multiple exemplars, producing diverse motions while preserving the authenticity of the source data. However, graph-based methods exhibit inherent limitations [106] in scalability. The approach necessitates storing the complete dataset and performing graph traversal during inference, introducing undesirable computational and storage overhead.  3.3.2 Deterministic Regression  Deterministic regression models [3, 12, 109] formulate interactive motion generation as a one-to-one mapping problem, predicting deterministic target motions from specified input conditions, typically supervised by L1 or L2 losses. These architectures commonly employ RNN [31, 180], or Transformer [219] backbones to capture temporal dependencies via autoregressive regression. Nevertheless, their one-to-one mapping paradigm fails  to capture the inherent stochasticity of human motions, often leading to mean poses and lifeless motions.  3.3.3 Generative Adversarial Networks  Generative Adversarial Networks (GANs) [64] have been commonly used for human interaction motion generation [61, 145, 245]. The GAN architecture comprises two key components: a generator (G) and a discriminator (D). The generator (G) transforms random noise vectors sampled from a standard normal distribution (z ∼ N (0, I)) into interaction motions (G(z)). Meanwhile, the discriminator (D) evaluates the authenticity of the generated motions by learning to differentiate between real human motion samples (x) from the training distribution and synthetic samples produced by G. This adversarial dynamic is formalized through the following objective function:  mGin mDax  h  Ex∼pdata(x) log D(x) +  Ez∼pz(z) log 1 − D(G(z))  i  ,  (1)  where pdata(x) represents the distribution of real human motion data and pz(z) denotes the prior distribution of the noise vector z. The generator seeks to minimize this objective by producing motions that the discriminator cannot reliably distinguish from real data, while the discriminator aims to maximize its ability to correctly classify real and generated motions. Despite their impressive generative capabilities, GANs present training challenges [66]. The inherent instability of adversarial training manifests itself in several critical issues: mode collapse, where the generator converges to produce only a limited subset of possible motions, and convergence problems, where the generator-discriminator dynamics fails to reach a stable equilibrium.

### 3.3 交互动作生成的基础方法  
本节介绍用于人体交互动作生成的基础方法，涵盖从经典方法到最新的深度生成框架。  

#### 3.3.1 运动图  
**基于图的方法**[106] 是人体交互动作生成的基础性方法 [4, 87, 154, 253]，其利用动作数据的固有结构来合成新序列。这类方法通常采用**运动图**（一种有向图，其中节点表示动作片段或姿态，边表示片段间的可行过渡）。新动作的合成通过图遍历实现——沿连接节点的随机游走生成连贯的动作序列。该框架支持融合多个样本的特征组合，在保持源数据真实性的同时生成多样化动作。  
**局限性**：基于图的方法存在可扩展性瓶颈 [106]。由于需要存储完整数据集并在推理时执行图遍历，会引入较高的计算和存储开销。  

#### 3.3.2 确定性回归  
**确定性回归模型**[3, 12, 109] 将交互动作生成建模为一对一映射问题，通过 L1 或 L2 损失监督，从指定输入条件预测确定性的目标动作。这类架构通常采用 **RNN**[31, 180] 或 **Transformer**[219] 作为主干网络，通过自回归回归捕捉时序依赖性。  
**缺陷**：一对一映射范式无法捕捉人体动作固有的随机性，常导致生成平均化姿态和缺乏生气的动作。  

#### 3.3.3 生成对抗网络  
**生成对抗网络（GANs）**[64] 被广泛用于人体交互动作生成 [61, 145, 245]。GAN 架构包含两个核心组件：  
- **生成器（G）**：将标准正态分布的随机噪声向量（$$z \sim \mathcal{N}(0, I)$$）转换为交互动作（$$G(z)$$）。  
- **判别器（D）**：通过区分训练分布中的真实动作样本（$$x$$）与生成器合成的样本，评估生成动作的真实性。  

对抗过程通过以下目标函数形式化：  
$$
\min_G \max_D \left[ \mathbb{E}_{x \sim p_{data}(x)} \log D(x) + \mathbb{E}_{z \sim p_z(z)} \log \left(1 - D(G(z))\right) \right], \tag{1}
$$
其中 $$p_{data}(x)$$ 表示真实动作数据分布，$$p_z(z)$$ 为噪声向量 $$z$$ 的先验分布。生成器通过生成判别器难以区分的动作来最小化目标，而判别器则力求最大化对真伪动作的分类能力。  

**训练挑战**[66]：  
1. **模式坍塌**：生成器仅生成有限动作子集；  
2. **收敛问题**：生成器与判别器的动态平衡难以稳定达成。  

---

### 两句话总结  
**重点**：人体交互动作生成的基础方法包括**运动图**（组合现有片段）、**确定性回归**（一对一映射）和**GANs**（对抗式生成），但各自存在可扩展性、多样性不足或训练不稳定的问题。**GANs** 虽能生成逼真动作，却受限于模式坍塌和收敛困难等固有挑战。



3.3.4 Variational Autoencoders  Variational Autoencoders (VAEs) [103] employ a twostage architecture: first encoding input data into a structured latent space, then sampling from this learned distribution to reconstruct the original data. By maximizing the Evidence Lower Bound (ELBO), VAEs approximate the intractable log-likelihood of the data distribution. The ELBO for a VAE is expressed as:  Lθ,φ(x) = Ez∼qφ(z|x) [ln pθ(x|z)]−DKL (qφ(z|x) ∥ pθ(z)) , (2) where x represents the input data, z denotes the latent variables, θ and φ are the parameters of the decoder and encoder networks respectively, qφ(z|x) is the approximate posterior, and p(z) is the prior distribution over the latent variables. Conditional Variational Autoencoders (cVAEs) [193] extend the VAE framework by incorporating conditioning variables, enabling controlled data generation based on specific attributes. In human interaction motion synthesis, cVAEs have demonstrated versatility through various conditioning approaches: motion class labels [59, 72, 120, 138], textual descriptions [148, 249], target poses [130, 215], and other control signals. The ELBO formulation is accordingly modified to incorporate the conditioning variable c:  Lθ,φ(x|c) = Ez∼qφ(z|x,c) ln pθ(x|z, c) −  DKL qφ(z|x, c) pθ(z|c) , (3)  where c represents the conditioning information.  3.3.5 Diffusion Models  Diffusion models [83] have emerged as an expressive framework for generative modeling, demonstrating the capability to capture the complex data distribution in interactive human motions [23,33,52,60,93,185,233,255, 264]. The framework consists of two key processes: a forward diffusion process that systematically corrupts data with Gaussian noise across multiple timesteps until reaching a standard Gaussian distribution, and a learned reverse process that progressively denoises the corrupted data to reconstruct realistic human motions. The forward diffusion process is formally expressed as:  q(xt|xt−1) = N (xt; p1 − βtxt−1, βtI), (4)  where βt represents the variance schedule at timestep t, and N denotes a Gaussian distribution. The reverse denoising process is modeled as:  pθ(xt−1|xt) = N (xt−1; μθ(xt, t), Σθ(xt, t)), (5)  where μθ and Σθ are the mean and covariance predicted by the neural network parameterized by θ. In contrast to GANs’ single-step adversarial approach, the gradual, multi-step training dynamics of diffusion models provides inherent stability, enabling them to capture fine-grained motion details while maintaining diversity in their outputs.

### 3.3.4 变分自编码器  
**变分自编码器（VAEs）**[103] 采用两阶段架构：  
1. **编码阶段**：将输入数据映射到结构化的潜在空间；  
2. **解码阶段**：从学习到的分布中采样以重建原始数据。  

通过最大化**证据下界（ELBO）**，VAEs 近似数据分布的难解对数似然。其目标函数定义为：  
$$
\mathcal{L}_{\theta,\phi}(x) = \mathbb{E}_{z \sim q_\phi(z|x)} [\ln p_\theta(x|z)] - D_{KL} \left( q_\phi(z|x) \parallel p_\theta(z) \right), \tag{2}
$$
其中：  
- $$x$$ 为输入数据，$$z$$ 为潜变量；  
- $$\theta$$ 和 $$\phi$$ 分别表示解码器与编码器参数；  
- $$q_\phi(z|x)$$ 是近似后验分布，$$p(z)$$ 为先验分布。  

**条件变分自编码器（cVAEs）**[193] 通过引入条件变量 $$c$$ 扩展了 VAE 框架，支持基于特定属性的可控生成。在人体交互动作合成中，cVAEs 已通过多种条件形式展现灵活性：  
- 动作类别标签 [59, 72, 120, 138]  
- 文本描述 [148, 249]  
- 目标姿态 [130, 215]  
- 其他控制信号  

对应的 ELBO 目标函数修改为：  
$$
\mathcal{L}_{\theta,\phi}(x|c) = \mathbb{E}_{z \sim q_\phi(z|x,c)} \ln p_\theta(x|z, c) - D_{KL} \left( q_\phi(z|x, c) \parallel p_\theta(z|c) \right), \tag{3}
$$

---

### 3.3.5 扩散模型  
**扩散模型**[83] 已成为生成建模的强大框架，能够捕捉人体交互动作的复杂分布 [23,33,52,60,93,185,233,255,264]。其核心包含两个过程：  
1. **前向扩散过程**：通过多步高斯噪声逐步破坏数据，直至变为标准高斯分布：  
   $$
   q(x_t|x_{t-1}) = \mathcal{N} \left( x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I} \right), \tag{4}
   $$
   其中 $$\beta_t$$ 为第 $$t$$ 步的噪声方差。  

2. **反向去噪过程**：通过神经网络学习逐步去噪以重建真实动作：  
   $$
   p_\theta(x_{t-1}|x_t) = \mathcal{N} \left( x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t) \right), \tag{5}
   $$
   其中 $$\mu_\theta$$ 和 $$\Sigma_\theta$$ 为神经网络预测的均值与协方差。  

**优势**：相比 GAN 的单步对抗训练，扩散模型的**渐进式多步优化**带来固有稳定性，既能捕捉细粒度动作细节，又能保持输出多样性。  

---

### 两句话总结  
**重点**：**VAEs/cVAEs** 通过结构化潜在空间实现可控生成，但依赖近似推断可能限制生成质量；**扩散模型**凭借多步噪声-去噪机制，在稳定训练的同时实现了高保真动作合成，成为当前最具潜力的生成范式之一。





3.3.6 Transformer-Based Language Models  Transformers [219] leverage self-attention mechanisms to efficiently capture long-range dependencies within  data sequences. Introduced by Vaswani et al. [219], the core self-attention equation is defined as:  Attention(Q, K, V ) = softmax QK⊤  √dk  V, (6)  where Q, K, and V are the query, key, and value matrices derived from input embeddings, and dk is the dimensionality of the key vectors. This mechanism allows the model to dynamically focus on relevant parts of the input sequence when generating each output element. Transformer-based language models, including GPTs [2, 18] and generative masked transformers [24], also become a promising paradigm for interaction motion modeling [91, 191]. These approaches typically implement a three-phase architecture: first, discretizing continuous motion data into tokens using encoders, like Vector Quantized Variational Autoencoders (VQ-VAEs) [217], which preserve essential motion structure and dynamics; second, modeling the sequential relationships between these tokens using transformer-based language models; and finally, projecting the tokenized representations back into continuous 3D motion sequences through a VQ-VAE decoder.  3.3.7 RL + Physics Simulation  Reinforcement Learning (RL) combined with physics simulation aims to generate more physically plausible interactive human motions [34, 80, 160, 229, 240, 258]. This approach leverages RL’s ability to learn optimal policies through trial-and-error while utilizing physics simulators to ensure that the generated motions adhere to fundamental physical laws. In this framework, an RL agent interacts with a physics-based environment, guided by rewards that promote target behaviors while accounting for constraints such as balance and collisions with objects or scenes. The design of reward functions plays a critical role in these approaches. On the one hand, RL-based methods often face training convergence challenges and exhibit limited generalization to novel actions. On the other hand, physics-based simulation remains essential as human motions inherently follow physical constraints in the real world—a fundamental aspect that purely kinematic-based methods struggle to capture.  3.3.8 LLM-Based Motion Planning  Recent advances in Large Language Models (LLMs) [150] have enabled their application as automated motion planners [32, 240, 248], translating high-level interaction goals into detailed step-by-step motion sequences. These approaches innovate by generating interactions without relying on extensive interaction datasets, instead leveraging the knowledge of human kinematics embedded in pre-trained LLMs through carefully designed prompts. LLMs can provide temporal specifications of interactions, identify relevant joint involvement, and describe precise interaction dynamics.  

### 3.3.6 基于Transformer的语言模型  
**Transformer**[219] 利用自注意力机制高效捕捉数据序列中的长程依赖关系。其核心自注意力公式定义为：  
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V, \tag{6}
$$
其中 $$Q$$、$$K$$、$$V$$ 为输入嵌入衍生的查询、键、值矩阵，$$d_k$$ 为键向量的维度。该机制使模型在生成每个输出元素时能动态聚焦于输入序列的相关部分。  

基于Transformer的语言模型（如 **GPTs**[2,18] 和生成式掩码Transformer[24]）已成为交互动作建模的新范式[91,191]。典型流程包含三阶段：  
1. **离散化编码**：使用**向量量化变分自编码器（VQ-VAEs）**[217] 将连续动作数据转化为保留运动结构与动态特征的token；  
2. **序列建模**：通过Transformer语言模型学习token间的时序关系；  
3. **连续重建**：利用VQ-VAE解码器将token化表示映射回连续的3D动作序列。  

---

### 3.3.7 强化学习与物理仿真结合  
**强化学习（RL）+物理仿真**的方法旨在生成更符合物理规律的交互动作[34,80,160,229,240,258]。其核心优势在于：  
- **RL的试错学习**：通过奖励机制优化策略，驱动智能体在物理仿真环境中实现目标行为；  
- **物理约束保障**：仿真器强制满足平衡性、物体碰撞等现实约束。  

**关键设计**：奖励函数需同时考虑动作目标（如握手力度）与物理约束（如避免跌倒）。但这类方法存在**训练收敛困难**和**新动作泛化性有限**的问题。值得注意的是，物理仿真对动作生成至关重要——纯运动学方法难以建模现实世界中人体运动固有的物理规律。  

---

### 3.3.8 基于大语言模型的运动规划  
**大语言模型（LLMs）**[150] 的最新进展使其能作为自动化运动规划器[32,240,248]，将高层交互目标转化为详细的动作序列。其创新性体现在：  
- **零样本生成**：无需依赖大规模交互数据集，通过精心设计的提示词（prompts）激活预训练LLM中嵌入的人类运动学知识；  
- **多维度规划**：LLMs可生成交互的时间规范、关节参与方案及精确的动力学描述。  

例如，输入"生成一个从坐姿起身并开门的三步动作"，LLM能输出包含重心转移、关节协调等细节的时序指令。  

---

### 两句话总结  
**重点**：**Transformer语言模型**通过token化-序列建模-重建的三阶段流程实现动作生成，而**RL+物理仿真**在奖励函数驱动下强制物理合理性；**LLM运动规划**的创新在于利用语言模型的零样本能力，将高层指令直接解码为可执行动作序列，开辟了无需专用训练数据的新路径。





4.3 Human-Scene Interaction Generation  In everyday life, humans effortlessly navigate and interact with complex environments. Recreating this natural ability is essential for 3D applications such as gaming, simulation, and character animation in virtual environments. Human-scene interaction (HSI) entails modeling how humans move and interact with their surroundings while adhering to a set of physical rules, such as collision avoidance and aligning with semantic or contextual constraints. Additionally, supplementary signals—such as text prompts, action labels, and target goals—further enhance the control and precision of motion generation in these systems. Existing work attempts to achieve scene-aware motion generation through these three pillars: motion generation systems, environment constraints, and contextual interaction understanding, as illustrated in Fig. 7. Motion generation system decomposes the complex sceneaware motion synthesis into modular and more tractable subtasks, a common strategy in recent researches. Environment constraints ensure physical plausibility by maintaining coherence between generated motions and spatial constraints. These constraints align human poses and movements with scene geometry and surfaces, preserving natural motion dynamics and physical consistency throughout the interaction sequence. Contextual interaction understanding enhances system capabilities through semantic comprehension of environmental context, enabling meaningful and nuanced interactions. This encompasses graph-based modeling of object relationships for joint-level interactions, high-level planning through LLMs, and interpretation of scene affordances via imagebased cues. These three aspects are fundamental to generating lifelike, physically coherent, and semantically appropriate human motions in 3D scenes, forming the foundation for contemporary advances in scene-aware motion generation. We summarize the characteristics of human-scene interaction generation methods in Table 3. The following sections explore recent advancements in addressing these challenges within scene-aware motion generation.



### 4.3 人-场景交互生成  
在日常生活中，人类能够轻松地在复杂环境中导航和交互。在游戏、仿真和虚拟角色动画等3D应用中，重现这种自然能力至关重要。**人-场景交互（HSI）**需要建模人类如何在遵守物理规则（如避免碰撞、符合语义或上下文约束）的同时与周围环境进行移动和互动。此外，**文本提示**、**动作标签**和**目标意图**等辅助信号可进一步提升这些系统中运动生成的控制力和精确度。  

如**图7**所示，现有研究通过三大支柱实现场景感知的运动生成：  
1. **运动生成系统**  
   - 将复杂的场景感知运动合成任务分解为模块化的子任务（这是近期研究的常用策略）  
   - 例如：==先规划路径再生成细节动作==

2. **环境约束**  
   - 通过保持生成动作与空间约束的一致性来确保**物理合理性**  
   - 将人体姿态和运动对齐到场景几何体与表面  
   - 在整个交互序列中维持自然运动动力学和物理连贯性  

3. **上下文交互理解**  
   - 通过对环境上下文的**语义理解**增强系统能力  
   - 具体技术包括：  
     - 基于图的物体关系建模（用于关节级交互）  
     - 通过LLM进行高层规划  
     - 基于图像线索解读场景功能（如"椅子可坐"）  

这三个方面是生成逼真、物理连贯且语义合理的3D场景人体运动的基础，构成了当代场景感知运动生成进展的核心。我们在**表3**中总结了人-场景交互生成方法的特征。下文将探讨该领域应对这些挑战的最新进展。  

---

### 两句话总结  
**重点**：**人-场景交互生成**依赖运动系统模块化、环境物理约束和上下文语义理解三大支柱，其中**LLM高层规划**与**场景功能解读**使交互既符合物理规律又具有语义合理性；当前研究通过分解复杂任务（如先导航后交互）逐步逼近人类在真实环境中的自然行为能力。



继续翻译论文《A Survey on Human Interaction Motion Generation》的内容，请翻译成中文，并给出两句话总结，重点的标注**重点**，如果是1 使用##，如果是 3.1使用###，公式都弄好,公式前后加上$$，并帮我可读性写高一点（同时不可以缺少语句，原文该有的都有，可以增加你的理解，让翻译得容易读懂）,我要复制进typora中：