# 动作生成相关 论文总结 摘要等











<details open="" style="box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;"><summary style="box-sizing: border-box; display: list-item; cursor: pointer;"><div class="markdown-heading" dir="auto" style="box-sizing: border-box; position: relative; display: inline-block;"><h2 tabindex="-1" class="heading-element" dir="auto" style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.5em; font-weight: 600; line-height: 1.25; padding-bottom: 0px; border-bottom: 0px; display: inline-block;">Motion Generation, Text/Speech/Music-Driven</h2><a id="user-content-motion-generation-textspeechmusic-driven" class="anchor" aria-label="Permalink: Motion Generation, Text/Speech/Music-Driven" href="https://github.com/Foruck/Awesome-Human-Motion?tab=readme-ov-file#motion-generation-textspeechmusic-driven" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; float: left; padding-right: 4px; margin: auto; line-height: 1; position: absolute; top: 35px; left: -28px; display: flex; width: 28px; height: 28px; border-radius: 6px; opacity: 0; justify-content: center; align-items: center; transform: translateY(calc(-50% - 0.3rem)); text-underline-offset: 0.2rem;"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div></summary><slot id="details-content" pseudo="details-content" style="display: block;"><ul dir="auto" style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 16px;"><details open="" style="box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;"><summary style="box-sizing: border-box; display: list-item; cursor: pointer;"><div class="markdown-heading" dir="auto" style="box-sizing: border-box; position: relative; display: inline-block;"><h3 tabindex="-1" class="heading-element" dir="auto" style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: 600; line-height: 1.25; display: inline-block;">2025</h3><a id="user-content-2025" class="anchor" aria-label="Permalink: 2025" href="https://github.com/Foruck/Awesome-Human-Motion?tab=readme-ov-file#2025" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; float: left; padding-right: 4px; margin: auto; line-height: 1; position: absolute; top: 32.5px; left: -28px; display: flex; width: 28px; height: 28px; border-radius: 6px; opacity: 0; justify-content: center; align-items: center; transform: translateY(-50%); text-underline-offset: 0.2rem;"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div></summary><ul dir="auto" style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 0px;"><li style="box-sizing: border-box;"><b style="box-sizing: border-box; font-weight: 600;">(TOG 2025)</b><span>&nbsp;</span><a href="https://zhongleilz.github.io/Sketch2Anim/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Sketch2Anim</a>: Towards Transferring Sketch Storyboards into 3D Animation, Zhong et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2505.00998" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">DSDFM</a>: Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis, Hua et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/abs/2504.05265" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">From Sparse Signal to Smooth Motion</a>: Real-Time Motion Generation with Rolling Prediction Models, Barquero et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://shape-move.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Shape My Moves</a>: Text-Driven Shape-Aware Synthesis of Human Motions, Liao et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://github.com/CVI-SZU/MG-MotionLLM" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MG-MotionLLM</a>: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities, Wu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://seokhyeonhong.github.io/projects/salad/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">SALAD</a>: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing, Hong et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://boeun-kim.github.io/page-PersonaBooth/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">PersonalBooth</a>: Personalized Text-to-Motion Generation, Kim et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/abs/2411.16575" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MARDM</a>: Rethinking Diffusion for Text-Driven Human Motion Generation, Meng et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.04829" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">StickMotion</a>: Generating 3D Human Motions by Drawing a Stickman, Wang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/abs/2411.16805" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">LLaMo</a>: Human Motion Instruction Tuning, Li et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://star-uu-wang.github.io/HOP/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">HOP</a>: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation, Cheng et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://atom-motion.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">AtoM</a>: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward, Han et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://jiro-zhang.github.io/EnergyMoGen/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">EnergyMoGen</a>: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space, Zhang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://languageofmotion.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">The Languate of Motion</a>: Unifying Verbal and Non-verbal Language of 3D Human Motion, Chen et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://shunlinlu.github.io/ScaMo/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ScaMo</a>: Exploring the Scaling Law in Autoregressive Motion Generation Model, Lu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://hhsinping.github.io/Move-in-2D/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Move in 2D</a>: 2D-Conditioned Human Motion Generation, Huang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://solami-ai.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">SOLAMI</a>: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters, Jiang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025)</b><span>&nbsp;</span><a href="https://lijiaman.github.io/projects/mvlift/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MVLift</a>: Lifting Motion to the 3D World via 2D Diffusion, Li et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(CVPR 2025 Workshop)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2505.09827" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Dyadic Mamba</a>: Long-term Dyadic Human Motion Synthesis, Tanke et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ACM Sensys 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.01768" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">SHADE-AD</a>: An LLM-Based Framework for Synthesizing Activity Data of Alzheimer’s Patients, Fu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICRA 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/abs/2410.16623" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MotionGlot</a>: A Multi-Embodied Motion Generation Model, Harithas et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://guytevet.github.io/CLoSD-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">CLoSD</a>: Closing the Loop between Simulation and Diffusion for multi-task character control, Tevet et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://genforce.github.io/PedGen/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">PedGen</a>: Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels, Liu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://openreview.net/forum?id=IEul1M5pyk" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">HGM³</a>: Hierarchical Generative Masked Motion Modeling with Hard Token Mining, Jeong et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://openreview.net/forum?id=LYawG8YkPa" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">LaMP</a>: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning, Li et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://openreview.net/forum?id=d23EVDRJ6g" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MotionDreamer</a>: One-to-Many Motion Synthesis with Localized Generative Masked Transformer, Wang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://openreview.net/forum?id=Oh8MuCacJW" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Lyu et al</a>: Towards Unified Human Motion-Language Understanding via Sparse Interpretable Characterization, Lyu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://zkf1997.github.io/DART/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">DART</a>: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control, Zhao et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ICLR 2025)</b><span>&nbsp;</span><a href="https://knoxzhao.github.io/Motion-Agent/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Motion-Agent</a>: A Conversational Framework for Human Motion Generation with LLMs, Wu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(IJCV 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2502.05534" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Fg-T2M++</a>: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation, Wang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2505.08293" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">M3G</a>: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis, Yin et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2505.05589" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ReactDance</a>: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation, Lin et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://wengwanjiang.github.io/ReAlign-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ReAlign</a>: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment, Weng et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://research.nvidia.com/labs/dair/genmo/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">GENMO</a>: A GENeralist Model for Human MOtion, Li et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2504.16722" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ProMoGen</a>: PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning, Xi et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://xiangyuezhang.com/SemTalk/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">SemTalk</a>: Holistic Co-speech Motion Generation with Frame-level Semantic Emphasis, Zhang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://foram-s1.github.io/DanceMosaic/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">DanceMosaic</a>: High-Fidelity Dance Generation with Multimodal Editability, Shah et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://yong-xie-xy.github.io/ReCoM/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ReCoM</a>: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer, Xie et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">PRIMAL</a>: Physically Reactive and Interactive Motor Model for Avatar Learning, Zhang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://www.pinlab.org/hmu" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">HMU</a>: Human Motion Unlearning, Matteis et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="http://inwoohwang.me/SFControl" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">SFControl</a>: Motion Synthesis with Sparse and Flexible Keyjoint Control, Hwang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.14919" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">GenM3</a>: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation, Shi et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://zju3dv.github.io/MotionStreamer/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MotionStreamer</a>: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space, Xiao et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.13859" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Less Is More</a>: Improving Motion Diffusion Models with Sparse Keyframes, Bae et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.13300" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Zeng et al</a>: Progressive Human Motion Generation Based on Text and Few Motion Frames, Zeng et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://mjwei3d.github.io/ACMo/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ACMo</a>: Attribute Controllable Motion Generation, Wei et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://jackyu6.github.io/HERO/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">HERO</a>: Human Reaction Generation from Videos, Yu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.06151" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">BioMoDiffuse</a>: Physics-Guided Biomechanical Diffusion for Controllable and Authentic Human Motion Synthesis, Kang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.06499" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ExGes</a>: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis, Zhou et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2502.17327" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">AnyTop</a>: Character Animation Diffusion with Any Topology, Gat et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span>Motion Anything: Any to Motion Generation, Zhang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2502.18309" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">GCDance</a>: Genre-Controlled 3D Full Body Dance Generation Driven By Music, Liu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://diouo.github.io/motionlab.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MotionLab</a>: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm, Guo et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://cjerry1243.github.io/casim_t2m/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">CASIM</a>: Composite Aware Semantic Injection for Text to Motion Generation, Chang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2501.19083" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MotionPCM</a>: Real-Time Motion Synthesis with Phased Consistency Model, Jiang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://andypinxinliu.github.io/GestureLSM/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">GestureLSM</a>: Latent Shortcut based Co-Speech Gesture Generation with Spatial-Temporal Modeling, Liu et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2501.18232" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Free-T2M</a>: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss, Chen et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2501.01449" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">LS-GAN</a>: Human Motion Synthesis with Latent-space GANs, Amballa et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/html/2501.16778v1" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">FlexMotion</a>: Lightweight, Physics-Aware, and Controllable Human Motion Generation, Tashakori et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2503.06897" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">HiSTF Mamba</a>: Hierarchical Spatiotemporal Fusion with Multi-Granular Body-Spatial Modeling for High-Fidelity Text-to-Motion Generation, Zhan et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(ArXiv 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2501.16551" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">PackDiT</a>: Joint Human Motion and Text Generation via Mutual Prompting, Jiang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(3DV 2025)</b><span>&nbsp;</span><a href="https://coral79.github.io/uni-motion/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Unimotion</a>: Unifying 3D Human Motion Synthesis and Understanding, Li et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(3DV 2025)</b><span>&nbsp;</span><a href="https://cyk990422.github.io/HoloGest.github.io//" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">HoloGest</a>: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures, Cheng et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(AAAI 2025)</b><span>&nbsp;</span><a href="https://hanyangclarence.github.io/unimumo_demo/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">UniMuMo</a>: Unified Text, Music and Motion Generation, Yang et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(AAAI 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/abs/2408.00352" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ALERT-Motion</a>: Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion, Miao et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(AAAI 2025)</b><span>&nbsp;</span><a href="https://cure-lab.github.io/MotionCraft/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MotionCraft</a>: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls, Bian et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(AAAI 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/pdf/2412.11193" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Light-T2M</a>: A Lightweight and Fast Model for Text-to-Motion Generation, Zeng et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(WACV 2025)</b><span>&nbsp;</span><a href="https://reindiffuse.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">ReinDiffuse</a>: Crafting Physically Plausible Motions with Reinforced Diffusion Model, Han et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(WACV 2025)</b><span>&nbsp;</span><a href="https://motion-rag.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">MoRAG</a>: Multi-Fusion Retrieval Augmented Generation for Human Motion, Shashank et al.</li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;">(WACV 2025)</b><span>&nbsp;</span><a href="https://arxiv.org/abs/2409.11920" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;">Mandelli et al</a>: Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models, Mandelli et al.</li></ul></details><details style="box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;"><summary style="box-sizing: border-box; display: list-item; cursor: pointer;"><div class="markdown-heading" dir="auto" style="box-sizing: border-box; position: relative; display: inline-block;"><h3 tabindex="-1" class="heading-element" dir="auto" style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: 600; line-height: 1.25; display: inline-block;">2024</h3><a id="user-content-2024" class="anchor" aria-label="Permalink: 2024" href="https://github.com/Foruck/Awesome-Human-Motion?tab=readme-ov-file#2024" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; float: left; padding-right: 4px; margin: auto; line-height: 1; position: absolute; top: 32.5px; left: -28px; display: flex; width: 28px; height: 28px; border-radius: 6px; opacity: 0; justify-content: center; align-items: center; transform: translateY(-50%); text-underline-offset: 0.2rem;"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div></summary><ul dir="auto" style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 0px;"><li style="box-sizing: border-box;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://xiangyue-zhang.github.io/SemTalk" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://inter-dance.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://zju3dv.github.io/Motion-2-to-3/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2412.07797" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://gabrie-l.github.io/coma-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://sopo-motion.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/pdf/2412.04343" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/pdf/2412.00112" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://whwjdqls.github.io/discord.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2411.19786" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2411.18303" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2411.17532" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://andypinxinliu.github.io/KinMo/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2411.14951" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://steve-zeyu-zhang.github.io/KMM" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2410.21747" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://li-ronghui.github.io/lodgepp" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2410.18977" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2410.14508" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2410.08931" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2410.06513" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://lhchen.top/MotionLLM/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2410.03311" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2409.13251" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/RohollahHS/BAD" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://von31.github.io/synNsync/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://aclanthology.org/2024.findings-emnlp.584/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://nips.cc/virtual/2024/poster/97700" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/xiyuanzh/UniMTS" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openreview.net/forum?id=FsdB3I9Y24" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://momu-diffusion.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://aigc3d.github.io/mogents/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2405.16273" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openreview.net/forum?id=BTSnh5YdeI" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/pdf/2502.20176" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://link.springer.com/chapter/10.1007/978-3-031-78104-9_30" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://bohongchen.github.io/SynTalker-Page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681487" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681657" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681034" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://link.springer.com/chapter/10.1007/978-3-031-72356-8_2" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://journals.sagepub.com/doi/full/10.1177/10711813241262026" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://jpthu17.github.io/GuidedMotion-project/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/100_ECCV_2024_paper.php" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://frank-zy-dou.github.io/projects/EMDM/index.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://yh2371.github.io/como/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/jsun57/CoMusion" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2405.18483" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/qrzou/ParCo" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2407.11532" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/line/ChronAccRet" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://idigitopia.github.io/projects/mhc/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/moonsliu/Pro-Motion" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2406.10740" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://eccv.ecva.net/virtual/2024/poster/266" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://vankouf.github.io/FreeMotion/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://foruck.github.io/KP/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2404.01700" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://neu-vi.github.io/SMooDi/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://exitudio.github.io/BAMM-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dai-wenxun.github.io/MotionLCM-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2312.10993" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2407.14502" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mingyuan-zhang.github.io/projects/LMM.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://research.nvidia.com/labs/toronto-ai/tesmo/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://tlcontrol.weilinwl.com/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://ieeexplore.ieee.org/abstract/document/10687922" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://ieeexplore.ieee.org/abstract/document/10645445" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/LinghaoChan/HumanTOMATO" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://sites.google.com/view/gphlvm/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://setarehc.github.io/CondMDI/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://aiganimation.github.io/CAMDM/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://vcc.tech/research/2024/LGTM" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://threedle.github.io/TEDi/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/Yi-Shi94/AMDM" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dl.acm.org/doi/10.1145/3658209" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2407.10481" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://hanchaoliu.github.io/Prog-MoGen/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/IDC-Flash/PacerPlus" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://amuse.is.tue.mpg.de/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://feifeifeiliu.github.io/probtalk/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://guytevet.github.io/mas-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://wandr.is.tue.mpg.de/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://ericguo5513.github.io/momask/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://yfeng95.github.io/ChatPose/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://zixiangzhou916.github.io/AvatarGPT/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://exitudio.github.io/MMM-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_AAMDM_Accelerated_Auto-regressive_Motion_Diffusion_Model_CVPR_2024_paper.pdf" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://tr3e.github.io/omg-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://barquerogerman.github.io/FlowMDM/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://digital-life-project.com/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://pantomatrix.github.io/EMAGE/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://xbpeng.github.io/projects/STMC/index.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/THU-LYJ-Lab/InstructMotion" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://sinmdm.github.io/SinMDM-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openreview.net/forum?id=sOJriBlOFd&amp;noteId=KaJUBoveeo" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://priormdm.github.io/priorMDM-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://neu-vi.github.io/omnicontrol/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openreview.net/forum?id=yQDFsuG9HP" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://lisiyao21.github.io/projects/Duolando/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2312.12227" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2312.12763" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://nhathoang2002.github.io/MotionMix-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/xiezhy6/B2A-HDM" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://ojs.aaai.org/index.php/AAAI/article/view/27936" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://qiqiapink.github.io/MotionGPT/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2305.13773" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://evm7.github.io/UNIMASKM-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2312.10960" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://ieeexplore.ieee.org/abstract/document/10399852" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/pdf/2312.12917" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li></ul></details><details style="box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;"><summary style="box-sizing: border-box; display: list-item; cursor: pointer;"><div class="markdown-heading" dir="auto" style="box-sizing: border-box; position: relative; display: inline-block;"><h3 tabindex="-1" class="heading-element" dir="auto" style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: 600; line-height: 1.25; display: inline-block;">2023</h3><a id="user-content-2023" class="anchor" aria-label="Permalink: 2023" href="https://github.com/Foruck/Awesome-Human-Motion?tab=readme-ov-file#2023" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; float: left; padding-right: 4px; margin: auto; line-height: 1; position: absolute; top: 32.5px; left: -28px; display: flex; width: 28px; height: 28px; border-radius: 6px; opacity: 0; justify-content: center; align-items: center; transform: translateY(-50%); text-underline-offset: 0.2rem;"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div></summary><ul dir="auto" style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 0px;"><li style="box-sizing: border-box;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/jpthu17/GraphMotion" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://motion-gpt.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mingyuan-zhang.github.io/projects/FineMoGen.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://jiawei-ren.github.io/projects/insactor/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/ZcyMonkey/AttT2M" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mathis.petrovich.fr/tmr" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://azadis.github.io/make-an-animation" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://nvlabs.github.io/PhysDiff" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://barquerogerman.github.io/BeLFusion/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://korrawe.github.io/gmd-project/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Aliakbarian_HMD-NeMo_Online_3D_Avatar_Motion_Generation_From_Sparse_Observations_ICCV_2023_paper.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://sinc.is.tue.mpg.de/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Priority-Centric_Human_Motion_Generation_in_Discrete_Latent_Space_ICCV_2023_paper.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Fg-T2M_Fine-Grained_Text-Driven_Human_Motion_Generation_via_Diffusion_Model_ICCV_2023_paper.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Breaking_The_Limits_of_Text-conditioned_3D_Motion_Synthesis_with_Elaborative_ICCV_2023_paper.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://weiyuli.xyz/GenMM/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://i.cs.hku.hk/~taku/kunkun2023.pdf" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://www.speech.kth.se/research/listen-denoise-action/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dulucas.github.io/agrol/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://talkshow.is.tue.mpg.de/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mael-zys.github.io/T2M-GPT/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://zixiangzhou916.github.io/UDE/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/junfanlin/oohmg" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://edge-dance.github.io/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://chenxin.tech/mld" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://sigal-raab.github.io/MoDi" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://vcai.mpi-inf.mpg.de/projects/MoFusion/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://arxiv.org/abs/2303.14926" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://guytevet.github.io/mdm-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mingyuan-zhang.github.io/projects/MotionDiffuse.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://www.mmlab-ntu.com/project/bailando/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://zixiangzhou916.github.io/UDE-2/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://pjyazdian.github.io/MotionScript/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li></ul></details><details style="box-sizing: border-box; display: block; margin-top: 0px; margin-bottom: 16px;"><summary style="box-sizing: border-box; display: list-item; cursor: pointer;"><div class="markdown-heading" dir="auto" style="box-sizing: border-box; position: relative; display: inline-block;"><h3 tabindex="-1" class="heading-element" dir="auto" style="box-sizing: border-box; margin-top: 24px; margin-bottom: 16px; font-size: 1.25em; font-weight: 600; line-height: 1.25; display: inline-block;">2022 and earlier</h3><a id="user-content-2022-and-earlier" class="anchor" aria-label="Permalink: 2022 and earlier" href="https://github.com/Foruck/Awesome-Human-Motion?tab=readme-ov-file#2022-and-earlier" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; float: left; padding-right: 4px; margin: auto; line-height: 1; position: absolute; top: 32.5px; left: -28px; display: flex; width: 28px; height: 28px; border-radius: 6px; opacity: 0; justify-content: center; align-items: center; transform: translateY(-50%); text-underline-offset: 0.2rem;"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div></summary><ul dir="auto" style="box-sizing: border-box; padding-left: 2em; margin-top: 0px; margin-bottom: 0px;"><li style="box-sizing: border-box;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/c-he/NeMF" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/nv-tlabs/PADL" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://pku-mocca.github.io/Rhythmic-Gesticulator-Page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://teach.is.tue.mpg.de/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://github.com/PACerv/ImplicitMotion" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810707.pdf" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://guytevet.github.io/motionclip-page/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://europe.naverlabs.com/research/computer-vision/posegpt" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mathis.petrovich.fr/temos/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://ericguo5513.github.io/TM2T/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://hongfz16.github.io/projects/AvatarCLIP.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dl.acm.org/doi/10.1145/3528223.3530178" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://ericguo5513.github.io/text-to-motion" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://www.mmlab-ntu.com/project/bailando/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://mathis.petrovich.fr/actor/index.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://google.github.io/aichoreographer/" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://dl.acm.org/doi/10.1145/3450626.3459881" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://yz-cnsdqz.github.io/eigenmotion/MOJO/index.html" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://www.ye-yuan.com/dlow" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li><li style="box-sizing: border-box; margin-top: 0.25em;"><b style="box-sizing: border-box; font-weight: 600;"></b><a href="https://www.ipab.inf.ed.ac.uk/cgvu/basketball.pdf" rel="nofollow" style="box-sizing: border-box; background-color: transparent; color: rgb(9, 105, 218); text-decoration: underline; text-underline-offset: 0.2rem;"></a></li></ul></details></ul></slot></details>



摘要翻译



核心突出点



帮我进行题目，摘要的翻译，并写出核心突出点。翻译完给我两句总结

### 1.1、Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation

**Sketch2Anim：将故事板草图转化为3D动画**

https://zhongleilz.github.io/Sketch2Anim/

![image-20250518102613236](assets/image-20250518102613236.png)

Abstract

Storyboarding is a common practice in 3D animation, where animators use 2D sketches as references to manually create motions—a process that is both time-consuming and requires significant expertise. To automate this workflow, we introduce Sketch2Anim, a novel framework that translates 2D storyboard sketches into 3D animations through conditional motion synthesis. Sketch2Anim features a multi-conditional motion generator, leveraging 3D keyposes, joint trajectories, and action words for precise motion control. To bridge the 2D-3D domain gap, we also propose a neural mapper that aligns 2D sketches with 3D motion constraints in a shared embedding space, enabling direct 2D-to-3D animation control. Our method produces high-quality 3D motions and naturally supports interactive editing. Extensive experiments and user studies confirm the effectiveness of our approach.

**摘要**
故事板是3D动画制作的常见流程，动画师需以2D草图为参考手动创建动作——这一过程既耗时又依赖专业经验。为实现该流程的自动化，我们提出了Sketch2Anim框架，通过条件动作合成将2D故事板草图转化为3D动画。Sketch2Anim的核心是多条件动作生成器，结合3D关键姿势、关节轨迹和动作关键词实现精准动作控制。为弥合2D与3D的领域差异，我们还提出了一种神经映射器，将2D草图与3D动作约束对齐到共享嵌入空间，从而直接实现2D到3D的动画控制。本方法能生成高质量的3D动作，并天然支持交互式编辑。大量实验和用户研究验证了其有效性。

两句总结

1. **技术贡献**：Sketch2Anim通过多条件生成器和跨域神经映射，首次实现从2D草图到3D动画的端到端自动化合成。
2. **应用价值**：该方法不仅提升了动画制作效率，其交互编辑功能也为艺术创作提供了灵活支持。



方法：

![img](assets/overview.png)

**方法**
我们的流程包含两个核心模块——**多条件动作生成器**和**2D-3D神经映射器**。与直接将2D关键姿势和轨迹提升至3D不同，我们训练了一个专用于在嵌入空间中对齐2D与3D领域的神经映射器。得益于这一共享嵌入空间，我们能够在动作生成器中利用更丰富、更精确的3D关键姿势和轨迹作为动作条件，同时在推理阶段复用从故事板草图检测到的2D关键姿势和轨迹。

两句总结

1. **技术突破**：通过嵌入空间对齐策略，解决了2D草图与3D动作的异构数据兼容性问题。
2. **效率优势**：推理时仅依赖2D输入即可调用预训练的3D动作先验，兼顾生成质量与计算效率。





### 1.2、Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis  人体动作合成中的确定性-随机性多样化隐特征映射

#### 1、链接：https://arxiv.org/pdf/2505.00998



是否开源：没看到 似乎都没主页

#### 2、图示：

![image-20250518110247782](assets/image-20250518110247782.png)

#### 3、摘要：

Abstract Human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. Recent score-based generative models (SGMs) have demonstrated impressive results on this task. However, their training process involves complex curvature trajectories, leading to unstable training process. In this paper, we propose a Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. DSDFM consists of two stages. The first human motion reconstruction stage aims to learn the latent space distribution of human motions. The second diverse motion generation stage aims to build connections between the Gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. This stage is achieved by the designed deterministic feature mapping procedure with DerODE and stochastic diverse output generation procedure with DivSDE. DSDFM is easy to train compared to previous SGMs-based methods and can enhance diversity without introducing additional training parameters. Through qualitative and quantitative experiments, DSDFM achieves stateof-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.

**摘要**
人体动作合成旨在生成逼真的人体动作序列，这一任务在计算机动画领域广受关注。基于分数的生成模型（SGMs）近期在该任务中展现了出色效果，但其训练过程涉及复杂的曲率轨迹，导致训练不稳定。本文提出**确定性-随机性多样化隐特征映射（DSDFM）**方法，包含两阶段：

1. **动作重建阶段**：学习人体动作的隐空间分布；
2. **多样化生成阶段**：通过设计的**确定性特征映射（DerODE）\**和\**随机多样化输出生成（DivSDE）**，建立高斯分布与动作隐空间的连接，在无需新增训练参数的情况下提升生成动作的多样性与准确性。相比传统SGMs，DSDFM训练更稳定且效果更优。定性与定量实验表明，DSDFM超越现有方法达到SOTA水平。

#### 4、方法：



### 1.3、(CVPR 2025) MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities 多粒度运动理解与生成的统一框架

#### 1、链接：



是否开源：否

#### 2、图示：



#### 3、摘要：

**框架描述**
**MG-MotionLLM** 通过统一指令处理多粒度运动相关任务：

- **粗粒度**：如文本生成动作、动作描述（上图模块）
- **细粒度**：如动作生成细节文本、动作定位（下图模块）

**MG-MotionLLM** can address diverse motion-relevant tasks at multiple granularities by giving different instructions in a unified manner.

- **coarse-grained**: e.g. text-to-motion and motion captioning (upper block)
- **fine-grained**: e.g. motion-to-detailed text and motion localization (bottom block).

![image-20250518110828575](assets/image-20250518110828575.png)

To achieve this, we propose multi-granularity training scheme with novel auxiliary tasks captures motion-related features at different levels, improving understanding across a wide range of tasks. Specifically, we pretrain the model with a total of **28** distinct motion-relevant tasks, including **12** existing classical **coarse-grained** tasks and **16** newly proposed **fine-grained** ones. Here, we display examples of prompt templates for a part of tasks used during training.

为实现这一目标，我们提出**多粒度训练方案**，通过新型辅助任务捕捉不同层级的运动特征，提升跨任务理解能力。具体而言，我们使用总计 **28** 种运动相关任务预训练模型，包括 **12** 个经典**粗粒度**任务和 **16** 个新提出的**细粒度**任务。下图展示了部分训练任务的提示模板示例。

[![tasks_template](https://github.com/CVI-SZU/MG-MotionLLM/raw/main/assets/tasks_template.png)](https://github.com/CVI-SZU/MG-MotionLLM/blob/main/assets/tasks_template.png)

## Visualization

#### 4、方法：





### 1.4、Shape My Moves Text-Driven Shape-Aware Synthesis of Human Motions  Shape My Moves：文本驱动的形状感知人体动作合成

#### 1、链接：



是否开源：否

#### 2、图示：



![image-20250518105341238](assets/image-20250518105341238.png)

#### 3、摘要：

Motion Synthesis on Various Body Shapes

We showcase results of our model **ShapeMove** when it is applied to synthesize the same actions for different body shapes. In the following videos, we use the same motion description with different shape descriptions to demonstrate how our method captures the natural variations in performing the same action by the different body types. The first row of video illustrates four sample body types. The Action buttons allow choosing and displaying different motions executed by these four body types.

**不同体型上的动作合成**
我们展示了**ShapeMove**模型为不同体型生成相同动作的结果。在下方视频中，我们使用相同的动作描述搭配不同体型描述，来演示我们的方法如何捕捉不同体型执行同一动作时的自然差异。首行视频展示了四种示例体型。"动作"按钮可选择并展示这四种体型执行的不同动作。



#### 4、方法：

![img](assets/overview.svg+xml)

Method [↑ back to top](https://shape-move.github.io/#top)

In the inference phase (b), our model predicts motion tokens and the shape parameter from text inputs. We de-quantize these tokens using FSQ, and project into shape parameters with Projector. We concatenate Shape and Motion feature, and decode into the generated motion sequence with the Motion Decoder. Our model effectively synthesizes shape parameters and shape-aware motions reflecting the physical form and actions described in the input text.

**方法** [↑返回顶部](https://shape-move.github.io/#top)
在推理阶段(b)，我们的模型从文本输入预测动作标记和体型参数。通过FSQ反量化这些标记，并用Projector映射为体型参数。将**体型特征**与**动作特征**拼接后，通过动作解码器生成最终动作序列。我们的模型能有效合成体型参数和形状感知动作，精准反映输入文本描述的物理形态与行为。

1. **技术突破**：首次实现通过单一文本输入同步控制动作风格与体型参数，消除传统方法对预设3D模型的依赖。
2. **应用价值**：为游戏NPC、虚拟试衣等需动态适配多样体型的场景提供自动化解决方案。





### 1.5、SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing   SALAD：基于骨骼感知的潜在扩散模型实现文本驱动动作生成与编辑

#### 1、链接：https://seokhyeonhong.github.io/projects/salad/



是否开源：

#### 2、图示：



#### 3、摘要：

Abstract

Text-driven motion generation has advanced significantly with the rise of denoising diffusion models. However, previous methods often oversimplify representations for the skeletal joints, temporal frames, and textual words, limiting their ability to fully capture the information within each modality and their interactions. Moreover, when using pre-trained models for downstream tasks, such as editing, they typically require additional efforts, including manual interventions, optimization, or fine-tuning. In this paper, we introduce a skeleton-aware latent diffusion (SALAD), a model that explicitly captures the intricate inter-relationships between joints, frames, and words. Furthermore, by leveraging cross-attention maps produced during the generation process, we enable the attention-based zero-shot text-driven motion editing using a pre-trained SALAD model, requiring no additional user input beyond text prompts. Our approach significantly outperforms previous methods in terms of text-motion alignment without compromising generation quality, and demonstrates practical versatility by providing diverse editing capabilities beyond generation.

**摘要**
随着去噪扩散模型的兴起，文本驱动的动作生成技术取得了显著进展。然而，现有方法通常过度简化骨骼关节、时间帧和文本单词的表征，难以充分捕捉各模态内部及其交互关系的信息。此外，当将预训练模型应用于编辑等下游任务时，往往需要额外的人工干预、优化或微调。

本文提出**骨骼感知潜在扩散模型（SALAD）**，该模型显式建模关节、帧和单词之间的复杂交互关系。通过利用生成过程中产生的交叉注意力图，我们实现了基于预训练SALAD模型的**零样本文本驱动动作编辑**，仅需文本提示即可完成，无需额外用户输入。实验表明，我们的方法在保持生成质量的同时，显著提升了文本-动作对齐性能，并通过提供多样化的编辑功能展现了超越生成的实用价值。

#### 4、方法：



# 表格



| 序号 | 文章题目+中稿会议                                            | 摘要翻译                                                     | 核心突出点     | Method部分截图                                               | 是否开源 | 相关链接                                       |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------- | ------------------------------------------------------------ | -------- | ---------------------------------------------- |
| 1.1  | Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation **Sketch2Anim：将故事板草图转化为3D动画** | Sketch2Anim通过多条件生成器和跨域神经映射，首次实现从2D草图到3D动画的端到端自动化合成。 | 分镜图         | ![image-20250518102616157](assets/image-20250518102616157.png) | 否       | https://zhongleilz.github.io/Sketch2Anim/      |
| 1.2  | Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis  人体动作合成中的确定性-随机性多样化隐特征映射 | 本文提出**确定性-随机性多样化隐特征映射（DSDFM）**方法，包含两阶段：**动作重建阶段**：学习人体动作的隐空间分布； **多样化生成阶段**：通过设计的**确定性特征映射（DerODE）\**和\**随机多样化输出生成（DivSDE）**，建立高斯分布与动作隐空间的连接，在无需新增训练参数的情况下提升生成动作的多样性与准确性。相比传统SGMs，DSDFM训练更稳定且效果更优。定性与定量实验表明，DSDFM超越现有方法达到SOTA水平。 |                |                                                              |          |                                                |
| 1.3  | (CVPR 2025) MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities 多粒度运动理解与生成的统一框架 | **框架描述** **MG-MotionLLM** 通过统一指令处理多粒度运动相关任务： **粗粒度**：如文本生成动作、动作描述（上图模块） **细粒度**：如动作生成细节文本、动作定位（下图模块） | 框架，粗细粒度 |                                                              |          |                                                |
| 1.4  | Shape My Moves Text-Driven Shape-Aware Synthesis of Human Motions  文本驱动的形状感知人体动作合成 | **不同体型上的动作合成**<br/>我们展示了**ShapeMove**模型为不同体型生成相同动作的结果。在下方视频中，我们使用相同的动作描述搭配**不同体型**描述，来演示我们的方法如何捕捉不同体型执行同一动作时的自然差异。首行视频展示了四种示例体型。"动作"按钮可选择并展示这四种体型执行的不同动作。 | 不同体型       | ![image-20250518105336663](assets/image-20250518105336663.png) | 否       | https://shape-move.github.io/                  |
| 1.5  | PersonaBooth: Personalized Text-to-Motion Generation 个性化  |                                                              |                |                                                              |          | https://boeun-kim.github.io/page-PersonaBooth/ |
| 1.6  | HOP : Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation  演讲 | **异质纠缠建模**：与现有方法（假设模态独立）不同，HOP **显式建模动作、音频、文本三者的动态交互**，通过拓扑纠缠表征跨模态依赖关系，解决生成动作单一性问题。 **跨模态对齐增强**：提出**重编程模块**重构音频-文本语义，弥补传统方法中模态割裂的缺陷，生成动作与语音/语义的协调性显著优于纯时序对齐方案（如单纯基于LSTM或Transformer的方法）。 | 多模态         |                                                              |          | https://star-uu-wang.github.io/HOP/            |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |
|      |                                                              |                                                              |                |                                                              |          |                                                |



### 1.5、PersonaBooth: Personalized Text-to-Motion Generation 个性化

#### 1、链接：https://boeun-kim.github.io/page-PersonaBooth/

也有提出数据集

是否开源：

#### 2、图示：



#### 3、摘要：

Abstract

This paper introduces Motion Personalization, a new task that generates personalized motions aligned with text descriptions using several basic motions containing Persona. To support this novel task, we introduce a new large-scale motion dataset called **PerMo (PersonaMotion)**, which captures the unique personas of multiple actors. We also propose a multi-modal finetuning method of a pretrained motion diffusion model called **PersonaBooth**. PersonaBooth addresses two main challenges: i) A significant distribution gap between the persona-focused PerMo dataset and the pretraining datasets, which lack persona-specific data, and ii) the difficulty of capturing a consistent persona from the motions vary in content (action type). To tackle the dataset distribution gap, we introduce a persona token to accept new persona features and perform multi-modal adaptation for both text and visuals during finetuning. To capture a consistent persona, we incorporate a contrastive learning technique to enhance intra-cohesion among samples with the same persona. Furthermore, we introduce a contextaware fusion mechanism to maximize the integration of persona cues from multiple input motions. PersonaBooth outperforms state-of-the-art motion style transfer methods, establishing a new benchmark for motion personalization.

本文提出了一项新任务——**动作个性化（Motion Personalization）**，其目标是通过包含人物特征（Persona）的若干基础动作，生成与文本描述对齐的个性化动作。为支持这一任务，我们提出了一个大规模动作数据集 **PerMo（PersonaMotion）**，其中捕捉了多名演员的独特人物特征。此外，我们提出了一种基于预训练动作扩散模型的多模态微调方法 **PersonaBooth**，重点解决两大挑战：

1. **数据分布差异**：PerMo数据集以人物特征为核心，而预训练数据集缺乏此类数据，导致显著分布差距；
2. **人物特征一致性**：因动作内容（如类型）差异，难以从多样动作中提取一致的人物特征。

针对数据分布差异，我们引入**人物标记（persona token）\**以适配新特征，并在微调时对文本和视觉模态进行多模态适应。为提升人物一致性，采用\**对比学习技术**增强同人物样本的内部聚合性，并提出**上下文感知融合机制**，从多输入动作中最大化整合人物特征线索。实验表明，PersonaBooth在动作个性化任务上优于当前最优的动作风格迁移方法，为该领域设立了新基准。



#### 4、方法：





### 1.6、HOP : Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation  演讲

#### 1、链接：https://star-uu-wang.github.io/HOP/



是否开源：

#### 2、图示：

![image-20250518112632181](assets/image-20250518112632181.png)

#### 3、摘要：

**伴随语音的肢体动作**是增强人类交流清晰度与表现力的关键非语言线索，在多模态研究中日益受到关注。尽管现有方法在动作准确性上取得进展，但生成**多样化且连贯的肢体动作**仍面临挑战——多数方法假设多模态输入相互独立，且缺乏对其交互关系的显式建模。为此，我们提出一种新型多模态学习方法 **HOP**，通过捕捉**动作、音频节奏与文本语义间的异质纠缠关系**，生成协调的肢体动作。基于时空图建模，我们实现了音频与动作的对齐；此外，为提升模态一致性，通过**重编程模块**构建音频-文本语义表征，促进跨模态适配。HOP使三模态系统能学习彼此特征，并以拓扑纠缠形式表征。大量实验表明，HOP性能达到最优，生成的伴随动作更自然、更具表现力。

------

### **核心创新与差异总结**

1. **异质纠缠建模**：与现有方法（假设模态独立）不同，HOP **显式建模动作、音频、文本三者的动态交互**，通过拓扑纠缠表征跨模态依赖关系，解决生成动作单一性问题。
2. **跨模态对齐增强**：提出**重编程模块**重构音频-文本语义，弥补传统方法中模态割裂的缺陷，生成动作与语音/语义的协调性显著优于纯时序对齐方案（如单纯基于LSTM或Transformer的方法）。

#### 4、方法：



### 1.7、Rethinking Diffusion for Text-Driven Human Motion Generation 重新思考扩散模型在文本驱动人体动作生成中的应用

#### 1、链接：https://neu-vi.github.io/MARDM/



是否开源：

#### 2、图示：



#### 3、摘要：

## Abstract

Since 2023, Vector Quantization (VQ)-based discrete generation methods have rapidly dominated human motion generation, primarily surpassing diffusion-based continuous generation methods in standard performance metrics. However, VQ-based methods have inherent limitations. Representing continuous motion data as limited discrete tokens leads to inevitable information loss, reduces the diversity of generated motions, and restricts their ability to function effectively as motion priors or generation guidance. In contrast, the continuous space generation nature of diffusion-based methods makes them well-suited to address these limitations and with even potential for model scalability. In this work, we systematically investigate why current VQ-based methods perform well and explore the limitations of existing diffusion-based methods from the perspective of motion data representation and distribution. Drawing on these insights, we preserve the inherent strengths of a diffusion-based human motion generation model and gradually optimize it with inspiration from VQ-based approaches. Our approach introduces a human motion diffusion model enabled to perform bidirectional masked autoregression, optimized with a reformed data representation and distribution. Additionally, we also propose more robust evaluation methods to fairly assess different-based methods. Extensive experiments on benchmark human motion generation datasets demonstrate that our method excels previous methods and achieves state-of-the-art performances.

自2023年以来，基于向量量化（VQ）的离散生成方法迅速主导了人体动作生成领域，在标准性能指标上主要超越了基于扩散模型的连续生成方法。然而，VQ方法存在固有局限性：将连续动作数据表示为有限的离散标记会导致不可避免的信息损失，降低生成动作的多样性，并限制其作为动作先验或生成引导的有效性。相比之下，扩散模型的连续空间生成特性使其更适合解决这些局限，甚至具备模型可扩展性的潜力。

本研究系统性地探究了当前VQ方法表现优异的原因，并从动作数据表示和分布的角度分析了现有扩散方法的局限性。基于这些洞见，我们保留了基于扩散模型的人体动作生成方法的固有优势，同时借鉴VQ方法的思路逐步优化。我们提出了一种支持双向掩码自回归的人体动作扩散模型，通过改进的数据表示和分布进行优化。此外，我们还提出了更鲁棒的评价方法，以公平评估不同方法。在标准人体动作生成数据集上的大量实验表明，我们的方法超越了现有方法，达到了最先进的性能水平。

#### 核心创新点

1. **理论突破**：首次系统分析了VQ与扩散模型在动作生成领域的优劣势，为后续研究提供了理论基础
2. 方法创新：
   - 提出双向掩码自回归的扩散模型架构
   - 设计改进的数据表示和分布策略
3. **评估革新**：建立了更公平、全面的评估体系

#### 与现有工作的区别

1. 不同于简单采用VQ或扩散模型的现有方案，本工作创造性地融合了两者的优势
2. 解决了传统扩散模型在动作生成中的关键瓶颈问题
3. 提出的评估方法克服了当前领域评价指标单一的问题

#### 4、方法：



### 1.?、AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward

#### 1、链接：https://atom-motion.github.io/



是否开源：coming soon.

#### 2、图示：



#### 3、摘要：



#### 4、方法：







手势

不同22joint的逼近目标速度做随机

joint相关性编码器











