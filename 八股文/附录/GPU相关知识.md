# GPU相关知识

主要参考的文章：[深入GPU硬件架构及运行机制 - 知乎](https://zhuanlan.zhihu.com/p/357112957)，这篇也转载了一篇博客园的文章，这里仅做整理来用。

阅读完这篇之后，应当能够回答下面的问题：

> 1、GPU是如何与CPU协调工作的？
> 2、GPU也有缓存机制吗？有几层？它们的速度差异多少？
> 3、GPU的渲染流程有哪些阶段？它们的功能分别是什么？
> 4、Early-Z技术是什么？发生在哪个阶段？这个阶段还会发生什么？会产生什么问题？如何解决？
> 5、SIMD和SIMT是什么？它们的好处是什么？co-issue呢？
> 6、GPU是并行处理的么？若是，硬件层是如何设计和实现的？
> 7、GPC、[TPC](https://zhida.zhihu.com/search?content_id=167487986&content_type=Article&match_order=1&q=TPC&zhida_source=entity)、SM是什么？Warp又是什么？它们和Core、Thread之间的关系如何？
> 8、顶点着色器（VS）和像素着色器（PS）可以是同一处理单元吗？为什么？
> 9、像素着色器（PS）的最小处理单位是1像素吗？为什么？会带来什么影响？
> 10、Shader中的if、for等语句会降低渲染效率吗？为什么？
> 11、如下图，渲染相同面积的图形，三角形数量少（左）的还是数量多（右）的效率更快？为什么？
>
> ![img](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/v2-194986889f4cfd4f3a103b968f73cdba_1440w.jpg)
>
>
> 12、GPU Context是什么？有什么作用？
> 13、造成渲染瓶颈的问题很可能有哪些？该如何避免或优化它们？



# 一、文章重点提炼

## 1.介绍

- 我们日常讨论GPU和显卡时，经常混为一谈，严格来说是有所区别的。GPU是显卡（Video card、Display card、Graphics card）最核心的部件，但除了GPU，显卡还有扇热器、通讯元件、与主板和显示器连接的各类插槽。
- GPU的发展历史过一下参考文章的对应部分即可；

> - **2008 - Tesla**
>   Tesla最初是给计算处理单元使用的，应用于早期的CUDA系列显卡芯片中，并不是真正意义上的普通图形处理芯片。
> - **2010 - Fermi**
>   Fermi是第一个完整的GPU计算架构。首款可支持与共享存储结合纯cache层次的GPU架构，支持ECC的GPU架构。
> - **2012 - Kepler**
>   Kepler相较于Fermi更快，效率更高，性能更好。
> - **2014 - Maxwell**
>   其全新的立体像素全局光照 (VXGI) 技术首次让游戏 GPU 能够提供实时的动态全局光照效果。基于 Maxwell 架构的 GTX 980 和 970 GPU 采用了包括多帧采样抗锯齿 (MFAA)、动态超级分辨率 (DSR)、VR Direct 以及超节能设计在内的一系列新技术。
> - **2016 - Pascal**
>   Pascal 架构将处理器和数据集成在同一个程序包内，以实现更高的计算效率。1080系列、1060系列基于Pascal架构
> - **2017 - Volta**
>   Volta 配备640 个Tensor 核心，每秒可提供超过100 兆次浮点运算(TFLOPS) 的深度学习效能，比前一代的Pascal 架构快5 倍以上。
> - **2018 - Turing**
>   Turing 架构配备了名为 RT Core 的专用光线追踪处理器，能够以高达每秒 10 Giga Rays 的速度对光线和声音在 3D 环境中的传播进行加速计算。Turing 架构将实时光线追踪运算加速至上一代 NVIDIA Pascal™ 架构的 25 倍，并能以高出 CPU 30 多倍的速度进行电影效果的最终帧渲染。2060系列、2080系列显卡也是跳过了Volta直接选择了Turing架构。
>
> 下图是部分GPU架构的发展历程：
>
> ![img](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/v2-c1e1e4dd317dbb9df6086589a57cbff4_1440w.jpg)



## 2.不同的GPU架构

GPU的微观结构因不同厂商、不同架构都会有所差异，但核心部件、概念、以及运行机制大同小异。下面将展示部分架构的GPU微观物理结构。

### （1）比如：Tesla（其他架构去文章里看即可）

![img](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/d8d6e3be58ab51479738e377a4776bd9.png)

### （2）共有的部分

纵观上一节的所有GPU架构，可以发现它们虽然有所差异，但存在着很多相同的概念和部件：

- GPC
- TPC
- Thread
- SM、SMX、SMM
- Warp
- SP
- Core
- ALU
- FPU
- SFU
- ROP
- Load/Store Unit
- L1 Cache
- L2 Cache
- Memory
- Register File

以上各个部件的用途将在后面进行阐述。
GPU为什么会有这么多层级且有这么多雷同的部件？答案是GPU的任务是天然并行的，现代GPU的架构皆是以高度并行能力而设计的。

![img](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/v2-7692eb02eeeb3ba83c0ea285944acec8_r.jpg)

从Fermi（2010年提出来的）开始NVIDIA使用类似的原理架构，使用一个Giga Thread Engine来管理所有正在进行的工作，GPU被划分成多个GPCs(Graphics Processing Cluster)，每个GPC拥有多个SM（SMX、SMM）和一个光栅化引擎(Raster Engine)，它们其中有很多的连接，最显著的是Crossbar，它可以连接GPCs和其它功能性模块（例如ROP或其他子系统）。

程序员编写的shader是在SM上完成的。每个SM包含许多为线程执行数学运算的Core（核心）。例如，一个线程可以是顶点或像素着色器调用。这些Core和其它单元由Warp Scheduler驱动，Warp Scheduler管理一组32个线程作为Warp（线程束）并将要执行的指令移交给Dispatch Units。

GPU中实际有多少这些单元（每个GPC有多少个SM，多少个GPC ......）取决于芯片配置本身。例如，GM204有4个GPC，每个GPC有4个SM，但Tegra X1有1个GPC和2个SM，它们均采用Maxwell设计。SM设计本身（内核数量，指令单位，调度程序......）也随着时间的推移而发生变化，并帮助使芯片变得如此高效，可以从高端台式机扩展到笔记本电脑移动。



`接下来，我们来看一下SM中有什么：`

![img](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/v2-7326d959209bf39d649985746e0f86ef_r.jpg)

如上图，对于某些GPU（如Fermi部分型号）的单个SM，包含：

- （1）32个运算核心 （Core，也叫流处理器Stream Processor）
- （2）16个LD/ST（load/store）模块来加载和存储数据
- （3）4个SFU（Special function units）执行特殊数学运算（sin、cos、log等）
- （4）128KB寄存器（Register File）
- （5）64KB L1缓存
- （6）全局内存缓存（Uniform Cache）
- （7）纹理读取单元（应该指的是上图里的Tex蓝色部分）
- （8）纹理缓存（Texture Cache）
- （9）PolyMorph Engine：多边形引擎负责属性装配（attribute Setup）、顶点拉取(VertexFetch)、曲面细分等（这个模块可以理解专门处理顶点相关的东西）。
- （10）2个Warp Schedulers：这个模块负责warp调度，一个warp由32个线程组成，warp调度器的指令通过Dispatch Units送到Core执行。
- （11）指令缓存（Instruction Cache）
- （12）内部链接网络（Interconnect Network）



## 3.GPU逻辑管线

了解上一节的部件和概念之后，可以深入阐述GPU的渲染过程和步骤。下面将以Fermi家族的SM为例，进行逻辑管线的详细说明（上面那个大图中也有这部分）。

![img](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/v2-8792c13419695fd5ca051208c427dbe7_r.jpg)

- (1)程序通过图形API(DX、GL、WEBGL)发出`drawcall`指令，指令会被推送到驱动程序，驱动会检查指令的合法性，然后会`把指令放到GPU可以读取的Pushbuffer`中。
- (2)经过一段时间或者显式调用flush指令后，驱动程序把Pushbuffer的内容发送给GPU，GPU通过`主机接口（Host Interface）`接受这些命令，并通过`前端（Front End）`处理这些命令。
- (3)在图元分配器(Primitive Distributor)中开始工作分配，处理indexbuffer中的顶点产生三角形分成批次(batches)，然后发送给多个GPCs(Graphics Processing Cluster)。这一步的理解就是提交上来n个三角形，分配给这几个GPC同时处理。

先熟悉一下前面这三个步骤，接下来我们要进入到GPC内部去看看了：

![image-20250310171606318](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/image-20250310171606318.png)

注意看SM指的应该是一格GPC（一共4列，每一列应该是一个GPC，每个GPC有4摞，指的应该是SM）。现在继续看后面的过程：

![image-20250310171939887](GPU%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.assets/image-20250310171939887.png)

- （4）在GPC中，每个SM中的Poly Morph Engine负责通过三角形索引(triangle indices)取出三角形的数据(vertex data)，即图中的`Vertex Fetch`模块（在PolyMorph Engine中可以看到）。
- （5）在获取数据之后，在SM中以32个线程为一组的线程束(Warp)来调度，来开始处理顶点数据。Warp是典型的单指令多线程（SIMT，SIMD单指令多数据的升级）的实现，也就是32个线程同时执行的指令是一模一样的，只是线程数据不一样，这样的好处就是一个warp只需要一套逻辑对指令进行解码和执行就可以了，芯片可以做的更小更快，之所以可以这么做是由于GPU需要处理的任务是天然并行的。
- （6）SM的warp调度器会按照顺序分发指令给整个warp，单个warp中的线程会锁步(lock-step)执行各自的指令，如果线程碰到不激活执行的情况也会被遮掩(be masked out)。被遮掩的原因有很多，例如当前的指令是if(true)的分支，但是当前线程的数据的条件是false，或者循环的次数不一样（比如for循环次数n不是常量，或被break提前终止了但是别的还在走），`因此在shader中的分支会显著增加时间消耗，在一个warp中的分支除非32个线程都走到if或者else里面，否则相当于所有的分支都走了一遍，线程不能独立执行指令而是以warp为单位，而这些warp之间才是独立的。`

> 对（6）的补充：
>
> 在GPU的SIMT架构中，**warp内线程的锁步执行**和**分支发散（Divergent Branch）**是理解性能瓶颈的关键。以下通过具体场景逐步解释：
>
> ---
>
> ### **1. Warp的锁步执行机制**
> - **什么是warp？**  
>   GPU将32个线程（NVIDIA架构）捆绑为一个warp，作为最小调度单位。所有线程**共享同一指令流**，硬件在每个时钟周期为整个warp分发一条指令。
>   
> - **锁步（Lock-Step）的含义**  
>   同一warp内的32个线程必须**严格同步执行相同的指令**，但每个线程处理自己的数据。例如，当warp执行`a = b + c`时，所有线程同时进行加法运算，但各自的`b`和`c`值可能不同。
>
> ---
>
> ### **2. 分支如何导致性能问题？**
> 当warp内的线程遇到条件分支（如`if-else`或`for`循环）时，**如果线程的条件不一致**，GPU必须通过**遮掩（Masking）**分别处理每个分支路径。这会导致**实际执行时间成倍增加**。
>
> ---
>
> #### **场景1：理想情况（无分支发散）**
> ```cpp
> // 所有线程的条件一致（假设condition为true）
> if (condition) {
>     // 所有32个线程都执行此代码块
> } else {
>     // 无线程执行
> }
> ```
> - **执行过程**  
>   Warp一次性执行`if`块内的指令，无需处理`else`块。  
>   **总时间 = `if`块的执行时间**
>
> ---
>
> #### **场景2：分支发散（最坏情况）**
> ```cpp
> // 假设16个线程满足条件，16个不满足
> if (threadId % 2 == 0) { 
>     // 16个线程执行此处
> } else {
>     // 另外16个线程执行此处
> }
> ```
> - **执行过程**  
>   1. **第一次执行**：GPU激活`if`块，**屏蔽（Mask Out）** 不满足条件的16个线程（执行`else`的线程被挂起）。  
>      此时，只有50%的硬件资源被利用。  
>   2. **第二次执行**：GPU激活`else`块，屏蔽原本执行`if`的16个线程。  
>      **总时间 = `if`块时间 + `else`块时间**  
> - **性能损失**  
>   原本可以并行执行的指令被拆分为两次串行执行，**时间翻倍**，硬件利用率仅为50%。
>
> ---
>
> ### **3. 遮掩（Masking）的工作原理**
> - **硬件如何管理分支？**  
>   每个warp维护一个**执行掩码（Execution Mask）**，标记当前活跃的线程。当遇到分支时：  
>   1. 根据线程条件更新掩码，仅激活符合条件的线程。  
>   2. 执行当前分支路径的指令，未激活的线程被挂起（不执行，但占用warp调度槽）。  
>   3. 处理完当前路径后，切换掩码并执行另一分支路径。  
>
> - **为什么说“所有分支都走了一遍”？**  
>   即使某些线程不满足条件，warp仍需为**每个可能的分支路径**完整执行所有指令。例如：  
>   ```cpp
>   if (cond) { A } else { B }
>   ```
>   - 无论线程是否进入`if`或`else`，整个warp必须先后执行`A`和`B`的代码，只是通过掩码控制哪些线程实际生效。
>
> ---
>
> ### **4. 循环中的分支发散**
> ```cpp
> // 不同线程的循环次数不同
> for (int i = 0; i < n; i++) { 
>     // 假设n是变量，不同线程的n值不同
> }
> ```
> - **执行过程**  
>   Warp必须执行到**所有线程的循环都结束**为止。例如：  
>   - 线程0的`n=3`，线程1的`n=5`  
>   - Warp会执行5次循环，但线程0在后2次循环中被屏蔽。  
> - **性能损失**  
>   实际循环次数由**最大的`n`值决定**，导致部分线程浪费计算资源。
>
> ---
>
> ### **5. 如何优化分支发散？**
> - **原则**：尽可能让同一warp内的线程**走相同的分支路径**。  
> - **方法**：  
>   1. **数据布局优化**：将相同条件的线程分组到同一warp中。  
>      （例如：通过排序或调整线程块大小）  
>   2. **避免细粒度分支**：使用算术逻辑代替`if-else`。  
>      ```cpp
>      // 原代码
>      if (x > 0) y = x; else y = 0;
>      // 优化为
>      y = max(x, 0);
>      ```
>   3. **减少循环次数差异**：确保同一warp内的循环次数一致。
>
> ---
>
> ### **总结**
> - **关键问题**：warp内线程的分支不一致会导致**串行执行所有分支路径**，显著增加时间。  
> - **性能影响**：分支发散时，时间开销 = 各分支路径时间的总和。  
> - **优化目标**：通过数据重组或算法调整，**最大化warp内线程的条件一致性**。

接着上面的（6）步骤，接下来是：

- （7）warp中的指令可以被一次完成，也可能经过多次调度，例如通常SM中的LD/ST(加载存取)单元数量明显少于基础数学操作单元。
- （8）由于某些指令比其他指令需要更长的时间才能完成，特别是内存加载，`warp调度器可能会简单地切换到另一个没有内存等待的warp，这是GPU如何克服内存读取延迟的关键`，只是简单地切换活动线程组。`为了使这种切换非常快，调度器管理的所有warp在寄存器文件中都有自己的寄存器。`这里就会有个矛盾产生，`shader需要越多的寄存器，就会给warp留下越少的空间，就会产生越少的warp`，这时候在碰到内存延迟的时候就会只是等待，而没有可以运行的warp可以切换。